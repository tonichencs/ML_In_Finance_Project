{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install gplearn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DsqycdKO2L7Z",
        "outputId": "664447c7-3653-4bf5-e401-a8af7175aa5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gplearn\n",
            "  Downloading gplearn-0.4.2-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: scikit-learn>=1.0.2 in /usr/local/lib/python3.11/dist-packages (from gplearn) (1.6.1)\n",
            "Requirement already satisfied: joblib>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from gplearn) (1.5.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.0.2->gplearn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.0.2->gplearn) (1.15.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.0.2->gplearn) (3.6.0)\n",
            "Downloading gplearn-0.4.2-py3-none-any.whl (25 kB)\n",
            "Installing collected packages: gplearn\n",
            "Successfully installed gplearn-0.4.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wBhTXzDZ1woP"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
        "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "from gplearn.genetic import SymbolicTransformer, SymbolicClassifier, SymbolicRegressor\n",
        "from gplearn.functions import make_function\n",
        "from gplearn.fitness import make_fitness\n",
        "from google.colab import drive\n",
        "\n",
        "import hashlib"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aUg_U-8t9Hws",
        "outputId": "b276164e-d9ff-430c-c74a-46ef49d335e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stock_data = pd.read_csv(\"/content/drive/MyDrive/final_data.csv\")\n",
        "stock_data  = stock_data [stock_data ['date'].between('2004-01-01', '2024-12-31')]"
      ],
      "metadata": {
        "id": "WQUcOD4M9PaR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "id_col = [\"PERMNO\", 'CUSIP', 'Ticker', 'SICCD','NAICS']\n",
        "time_col = ['MthCalDt']\n",
        "factor_col = ['mom_12','mom_6', 'vol_12', 'vol_6', 'rev_1', 'rvol_1', 'beta',\"rsi_6\", 'trend_strength']\n",
        "market_col = ['qmj_safety','seas_11_15na', 'ret_3_1', 'iskew_ff3_21d', 'rskew_21d', 'sti_gr1a',\n",
        "              'earnings_variability', 'nfna_gr1a', 'seas_16_20an', 'corr_1260d']\n",
        "fin_col = ['capxy', 'chechy', 'cshfdy', 'cshpry', 'dltry', 'dpcy', 'epspxy', 'oibdpy', 'txty']"
      ],
      "metadata": {
        "id": "TtsRMBkXCyai"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_features = factor_col + market_col + fin_col"
      ],
      "metadata": {
        "id": "fLofDBzbPyTa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\"\"\"\n",
        "def safe_div(x1, x2):\n",
        "    return np.where(np.abs(x2) < 1e-8, 0, x1 / x2)\n",
        "\n",
        "def abs_func(x):\n",
        "    return np.abs(x)\n",
        "\n",
        "def sign_func(x):\n",
        "    return np.sign(x)\n",
        "\n",
        "def max_func(x1, x2):\n",
        "    return np.maximum(x1, x2)\n",
        "\n",
        "def min_func(x1, x2):\n",
        "    return np.minimum(x1, x2)\n",
        "\n",
        "\n",
        "safe_division = make_function(function=safe_div, name='safe_div', arity=2)\n",
        "absolute = make_function(function=abs_func, name='abs', arity=1)\n",
        "sign = make_function(function=sign_func, name='sign', arity=1)\n",
        "maximum = make_function(function=max_func, name='max', arity=2)\n",
        "minimum = make_function(function=min_func, name='min', arity=2)\n",
        "\n",
        "\n",
        "function_set = ['add', 'sub', 'mul', safe_division, 'sqrt', 'log', 'abs',\n",
        "                'neg', 'inv', maximum, minimum, sign]\n",
        "\n",
        "class PanelGeneticFactorGenerator:\n",
        "    def __init__(self, population_size=500, generations=10,\n",
        "                 tournament_size=20, stopping_criteria=0.01,\n",
        "                 const_range=(-1., 1.), init_depth=(2, 6),\n",
        "                 init_method='half and half', function_set=None,\n",
        "                 parsimony_coefficient=0.01, random_state=42):\n",
        "\n",
        "        self.population_size = population_size\n",
        "        self.generations = generations\n",
        "        self.tournament_size = tournament_size\n",
        "        self.stopping_criteria = stopping_criteria\n",
        "        self.const_range = const_range\n",
        "        self.init_depth = init_depth\n",
        "        self.init_method = init_method\n",
        "        self.function_set = function_set or ['add', 'sub', 'mul', 'div']\n",
        "        self.parsimony_coefficient = parsimony_coefficient\n",
        "        self.random_state = random_state\n",
        "\n",
        "        self.generated_factors = {}\n",
        "        self.factor_expressions = {}\n",
        "\n",
        "    def prepare_data(self, df, feature_cols, target_col):\n",
        "\n",
        "        X = df[feature_cols].copy()\n",
        "        X = X.fillna(X.median())\n",
        "        X = X.replace([np.inf, -np.inf], np.nan).fillna(X.median())\n",
        "\n",
        "        y = df[target_col].fillna(df[target_col].median())\n",
        "        y = y.replace([np.inf, -np.inf], np.nan).fillna(y.median())\n",
        "\n",
        "        return X, y\n",
        "\n",
        "    def generate_supervised_factors(self, df, feature_cols, target_col, n_factors=10):\n",
        "\n",
        "        print(f\"Start generating {n_factors} supervision factors\")\n",
        "\n",
        "        X, y = self.prepare_data(df, feature_cols, target_col)\n",
        "\n",
        "        print(f\"Data Shape: X={X.shape}, y={y.shape}\")\n",
        "\n",
        "        for i in range(n_factors):\n",
        "            print(f\"生成第 {i+1}/{n_factors} 个监督因子...\")\n",
        "\n",
        "            try:\n",
        "                regressor = SymbolicRegressor(\n",
        "                    population_size=self.population_size,\n",
        "                    generations=self.generations,\n",
        "                    tournament_size=self.tournament_size,\n",
        "                    stopping_criteria=self.stopping_criteria,\n",
        "                    const_range=self.const_range,\n",
        "                    init_depth=self.init_depth,\n",
        "                    init_method=self.init_method,\n",
        "                    function_set=self.function_set,\n",
        "                    parsimony_coefficient=self.parsimony_coefficient,\n",
        "                    random_state=self.random_state + i,  # 不同的随机种子\n",
        "                    verbose=0,\n",
        "                    n_jobs=1  # 避免并行计算问题\n",
        "                )\n",
        "\n",
        "                regressor.fit(X, y)\n",
        "                factor_values = regressor.predict(X)\n",
        "\n",
        "                factor_name = f'genetic_factor_{i+1}'\n",
        "                self.generated_factors[factor_name] = factor_values\n",
        "                self.factor_expressions[factor_name] = str(regressor._program)\n",
        "\n",
        "                print(f\"{factor_name} generate successfully\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"{i+1} failed {e}\")\n",
        "                continue\n",
        "\n",
        "        print(f\"{len(self.generated_factors)} factors generate successfully\")\n",
        "        return self.generated_factors\n",
        "\n",
        "    def get_factor_dataframe(self, original_df):\n",
        "\n",
        "        if not self.generated_factors:\n",
        "            print(\"No factor was generated\")\n",
        "            return None\n",
        "\n",
        "        factor_df = pd.DataFrame(self.generated_factors, index=original_df.index)\n",
        "\n",
        "        \"\"\"\n",
        "        \"\"\"\n",
        "        for col in id_col + time_col:\n",
        "            if col in original_df.columns:\n",
        "                factor_df[col] = original_df[col]\n",
        "\n",
        "        return factor_df\n",
        "      \"\"\"\n",
        "      \"\"\"\n",
        "    def print_factor_expressions(self):\n",
        "\n",
        "        print(\"\\n=== factor expressions ===\")\n",
        "        for factor_name, expression in self.factor_expressions.items():\n",
        "            print(f\"{factor_name}: {expression}\")\n",
        "\n",
        "    def evaluate_factors(self, df, target_col):\n",
        "\n",
        "        if not self.generated_factors:\n",
        "            print(\"No factor was generated\")\n",
        "            return\n",
        "\n",
        "        print(\"\\n=== factor evaluate ===\")\n",
        "        target = df[target_col]\n",
        "\n",
        "        for factor_name, factor_values in self.generated_factors.items():\n",
        "\n",
        "            corr = np.corrcoef(factor_values, target)[0, 1]\n",
        "\n",
        "            factor_returns = pd.Series(factor_values)\n",
        "            ic_mean = corr\n",
        "            ic_std = np.std(factor_returns)\n",
        "            ir = ic_mean / ic_std if ic_std != 0 else 0\n",
        "\n",
        "            print(f\"{factor_name}:\")\n",
        "            print(f\"  correlation: {corr:.4f}\")\n",
        "            print(f\"  IC: {ir:.4f}\")\n",
        "            print(f\"  length of expression: {len(self.factor_expressions[factor_name])}\")\n",
        "            print()\n",
        "\n",
        "def run_genetic_factor_generation(df, target_col, n_factors=10):\n",
        "\n",
        "    print(\"Start generation\")\n",
        "\n",
        "    if target_col is None:\n",
        "        raise ValueError(\"Must be a target_col\")\n",
        "\n",
        "    if isinstance(target_col, list):\n",
        "        target_col = target_col[0]\n",
        "\n",
        "    print(f\"Shape of data: {df.shape}\")\n",
        "    print(f\"Target: '{target_col}'\")\n",
        "\n",
        "\n",
        "    generator = PanelGeneticFactorGenerator(\n",
        "        population_size=500,\n",
        "        generations=10,\n",
        "        tournament_size=20,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        generated_factors = generator.generate_supervised_factors(\n",
        "            df, all_features, target_col, n_factors=n_factors\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(f\"Falied: {e}\")\n",
        "        return None, None\n",
        "\n",
        "    factor_df = generator.get_factor_dataframe(df)\n",
        "    generator.print_factor_expressions()\n",
        "\n",
        "    print(\"Factor evaluate...\")\n",
        "    try:\n",
        "        generator.evaluate_factors(df, target_col)\n",
        "    except Exception as e:\n",
        "        print(f\"Evaluation failed: {e}\")\n",
        "\n",
        "    print(f\"{len(generator.generated_factors)} new factor\")\n",
        "\n",
        "    return generator, factor_df\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "id": "BFXGtD8l21tU",
        "outputId": "e1d67782-95d2-4d68-9f67-15e9e7f4c989"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "unindent does not match any outer indentation level (<tokenize>, line 120)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<tokenize>\"\u001b[0;36m, line \u001b[0;32m120\u001b[0m\n\u001b[0;31m    \"\"\"\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
        "\n",
        "def tanh_func(x):\n",
        "    return np.tanh(x)\n",
        "\n",
        "def safe_log(x):\n",
        "    return np.log(np.abs(x) + 1e-8)\n",
        "\n",
        "def safe_div(x1, x2):\n",
        "    return np.where(np.abs(x2) < 1e-8, 0, x1 / x2)\n",
        "\n",
        "def safe_sqrt(x):\n",
        "    return np.sqrt(np.abs(x))\n",
        "\n",
        "def safe_exp(x):\n",
        "    return np.exp(np.clip(x, -10, 10))\n",
        "\n",
        "sigmoid_func = make_function(function=sigmoid, name='sigmoid', arity=1)\n",
        "tanh_function = make_function(function=tanh_func, name='tanh', arity=1)\n",
        "safe_log_func = make_function(function=safe_log, name='safe_log', arity=1)\n",
        "safe_div_func = make_function(function=safe_div, name='safe_div', arity=2)\n",
        "safe_sqrt_func = make_function(function=safe_sqrt, name='safe_sqrt', arity=1)\n",
        "safe_exp_func = make_function(function=safe_exp, name='safe_exp', arity=1)\n",
        "\n",
        "classification_functions = ['add', 'sub', 'mul', safe_div_func, sigmoid_func, tanh_function, safe_sqrt_func, safe_exp_func]\n",
        "\n",
        "class ClassificationGeneticFactorGenerator:\n",
        "    def __init__(self, population_size=300, generations=10,\n",
        "                 tournament_size=20, stopping_criteria=0.01,\n",
        "                 const_range=(-2., 2.), init_depth=(2, 5),\n",
        "                 random_state=42, train_ratio=0.7):\n",
        "\n",
        "        self.population_size = population_size\n",
        "        self.generations = generations\n",
        "        self.tournament_size = tournament_size\n",
        "        self.stopping_criteria = stopping_criteria\n",
        "        self.const_range = const_range\n",
        "        self.init_depth = init_depth\n",
        "        self.random_state = random_state\n",
        "        self.train_ratio = train_ratio\n",
        "\n",
        "        self.generated_factors = {}\n",
        "        self.factor_expressions = {}\n",
        "        self.target_info = {}\n",
        "        self.trained_models = {}\n",
        "        self.label_encoder = None\n",
        "\n",
        "    def split_data(self, df):\n",
        "        n_train = int(len(df) * self.train_ratio)\n",
        "        train_df = df.iloc[:n_train].copy()\n",
        "        full_df = df.copy()\n",
        "\n",
        "        print(f\"Training set size: {len(train_df)} rows ({self.train_ratio*100:.0f}%)\")\n",
        "        print(f\"Full dataset size: {len(full_df)} rows (100%)\")\n",
        "\n",
        "        return train_df, full_df\n",
        "\n",
        "    def analyze_target(self, y):\n",
        "        unique_vals = np.unique(y)\n",
        "        n_unique = len(unique_vals)\n",
        "\n",
        "        target_info = {\n",
        "            'unique_values': unique_vals,\n",
        "            'n_unique': n_unique,\n",
        "            'is_binary': n_unique == 2,\n",
        "            'is_multiclass': n_unique > 2 and n_unique <= 10,\n",
        "            'distribution': pd.Series(y).value_counts().to_dict()\n",
        "        }\n",
        "\n",
        "        return target_info\n",
        "\n",
        "    def prepare_classification_target(self, y):\n",
        "        target_info = self.analyze_target(y)\n",
        "        self.target_info = target_info\n",
        "\n",
        "        if target_info['is_binary']:\n",
        "            if self.label_encoder is None:\n",
        "                self.label_encoder = LabelEncoder()\n",
        "                y_encoded = self.label_encoder.fit_transform(y).astype(float)\n",
        "            else:\n",
        "                y_encoded = self.label_encoder.transform(y).astype(float)\n",
        "            return y_encoded, 'binary'\n",
        "\n",
        "        elif target_info['is_multiclass']:\n",
        "            if self.label_encoder is None:\n",
        "                self.label_encoder = LabelEncoder()\n",
        "                y_encoded = self.label_encoder.fit_transform(y).astype(float)\n",
        "            else:\n",
        "                y_encoded = self.label_encoder.transform(y).astype(float)\n",
        "            return y_encoded, 'multiclass'\n",
        "\n",
        "        else:\n",
        "            return y.astype(float), 'regression'\n",
        "\n",
        "    def prepare_data(self, df, feature_cols, target_col):\n",
        "        X = df[feature_cols].copy()\n",
        "        X = X.fillna(X.median())\n",
        "        X = X.replace([np.inf, -np.inf], np.nan).fillna(X.median())\n",
        "\n",
        "        y = df[target_col].copy()\n",
        "        y = y.fillna(y.mode()[0] if not y.mode().empty else y.iloc[0])\n",
        "\n",
        "        return X, y\n",
        "\n",
        "    def apply_factor_to_data(self, X, model, factor_name, task_type):\n",
        "        try:\n",
        "            factor_values = model.predict(X)\n",
        "\n",
        "            if 'binary' in factor_name or task_type == 'binary':\n",
        "                factor_values = sigmoid(factor_values)\n",
        "            elif 'ordinal' in factor_name:\n",
        "                pass\n",
        "\n",
        "            return factor_values\n",
        "        except Exception as e:\n",
        "            print(f\"Error applying factor {factor_name}: {e}\")\n",
        "            return np.zeros(len(X))\n",
        "\n",
        "    def generate_binary_classification_factors(self, X_train, y_train, n_factors=5):\n",
        "        print(f\"Generating {n_factors} binary classification factors...\")\n",
        "\n",
        "        for i in range(n_factors):\n",
        "            try:\n",
        "                regressor = SymbolicRegressor(\n",
        "                    population_size=self.population_size,\n",
        "                    generations=self.generations,\n",
        "                    tournament_size=self.tournament_size,\n",
        "                    stopping_criteria=self.stopping_criteria,\n",
        "                    const_range=self.const_range,\n",
        "                    init_depth=self.init_depth,\n",
        "                    function_set=['add', 'sub', 'mul', safe_div_func, sigmoid_func],\n",
        "                    parsimony_coefficient=0.01,\n",
        "                    random_state=self.random_state + i,\n",
        "                    verbose=0,\n",
        "                    n_jobs=1\n",
        "                )\n",
        "\n",
        "                regressor.fit(X_train, y_train)\n",
        "\n",
        "                factor_name = f'genetic_binary_{i+1}'\n",
        "                self.trained_models[factor_name] = regressor\n",
        "                self.factor_expressions[factor_name] = str(regressor._program)\n",
        "\n",
        "                factor_values = regressor.predict(X_train)\n",
        "                factor_values = sigmoid(factor_values)\n",
        "\n",
        "                try:\n",
        "                    auc = roc_auc_score(y_train, factor_values)\n",
        "                    print(f\"  {factor_name} training AUC: {auc:.4f}\")\n",
        "                except:\n",
        "                    corr = np.corrcoef(factor_values, y_train)[0, 1]\n",
        "                    print(f\"  {factor_name} training correlation: {corr:.4f}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"  Binary factor {i+1} training failed: {e}\")\n",
        "                continue\n",
        "\n",
        "    def generate_multiclass_factors(self, X_train, y_train, n_factors=5):\n",
        "        print(f\"Generating {n_factors} multiclass factors...\")\n",
        "\n",
        "        unique_classes = np.unique(y_train)\n",
        "        n_classes = len(unique_classes)\n",
        "        factors_per_class = max(1, n_factors // n_classes)\n",
        "\n",
        "        for class_idx, target_class in enumerate(unique_classes):\n",
        "            y_binary = (y_train == target_class).astype(float)\n",
        "\n",
        "            for i in range(factors_per_class):\n",
        "                try:\n",
        "                    regressor = SymbolicRegressor(\n",
        "                        population_size=max(200, self.population_size // 2),\n",
        "                        generations=max(10, self.generations // 2),\n",
        "                        tournament_size=self.tournament_size,\n",
        "                        stopping_criteria=self.stopping_criteria,\n",
        "                        const_range=self.const_range,\n",
        "                        init_depth=(1, 4),\n",
        "                        function_set=['add', 'sub', 'mul', safe_div_func, sigmoid_func],\n",
        "                        parsimony_coefficient=0.02,\n",
        "                        random_state=self.random_state + class_idx * 100 + i,\n",
        "                        verbose=0,\n",
        "                        n_jobs=1\n",
        "                    )\n",
        "\n",
        "                    regressor.fit(X_train, y_binary)\n",
        "\n",
        "                    factor_name = f'genetic_class_{target_class}_{i+1}'\n",
        "                    self.trained_models[factor_name] = regressor\n",
        "                    self.factor_expressions[factor_name] = str(regressor._program)\n",
        "\n",
        "                    factor_values = regressor.predict(X_train)\n",
        "                    factor_values = sigmoid(factor_values)\n",
        "\n",
        "                    try:\n",
        "                        auc = roc_auc_score(y_binary, factor_values)\n",
        "                        print(f\"  {factor_name} training AUC: {auc:.4f}\")\n",
        "                    except:\n",
        "                        corr = np.corrcoef(factor_values, y_binary)[0, 1]\n",
        "                        print(f\"  {factor_name} training correlation: {corr:.4f}\")\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"  Class {target_class} factor {i+1} training failed: {e}\")\n",
        "                    continue\n",
        "\n",
        "    def generate_ordinal_factors(self, X_train, y_train, n_factors=5):\n",
        "        print(f\"Generating {n_factors} ordinal factors...\")\n",
        "\n",
        "        for i in range(n_factors):\n",
        "            try:\n",
        "                regressor = SymbolicRegressor(\n",
        "                    population_size=self.population_size,\n",
        "                    generations=self.generations,\n",
        "                    tournament_size=self.tournament_size,\n",
        "                    stopping_criteria=self.stopping_criteria,\n",
        "                    const_range=self.const_range,\n",
        "                    init_depth=self.init_depth,\n",
        "                    function_set=['add', 'sub', 'mul', safe_div_func, tanh_function],\n",
        "                    parsimony_coefficient=0.01,\n",
        "                    random_state=self.random_state + i + 1000,\n",
        "                    verbose=0,\n",
        "                    n_jobs=1\n",
        "                )\n",
        "\n",
        "                regressor.fit(X_train, y_train)\n",
        "\n",
        "                factor_name = f'genetic_ordinal_{i+1}'\n",
        "                self.trained_models[factor_name] = regressor\n",
        "                self.factor_expressions[factor_name] = str(regressor._program)\n",
        "\n",
        "                factor_values = regressor.predict(X_train)\n",
        "\n",
        "                y_min, y_max = y_train.min(), y_train.max()\n",
        "                factor_min, factor_max = factor_values.min(), factor_values.max()\n",
        "                if factor_max > factor_min:\n",
        "                    factor_values = (factor_values - factor_min) / (factor_max - factor_min) * (y_max - y_min) + y_min\n",
        "\n",
        "                corr = np.corrcoef(factor_values, y_train)[0, 1]\n",
        "                print(f\"  {factor_name} training correlation: {corr:.4f}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"  Ordinal factor {i+1} training failed: {e}\")\n",
        "                continue\n",
        "\n",
        "    def train_factors(self, train_df, feature_cols, target_col, n_factors=10):\n",
        "        print(\"=\" * 60)\n",
        "        print(\"Step 1: Training factors on training set\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        X_train, y_train = self.prepare_data(train_df, feature_cols, target_col)\n",
        "        y_processed, task_type = self.prepare_classification_target(y_train)\n",
        "\n",
        "        if task_type == 'binary':\n",
        "            self.generate_binary_classification_factors(X_train, y_processed, n_factors)\n",
        "        elif task_type == 'multiclass':\n",
        "            multiclass_factors = max(1, n_factors // 2)\n",
        "            ordinal_factors = n_factors - multiclass_factors\n",
        "            self.generate_multiclass_factors(X_train, y_processed, multiclass_factors)\n",
        "            self.generate_ordinal_factors(X_train, y_processed, ordinal_factors)\n",
        "        else:\n",
        "            self.generate_ordinal_factors(X_train, y_processed, n_factors)\n",
        "\n",
        "        print(f\"\\nTraining completed! Successfully trained {len(self.trained_models)} factors\")\n",
        "        return task_type\n",
        "\n",
        "    def apply_factors_to_full_data(self, full_df, feature_cols, target_col, task_type):\n",
        "        print(\"Step 2: Applying factors to full dataset\")\n",
        "\n",
        "        X_full, y_full = self.prepare_data(full_df, feature_cols, target_col)\n",
        "        y_processed, _ = self.prepare_classification_target(y_full)\n",
        "\n",
        "        for factor_name, model in self.trained_models.items():\n",
        "            try:\n",
        "                factor_values = self.apply_factor_to_data(X_full, model, factor_name, task_type)\n",
        "                self.generated_factors[factor_name] = factor_values\n",
        "\n",
        "                if task_type == 'binary' and 'binary' in factor_name:\n",
        "                    try:\n",
        "                        auc = roc_auc_score(y_processed, factor_values)\n",
        "                        print(f\"  {factor_name} full set AUC: {auc:.4f}\")\n",
        "                    except:\n",
        "                        corr = np.corrcoef(factor_values, y_processed)[0, 1]\n",
        "                        print(f\"  {factor_name} full set correlation: {corr:.4f}\")\n",
        "                else:\n",
        "                    corr = np.corrcoef(factor_values, y_processed)[0, 1]\n",
        "                    print(f\"  {factor_name} full set correlation: {corr:.4f}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"  Failed to apply factor {factor_name} to full set: {e}\")\n",
        "                continue\n",
        "\n",
        "        print(f\"\\nFactor application completed! Successfully generated {len(self.generated_factors)} factors on full dataset\")\n",
        "\n",
        "    def generate_classification_factors(self, df, feature_cols, target_col, n_factors=10):\n",
        "\n",
        "        train_df, full_df = self.split_data(df)\n",
        "\n",
        "        task_type = self.train_factors(train_df, feature_cols, target_col, n_factors)\n",
        "\n",
        "        self.apply_factors_to_full_data(full_df, feature_cols, target_col, task_type)\n",
        "\n",
        "        return self.generated_factors\n",
        "\n",
        "    def get_factor_dataframe(self, original_df, id_col=None, time_col=None):\n",
        "\n",
        "        if not self.generated_factors:\n",
        "            return None\n",
        "\n",
        "        factor_df = pd.DataFrame(self.generated_factors, index=original_df.index)\n",
        "\n",
        "        if id_col:\n",
        "            for col in id_col if isinstance(id_col, list) else [id_col]:\n",
        "                if col in original_df.columns:\n",
        "                    factor_df[col] = original_df[col]\n",
        "\n",
        "        if time_col:\n",
        "            for col in time_col if isinstance(time_col, list) else [time_col]:\n",
        "                if col in original_df.columns:\n",
        "                    factor_df[col] = original_df[col]\n",
        "\n",
        "        return factor_df\n",
        "\n",
        "    def print_factor_expressions(self):\n",
        "        if not self.factor_expressions:\n",
        "            print(\"No factor expressions generated\")\n",
        "            return\n",
        "\n",
        "        print(\"Factor expressions:\")\n",
        "\n",
        "        for factor_name, expression in self.factor_expressions.items():\n",
        "            print(f\"{factor_name}: {expression}\")\n",
        "\n",
        "    def evaluate_factors_on_splits(self, df, feature_cols, target_col):\n",
        "        if not self.generated_factors:\n",
        "            print(\"No factors generated for evaluation\")\n",
        "            return\n",
        "\n",
        "        print(\"\\n\" + \"=\" * 60)\n",
        "        print(\"Factor performance evaluation (training set vs full set)\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        train_df, full_df = self.split_data(df)\n",
        "\n",
        "        X_train, y_train = self.prepare_data(train_df, feature_cols, target_col)\n",
        "        y_train_processed, task_type = self.prepare_classification_target(y_train)\n",
        "\n",
        "        X_full, y_full = self.prepare_data(full_df, feature_cols, target_col)\n",
        "        y_full_processed, _ = self.prepare_classification_target(y_full)\n",
        "\n",
        "        for factor_name, model in self.trained_models.items():\n",
        "            try:\n",
        "                train_factor_values = self.apply_factor_to_data(X_train, model, factor_name, task_type)\n",
        "\n",
        "                full_factor_values = self.generated_factors[factor_name]\n",
        "\n",
        "                if task_type == 'binary' and 'binary' in factor_name:\n",
        "                    try:\n",
        "                        train_auc = roc_auc_score(y_train_processed, train_factor_values)\n",
        "                        full_auc = roc_auc_score(y_full_processed, full_factor_values)\n",
        "                        print(f\"{factor_name}:\")\n",
        "                        print(f\"  Training set AUC: {train_auc:.4f}\")\n",
        "                        print(f\"  Full set AUC: {full_auc:.4f}\")\n",
        "                        print(f\"  Performance difference: {abs(full_auc - train_auc):.4f}\")\n",
        "                    except:\n",
        "                        train_corr = np.corrcoef(train_factor_values, y_train_processed)[0, 1]\n",
        "                        full_corr = np.corrcoef(full_factor_values, y_full_processed)[0, 1]\n",
        "                        print(f\"{factor_name}:\")\n",
        "                        print(f\"  Training set correlation: {train_corr:.4f}\")\n",
        "                        print(f\"  Full set correlation: {full_corr:.4f}\")\n",
        "                        print(f\"  Performance difference: {abs(full_corr - train_corr):.4f}\")\n",
        "                else:\n",
        "                    train_corr = np.corrcoef(train_factor_values, y_train_processed)[0, 1]\n",
        "                    full_corr = np.corrcoef(full_factor_values, y_full_processed)[0, 1]\n",
        "                    print(f\"{factor_name}:\")\n",
        "                    print(f\"  Training set correlation: {train_corr:.4f}\")\n",
        "                    print(f\"  Full set correlation: {full_corr:.4f}\")\n",
        "                    print(f\"  Performance difference: {abs(full_corr - train_corr):.4f}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"{factor_name}: Evaluation failed - {e}\")\n",
        "\n",
        "def run_classification_genetic_generation(df, feature_cols, target_col, n_factors=10, train_ratio=0.7):\n",
        "    generator = ClassificationGeneticFactorGenerator(\n",
        "        population_size=300,\n",
        "        generations=8,\n",
        "        tournament_size=15,\n",
        "        random_state=42,\n",
        "        train_ratio=train_ratio\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        print(f\"Starting to generate {n_factors} classification factors...\")\n",
        "        print(f\"Using first {train_ratio*100:.0f}% of data for training, then applying to full dataset\")\n",
        "\n",
        "        generated_factors = generator.generate_classification_factors(\n",
        "            df, feature_cols, target_col, n_factors=n_factors\n",
        "        )\n",
        "\n",
        "        if not generated_factors:\n",
        "            print(\"Factor generation failed\")\n",
        "            return None, None\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Generation process failed: {e}\")\n",
        "        return None, None\n",
        "\n",
        "    factor_df = generator.get_factor_dataframe(df)\n",
        "\n",
        "    generator.print_factor_expressions()\n",
        "\n",
        "    generator.evaluate_factors_on_splits(df, feature_cols, target_col)\n",
        "\n",
        "    return generator, factor_df"
      ],
      "metadata": {
        "id": "HvRfn6luXz9D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generator, new_factors = run_classification_genetic_generation(\n",
        "    stock_data[all_features + [\"pred_ret\"]+[\"MthRet\"]],\n",
        "    all_features +[\"MthRet\"],\n",
        "    target_col='pred_ret',\n",
        "    n_factors=10\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AqdJtaQP89KL",
        "outputId": "a53b27b7-510e-4b23-f41d-092a223951a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "开始生成 10 个分类因子...\n",
            "使用前 70% 数据进行训练，然后应用到全数据集\n",
            "训练集大小: 1065615 行 (70%)\n",
            "全数据集大小: 1522308 行 (100%)\n",
            "============================================================\n",
            "第一步: 在训练集上训练因子\n",
            "============================================================\n",
            "生成 10 个有序因子...\n",
            "  genetic_ordinal_1 训练集相关性: 0.0040\n",
            "  genetic_ordinal_2 训练集相关性: nan\n",
            "  genetic_ordinal_3 训练集相关性: 0.0040\n",
            "  genetic_ordinal_4 训练集相关性: 0.0040\n",
            "  genetic_ordinal_5 训练集相关性: 0.0040\n",
            "  genetic_ordinal_6 训练集相关性: 0.0040\n",
            "  genetic_ordinal_7 训练集相关性: 0.0040\n",
            "  genetic_ordinal_8 训练集相关性: 0.0040\n",
            "  genetic_ordinal_9 训练集相关性: nan\n",
            "  genetic_ordinal_10 训练集相关性: 0.0024\n",
            "\n",
            "训练完成! 成功训练了 10 个因子\n",
            "第二步: 将因子应用到全数据集\n",
            "  genetic_ordinal_1 全集相关性: 0.0014\n",
            "  genetic_ordinal_2 全集相关性: nan\n",
            "  genetic_ordinal_3 全集相关性: 0.0014\n",
            "  genetic_ordinal_4 全集相关性: 0.0014\n",
            "  genetic_ordinal_5 全集相关性: 0.0014\n",
            "  genetic_ordinal_6 全集相关性: 0.0014\n",
            "  genetic_ordinal_7 全集相关性: 0.0014\n",
            "  genetic_ordinal_8 全集相关性: 0.0014\n",
            "  genetic_ordinal_9 全集相关性: nan\n",
            "  genetic_ordinal_10 全集相关性: 0.0001\n",
            "\n",
            "因子应用完成! 成功生成了 10 个因子到全数据集\n",
            "因子表达式:\n",
            "genetic_ordinal_1: tanh(tanh(X25))\n",
            "genetic_ordinal_2: sub(X28, X28)\n",
            "genetic_ordinal_3: tanh(tanh(X25))\n",
            "genetic_ordinal_4: tanh(tanh(X25))\n",
            "genetic_ordinal_5: tanh(tanh(X25))\n",
            "genetic_ordinal_6: tanh(tanh(X25))\n",
            "genetic_ordinal_7: tanh(tanh(X25))\n",
            "genetic_ordinal_8: tanh(tanh(X25))\n",
            "genetic_ordinal_9: -0.011\n",
            "genetic_ordinal_10: mul(X28, mul(X28, X25))\n",
            "\n",
            "============================================================\n",
            "因子性能评估对比 (训练集 vs 全集)\n",
            "============================================================\n",
            "训练集大小: 1065615 行 (70%)\n",
            "全数据集大小: 1522308 行 (100%)\n",
            "genetic_ordinal_1:\n",
            "  训练集相关性: 0.0040\n",
            "  全集相关性: 0.0014\n",
            "  性能差异: 0.0025\n",
            "genetic_ordinal_2:\n",
            "  训练集相关性: nan\n",
            "  全集相关性: nan\n",
            "  性能差异: nan\n",
            "genetic_ordinal_3:\n",
            "  训练集相关性: 0.0040\n",
            "  全集相关性: 0.0014\n",
            "  性能差异: 0.0025\n",
            "genetic_ordinal_4:\n",
            "  训练集相关性: 0.0040\n",
            "  全集相关性: 0.0014\n",
            "  性能差异: 0.0025\n",
            "genetic_ordinal_5:\n",
            "  训练集相关性: 0.0040\n",
            "  全集相关性: 0.0014\n",
            "  性能差异: 0.0025\n",
            "genetic_ordinal_6:\n",
            "  训练集相关性: 0.0040\n",
            "  全集相关性: 0.0014\n",
            "  性能差异: 0.0025\n",
            "genetic_ordinal_7:\n",
            "  训练集相关性: 0.0040\n",
            "  全集相关性: 0.0014\n",
            "  性能差异: 0.0025\n",
            "genetic_ordinal_8:\n",
            "  训练集相关性: 0.0040\n",
            "  全集相关性: 0.0014\n",
            "  性能差异: 0.0025\n",
            "genetic_ordinal_9:\n",
            "  训练集相关性: nan\n",
            "  全集相关性: nan\n",
            "  性能差异: nan\n",
            "genetic_ordinal_10:\n",
            "  训练集相关性: 0.0024\n",
            "  全集相关性: 0.0001\n",
            "  性能差异: 0.0024\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "constant_cols = [col for col in new_factors.columns if new_factors[col].nunique() <= 1]\n",
        "new_factors.drop(columns=constant_cols, inplace=True)"
      ],
      "metadata": {
        "id": "F94-ztMxPoyy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "uni_col = []\n",
        "for i in new_factors.columns:\n",
        "  if uni_col == []:\n",
        "    uni_col.append(i)\n",
        "  for j in uni_col:\n",
        "    if np.all(new_factors[i].values == new_factors[j].values):\n",
        "      break\n",
        "    uni_col.append(i)\n",
        "gp_factor = new_factors[uni_col].apply(lambda x: (x-x.mean())/x.std())"
      ],
      "metadata": {
        "id": "cwxHGtAyCEYY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FastRandomForestFactorGenerator:\n",
        "    def __init__(self, n_estimators=200, random_state=42, n_jobs=-1, train_ratio=0.7):\n",
        "        self.n_estimators = n_estimators\n",
        "        self.random_state = random_state\n",
        "        self.n_jobs = n_jobs\n",
        "        self.train_ratio = train_ratio\n",
        "\n",
        "        self.generated_factors = {}\n",
        "        self.factor_info = {}\n",
        "        self.rf_models = {}\n",
        "        self.label_encoder = None\n",
        "\n",
        "        self._X_train_processed = None\n",
        "        self._y_train_processed = None\n",
        "        self._task_type = None\n",
        "\n",
        "    def split_data(self, df):\n",
        "        n_train = int(len(df) * self.train_ratio)\n",
        "        train_df = df.iloc[:n_train].copy()\n",
        "        full_df = df.copy()\n",
        "\n",
        "        print(f\"Training set size: {len(train_df)} rows ({self.train_ratio*100:.0f}%)\")\n",
        "        print(f\"Full dataset size: {len(full_df)} rows (100%)\")\n",
        "\n",
        "        return train_df, full_df\n",
        "\n",
        "    def prepare_data(self, df, feature_cols, target_col=None, is_training=True):\n",
        "        print(\"Processing data...\")\n",
        "        X = df[feature_cols].copy()\n",
        "\n",
        "        numeric_cols = X.select_dtypes(include=[np.number]).columns\n",
        "        X[numeric_cols] = X[numeric_cols].fillna(X[numeric_cols].median())\n",
        "        X[numeric_cols] = X[numeric_cols].replace([np.inf, -np.inf], np.nan)\n",
        "        X[numeric_cols] = X[numeric_cols].fillna(X[numeric_cols].median())\n",
        "\n",
        "        if target_col is not None:\n",
        "            y = df[target_col].copy()\n",
        "\n",
        "            if is_training:\n",
        "                if y.dtype == 'object' or len(np.unique(y)) <= 10:\n",
        "                    if y.dtype == 'object':\n",
        "                        self.label_encoder = LabelEncoder()\n",
        "                        y = self.label_encoder.fit_transform(y)\n",
        "                    else:\n",
        "                        self.label_encoder = LabelEncoder()\n",
        "                        y = self.label_encoder.fit_transform(y)\n",
        "                    self._task_type = 'classification'\n",
        "                else:\n",
        "                    y = y.fillna(y.median())\n",
        "                    y = y.replace([np.inf, -np.inf], np.nan).fillna(y.median())\n",
        "                    self._task_type = 'regression'\n",
        "\n",
        "                self._X_train_processed = X\n",
        "                self._y_train_processed = y\n",
        "            else:\n",
        "                if self._task_type == 'classification' and self.label_encoder is not None:\n",
        "                    try:\n",
        "                        y = self.label_encoder.transform(y)\n",
        "                    except ValueError:\n",
        "                        known_classes = set(self.label_encoder.classes_)\n",
        "                        y = y.apply(lambda x: x if x in known_classes else self.label_encoder.classes_[0])\n",
        "                        y = self.label_encoder.transform(y)\n",
        "                else:\n",
        "                    y = y.fillna(y.median())\n",
        "                    y = y.replace([np.inf, -np.inf], np.nan).fillna(y.median())\n",
        "\n",
        "            return X, y\n",
        "\n",
        "        return X\n",
        "\n",
        "    def _train_main_model(self, X_train, y_train):\n",
        "        print(f\"Training main model for {self._task_type}...\")\n",
        "\n",
        "        if self._task_type == 'classification':\n",
        "            rf = RandomForestClassifier(\n",
        "                n_estimators=self.n_estimators,\n",
        "                random_state=self.random_state,\n",
        "                n_jobs=self.n_jobs,\n",
        "                max_depth=8,\n",
        "                min_samples_split=50\n",
        "            )\n",
        "        else:\n",
        "            rf = RandomForestRegressor(\n",
        "                n_estimators=self.n_estimators,\n",
        "                random_state=self.random_state,\n",
        "                n_jobs=self.n_jobs,\n",
        "                max_depth=8,\n",
        "                min_samples_split=50\n",
        "            )\n",
        "\n",
        "        rf.fit(X_train, y_train)\n",
        "        self.rf_models['main'] = rf\n",
        "        self.rf_models['task_type'] = self._task_type\n",
        "\n",
        "        print(f\"Main model trained ({self._task_type})\")\n",
        "        return rf\n",
        "\n",
        "    def generate_tree_prediction_factors(self, X_train, y_train, n_factors=10):\n",
        "        print(f\"Generating {n_factors} tree prediction factors...\")\n",
        "\n",
        "        rf = self.rf_models.get('main')\n",
        "        if rf is None:\n",
        "            rf = self._train_main_model(X_train, y_train)\n",
        "\n",
        "        n_factors = min(n_factors, len(rf.estimators_))\n",
        "\n",
        "        for i in range(n_factors):\n",
        "            factor_name = f'rf_tree_pred_{i+1}'\n",
        "            self.factor_info[factor_name] = {\n",
        "                'type': 'tree_prediction',\n",
        "                'tree_index': i,\n",
        "                'task_type': self._task_type\n",
        "            }\n",
        "\n",
        "        print(f\"Configured {n_factors} tree prediction factors\")\n",
        "\n",
        "    def generate_leaf_index_factors(self, X_train, y_train, n_factors=8):\n",
        "        print(f\"Generating {n_factors} leaf index factors...\")\n",
        "\n",
        "        rf = self.rf_models.get('main')\n",
        "        if rf is None:\n",
        "            rf = self._train_main_model(X_train, y_train)\n",
        "\n",
        "        leaf_indices = rf.apply(X_train)\n",
        "        n_factors = min(n_factors, leaf_indices.shape[1])\n",
        "\n",
        "        for i in range(n_factors):\n",
        "            factor_name = f'rf_leaf_idx_{i+1}'\n",
        "            self.factor_info[factor_name] = {\n",
        "                'type': 'leaf_index',\n",
        "                'tree_index': i\n",
        "            }\n",
        "\n",
        "        print(f\"Configured {n_factors} leaf index factors\")\n",
        "\n",
        "    def generate_feature_importance_factors(self, X_train, y_train):\n",
        "        print(\"Generating feature importance factors...\")\n",
        "\n",
        "        rf = self.rf_models.get('main')\n",
        "        if rf is None:\n",
        "            rf = self._train_main_model(X_train, y_train)\n",
        "\n",
        "        main_importance = rf.feature_importances_\n",
        "        self.rf_models['main_importance'] = main_importance\n",
        "\n",
        "        importance_configs = ['main', 'squared', 'log']\n",
        "        for config in importance_configs:\n",
        "            factor_name = f'rf_importance_{config}'\n",
        "            self.factor_info[factor_name] = {\n",
        "                'type': 'feature_importance',\n",
        "                'config': config,\n",
        "                'top_features': list(X_train.columns[np.argsort(main_importance)[-5:][::-1]])\n",
        "            }\n",
        "\n",
        "        print(\"Configured 3 feature importance factors\")\n",
        "\n",
        "    def generate_bootstrap_factors(self, X_train, y_train, n_factors=6):\n",
        "        print(f\"Generating {n_factors} bootstrap factors...\")\n",
        "\n",
        "        rf = self.rf_models.get('main')\n",
        "        if rf is None:\n",
        "            rf = self._train_main_model(X_train, y_train)\n",
        "\n",
        "        for i in range(n_factors):\n",
        "            factor_name = f'rf_bootstrap_{i+1}'\n",
        "            self.factor_info[factor_name] = {\n",
        "                'type': 'bootstrap',\n",
        "                'bootstrap_seed': self.random_state + i,\n",
        "                'sample_ratio': 0.7\n",
        "            }\n",
        "\n",
        "        print(f\"Configured {n_factors} bootstrap factors\")\n",
        "\n",
        "    def generate_tree_depth_factors(self, X_train, y_train, n_factors=4):\n",
        "        print(f\"Generating {n_factors} tree depth factors...\")\n",
        "\n",
        "        rf = self.rf_models.get('main')\n",
        "        if rf is None:\n",
        "            rf = self._train_main_model(X_train, y_train)\n",
        "\n",
        "        leaf_indices = rf.apply(X_train)\n",
        "        n_factors = min(n_factors, leaf_indices.shape[1])\n",
        "\n",
        "        for i in range(n_factors):\n",
        "            factor_name = f'rf_depth_{i+1}'\n",
        "            self.factor_info[factor_name] = {\n",
        "                'type': 'tree_depth',\n",
        "                'tree_index': i\n",
        "            }\n",
        "\n",
        "        print(f\"Configured {n_factors} tree depth factors\")\n",
        "\n",
        "    def train_factors(self, train_df, feature_cols, target_col):\n",
        "        print(\"=\" * 60)\n",
        "        print(\"Step 1: Training random forest factors on training set\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        X_train, y_train = self.prepare_data(train_df, feature_cols, target_col, is_training=True)\n",
        "\n",
        "        self._train_main_model(X_train, y_train)\n",
        "\n",
        "        self.generate_tree_prediction_factors(X_train, y_train, n_factors=10)\n",
        "        self.generate_leaf_index_factors(X_train, y_train, n_factors=8)\n",
        "        self.generate_feature_importance_factors(X_train, y_train)\n",
        "        self.generate_bootstrap_factors(X_train, y_train, n_factors=6)\n",
        "        self.generate_tree_depth_factors(X_train, y_train, n_factors=4)\n",
        "\n",
        "        print(f\"\\nTraining completed! Successfully configured {len(self.factor_info)} factors\")\n",
        "        return self._task_type\n",
        "\n",
        "    def apply_factors_to_full_data(self, full_df, feature_cols, target_col, task_type):\n",
        "        print(\"\\n\" + \"=\" * 60)\n",
        "        print(\"Step 2: Applying factors to full dataset\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        X_full, y_full = self.prepare_data(full_df, feature_cols, target_col, is_training=False)\n",
        "        rf = self.rf_models['main']\n",
        "\n",
        "        for factor_name, info in self.factor_info.items():\n",
        "            try:\n",
        "                factor_type = info['type']\n",
        "\n",
        "                if factor_type == 'tree_prediction':\n",
        "                    tree_idx = info['tree_index']\n",
        "                    if task_type == 'classification':\n",
        "                        tree_probs = rf.estimators_[tree_idx].predict_proba(X_full)\n",
        "                        if tree_probs.shape[1] > 1:\n",
        "                            factor_values = tree_probs[:, 1]\n",
        "                        else:\n",
        "                            factor_values = tree_probs[:, 0]\n",
        "                    else:\n",
        "                        factor_values = rf.estimators_[tree_idx].predict(X_full)\n",
        "\n",
        "                elif factor_type == 'leaf_index':\n",
        "                    tree_idx = info['tree_index']\n",
        "                    leaf_indices = rf.apply(X_full)\n",
        "                    factor_values = leaf_indices[:, tree_idx].astype(float)\n",
        "\n",
        "                elif factor_type == 'feature_importance':\n",
        "                    config = info['config']\n",
        "                    main_importance = self.rf_models['main_importance']\n",
        "\n",
        "                    if config == 'main':\n",
        "                        weighted_importance = main_importance\n",
        "                    elif config == 'squared':\n",
        "                        weighted_importance = main_importance ** 2\n",
        "                        weighted_importance = weighted_importance / weighted_importance.sum()\n",
        "                    elif config == 'log':\n",
        "                        weighted_importance = np.log(main_importance + 1e-8)\n",
        "                        weighted_importance = (weighted_importance - weighted_importance.min()) / (weighted_importance.max() - weighted_importance.min() + 1e-8)\n",
        "\n",
        "                    factor_values = np.dot(X_full, weighted_importance)\n",
        "\n",
        "                elif factor_type == 'bootstrap':\n",
        "                    bootstrap_seed = info['bootstrap_seed']\n",
        "                    sample_ratio = info['sample_ratio']\n",
        "\n",
        "                    np.random.seed(bootstrap_seed)\n",
        "                    sample_size = int(sample_ratio * len(X_full))\n",
        "                    sample_idx = np.random.choice(len(X_full), size=sample_size, replace=True)\n",
        "\n",
        "                    if task_type == 'classification':\n",
        "                        base_predictions = rf.predict_proba(X_full)\n",
        "                        if base_predictions.shape[1] > 1:\n",
        "                            base_predictions = base_predictions[:, 1]\n",
        "                        else:\n",
        "                            base_predictions = base_predictions[:, 0]\n",
        "                    else:\n",
        "                        base_predictions = rf.predict(X_full)\n",
        "\n",
        "                    base_predictions = base_predictions.astype(np.float64)\n",
        "\n",
        "                    X_sample = X_full.iloc[sample_idx] if hasattr(X_full, 'iloc') else X_full[sample_idx]\n",
        "                    if task_type == 'classification':\n",
        "                        sample_predictions = rf.predict_proba(X_sample)\n",
        "                        if sample_predictions.shape[1] > 1:\n",
        "                            sample_predictions = sample_predictions[:, 1]\n",
        "                        else:\n",
        "                            sample_predictions = sample_predictions[:, 0]\n",
        "                    else:\n",
        "                        sample_predictions = rf.predict(X_sample)\n",
        "\n",
        "                    sample_predictions = sample_predictions.astype(np.float64)\n",
        "\n",
        "                    factor_values = base_predictions.copy()\n",
        "                    factor_values[sample_idx] = sample_predictions\n",
        "\n",
        "                    unsampled_mask = np.ones(len(X_full), dtype=bool)\n",
        "                    unsampled_mask[sample_idx] = False\n",
        "                    bootstrap_idx = int(factor_name.split('_')[-1]) - 1\n",
        "                    factor_values[unsampled_mask] *= (0.95 + bootstrap_idx * 0.01)\n",
        "\n",
        "                elif factor_type == 'tree_depth':\n",
        "                    tree_idx = info['tree_index']\n",
        "                    leaf_indices = rf.apply(X_full)\n",
        "                    depths = leaf_indices[:, tree_idx].astype(float)\n",
        "                    factor_values = (depths - depths.min()) / (depths.max() - depths.min() + 1e-8)\n",
        "\n",
        "                factor_values = factor_values.astype(np.float64)\n",
        "                self.generated_factors[factor_name] = factor_values\n",
        "\n",
        "                try:\n",
        "                    if task_type == 'classification' and 'tree_pred' in factor_name:\n",
        "                        auc = roc_auc_score(y_full, factor_values)\n",
        "                        print(f\"  {factor_name} full set AUC: {auc:.4f}\")\n",
        "                    else:\n",
        "                        corr = np.corrcoef(factor_values, y_full)[0, 1]\n",
        "                        if not np.isnan(corr):\n",
        "                            print(f\"  {factor_name} full set correlation: {corr:.4f}\")\n",
        "                except:\n",
        "                    print(f\"  {factor_name} generated\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"  Factor {factor_name} application failed: {e}\")\n",
        "                continue\n",
        "\n",
        "        print(f\"\\nFactor application completed! Successfully generated {len(self.generated_factors)} factors on full dataset\")\n",
        "\n",
        "    def generate_all_rf_factors(self, df, feature_cols, target_col):\n",
        "        print(\"=\" * 60)\n",
        "        print(\"Starting FAST Random Forest Factor Generation\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        train_df, full_df = self.split_data(df)\n",
        "\n",
        "        task_type = self.train_factors(train_df, feature_cols, target_col)\n",
        "\n",
        "        self.apply_factors_to_full_data(full_df, feature_cols, target_col, task_type)\n",
        "\n",
        "        print(\"=\" * 60)\n",
        "        print(f\"Generated {len(self.generated_factors)} factors successfully!\")\n",
        "        print(f\"Models trained: 1 (vs ~3-5 in slow version)\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        return self.generated_factors\n",
        "\n",
        "    def get_factor_dataframe(self, original_df):\n",
        "        if not self.generated_factors:\n",
        "            print(\"No factors were generated\")\n",
        "            return None\n",
        "\n",
        "        factor_df = pd.DataFrame(self.generated_factors, index=original_df.index)\n",
        "        return factor_df\n",
        "\n",
        "    def get_factor_summary(self):\n",
        "        if not self.generated_factors:\n",
        "            return None\n",
        "\n",
        "        summary_data = []\n",
        "        for factor_name, factor_values in self.generated_factors.items():\n",
        "            info = self.factor_info.get(factor_name, {})\n",
        "            summary_data.append({\n",
        "                'factor_name': factor_name,\n",
        "                'type': info.get('type', 'unknown'),\n",
        "                'mean': np.mean(factor_values),\n",
        "                'std': np.std(factor_values),\n",
        "                'min': np.min(factor_values),\n",
        "                'max': np.max(factor_values),\n",
        "                'unique_values': len(np.unique(factor_values)),\n",
        "                'info': str(info)\n",
        "            })\n",
        "\n",
        "        summary_df = pd.DataFrame(summary_data)\n",
        "        return summary_df\n",
        "\n",
        "    def evaluate_factors_on_splits(self, df, feature_cols, target_col):\n",
        "        if not self.generated_factors:\n",
        "            print(\"No factors generated for evaluation\")\n",
        "            return\n",
        "\n",
        "        print(\"\\n\" + \"=\" * 60)\n",
        "        print(\"Factor performance evaluation (training set vs full set)\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        train_df, full_df = self.split_data(df)\n",
        "\n",
        "        X_train, y_train = self.prepare_data(train_df, feature_cols, target_col, is_training=True)\n",
        "\n",
        "        X_full, y_full = self.prepare_data(full_df, feature_cols, target_col, is_training=False)\n",
        "\n",
        "        rf = self.rf_models['main']\n",
        "        task_type = self._task_type\n",
        "\n",
        "        for factor_name in self.generated_factors.keys():\n",
        "            try:\n",
        "                info = self.factor_info[factor_name]\n",
        "                factor_type = info['type']\n",
        "\n",
        "                if factor_type == 'tree_prediction':\n",
        "                    tree_idx = info['tree_index']\n",
        "                    if task_type == 'classification':\n",
        "                        tree_probs = rf.estimators_[tree_idx].predict_proba(X_train)\n",
        "                        if tree_probs.shape[1] > 1:\n",
        "                            train_factor_values = tree_probs[:, 1]\n",
        "                        else:\n",
        "                            train_factor_values = tree_probs[:, 0]\n",
        "                    else:\n",
        "                        train_factor_values = rf.estimators_[tree_idx].predict(X_train)\n",
        "\n",
        "                elif factor_type == 'leaf_index':\n",
        "                    tree_idx = info['tree_index']\n",
        "                    leaf_indices = rf.apply(X_train)\n",
        "                    train_factor_values = leaf_indices[:, tree_idx].astype(float)\n",
        "\n",
        "                else:\n",
        "                    train_factor_values = None\n",
        "\n",
        "                full_factor_values = self.generated_factors[factor_name]\n",
        "\n",
        "                if train_factor_values is not None:\n",
        "                    if task_type == 'classification' and 'tree_pred' in factor_name:\n",
        "                        try:\n",
        "                            train_auc = roc_auc_score(y_train, train_factor_values)\n",
        "                            full_auc = roc_auc_score(y_full, full_factor_values)\n",
        "                            print(f\"{factor_name}:\")\n",
        "                            print(f\"  Training set AUC: {train_auc:.4f}\")\n",
        "                            print(f\"  Full set AUC: {full_auc:.4f}\")\n",
        "                            print(f\"  Performance difference: {abs(full_auc - train_auc):.4f}\")\n",
        "                        except:\n",
        "                            train_corr = np.corrcoef(train_factor_values, y_train)[0, 1]\n",
        "                            full_corr = np.corrcoef(full_factor_values, y_full)[0, 1]\n",
        "                            if not (np.isnan(train_corr) or np.isnan(full_corr)):\n",
        "                                print(f\"{factor_name}:\")\n",
        "                                print(f\"  Training set correlation: {train_corr:.4f}\")\n",
        "                                print(f\"  Full set correlation: {full_corr:.4f}\")\n",
        "                                print(f\"  Performance difference: {abs(full_corr - train_corr):.4f}\")\n",
        "                    else:\n",
        "                        train_corr = np.corrcoef(train_factor_values, y_train)[0, 1]\n",
        "                        full_corr = np.corrcoef(full_factor_values, y_full)[0, 1]\n",
        "                        if not (np.isnan(train_corr) or np.isnan(full_corr)):\n",
        "                            print(f\"{factor_name}:\")\n",
        "                            print(f\"  Training set correlation: {train_corr:.4f}\")\n",
        "                            print(f\"  Full set correlation: {full_corr:.4f}\")\n",
        "                            print(f\"  Performance difference: {abs(full_corr - train_corr):.4f}\")\n",
        "                else:\n",
        "                    full_corr = np.corrcoef(full_factor_values, y_full)[0, 1]\n",
        "                    if not np.isnan(full_corr):\n",
        "                        print(f\"{factor_name}: Full set correlation: {full_corr:.4f}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"{factor_name}: Evaluation failed - {e}\")\n",
        "\n",
        "    def evaluate_factors(self, df, target_col):\n",
        "        if not self.generated_factors:\n",
        "            return None\n",
        "\n",
        "        try:\n",
        "            target = df[target_col].fillna(df[target_col].median())\n",
        "            if self.label_encoder is not None and target.dtype == 'object':\n",
        "                target = self.label_encoder.transform(target)\n",
        "        except:\n",
        "            target = df[target_col].fillna(df[target_col].median())\n",
        "\n",
        "        evaluation_results = []\n",
        "\n",
        "        for factor_name, factor_values in self.generated_factors.items():\n",
        "            try:\n",
        "                corr = np.corrcoef(factor_values, target)[0, 1]\n",
        "                if np.isnan(corr):\n",
        "                    corr = 0\n",
        "            except:\n",
        "                corr = 0\n",
        "\n",
        "            factor_std = np.std(factor_values)\n",
        "            unique_ratio = len(np.unique(factor_values)) / len(factor_values)\n",
        "\n",
        "            evaluation_results.append({\n",
        "                'factor_name': factor_name,\n",
        "                'type': self.factor_info.get(factor_name, {}).get('type', 'unknown'),\n",
        "                'correlation': corr,\n",
        "                'abs_correlation': abs(corr),\n",
        "                'std': factor_std,\n",
        "                'unique_ratio': unique_ratio\n",
        "            })\n",
        "\n",
        "        eval_df = pd.DataFrame(evaluation_results)\n",
        "        eval_df = eval_df.sort_values('abs_correlation', ascending=False)\n",
        "\n",
        "        print(\"\\nTop 10 factors by correlation:\")\n",
        "        print(eval_df.head(10)[['factor_name', 'type', 'correlation', 'unique_ratio']])\n",
        "\n",
        "        return eval_df\n",
        "\n",
        "def run_fast_rf_factor_generation(df, target_col, feature_cols=None, n_jobs=-1, train_ratio=0.7):\n",
        "    if feature_cols is None:\n",
        "        feature_cols = [col for col in df.columns if col != target_col]\n",
        "\n",
        "    print(f\"Target column: {target_col}\")\n",
        "    print(f\"Found {len(feature_cols)} feature columns\")\n",
        "    print(f\"Data shape: {df.shape}\")\n",
        "    print(f\"Train ratio: {train_ratio*100:.0f}%\")\n",
        "\n",
        "    generator = FastRandomForestFactorGenerator(\n",
        "        n_estimators=200,\n",
        "        random_state=42,\n",
        "        n_jobs=n_jobs,\n",
        "        train_ratio=train_ratio\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        print(f\"Starting random forest factor generation...\")\n",
        "        print(f\"Using first {train_ratio*100:.0f}% of data for training, then applying to full dataset\")\n",
        "\n",
        "        generated_factors = generator.generate_all_rf_factors(df, feature_cols, target_col)\n",
        "\n",
        "        if not generated_factors:\n",
        "            print(\"Factor generation failed\")\n",
        "            return None, None, None, None\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Generation process failed: {e}\")\n",
        "        return None, None, None, None\n",
        "\n",
        "    factor_df = generator.get_factor_dataframe(df)\n",
        "\n",
        "    summary = generator.get_factor_summary()\n",
        "    if summary is not None:\n",
        "        print(\"\\nFactor types summary:\")\n",
        "        print(summary.groupby('type').size())\n",
        "\n",
        "    generator.evaluate_factors_on_splits(df, feature_cols, target_col)\n",
        "\n",
        "    evaluation_results = generator.evaluate_factors(df, target_col)\n",
        "\n",
        "    return generator, factor_df, summary, evaluation_results"
      ],
      "metadata": {
        "id": "j7o7G_DyQbEh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generator, factor_df, summary, evaluation = run_fast_rf_factor_generation(\n",
        "    stock_data[all_features + ['pred_cat']+[\"MthRet\"]],\n",
        "    target_col='pred_cat'\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uMnOZWFkRwvY",
        "outputId": "ad7a8a3d-8fba-4a17-ff0e-6f54708f9aa7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🎯 Target column: pred_cat\n",
            "📊 Found 29 feature columns\n",
            "📈 Data shape: (1522308, 30)\n",
            "🔄 Train ratio: 70%\n",
            "开始生成随机森林因子...\n",
            "使用前 70% 数据进行训练，然后应用到全数据集\n",
            "============================================================\n",
            "🚀 Starting FAST Random Forest Factor Generation\n",
            "============================================================\n",
            "训练集大小: 1065615 行 (70%)\n",
            "全数据集大小: 1522308 行 (100%)\n",
            "============================================================\n",
            "第一步: 在训练集上训练随机森林因子\n",
            "============================================================\n",
            "📊 Processing data...\n",
            "🎯 Training main model for classification...\n",
            "✅ Main model trained (classification)\n",
            "🌳 Generating 10 tree prediction factors...\n",
            "✅ Configured 10 tree prediction factors\n",
            "🍃 Generating 8 leaf index factors...\n",
            "✅ Configured 8 leaf index factors\n",
            "⚖️ Generating feature importance factors...\n",
            "✅ Configured 3 feature importance factors\n",
            "🎲 Generating 6 bootstrap factors...\n",
            "✅ Configured 6 bootstrap factors\n",
            "📏 Generating 4 tree depth factors...\n",
            "✅ Configured 4 tree depth factors\n",
            "\n",
            "训练完成! 成功配置了 31 个因子\n",
            "\n",
            "============================================================\n",
            "第二步: 将因子应用到全数据集\n",
            "============================================================\n",
            "📊 Processing data...\n",
            "  rf_tree_pred_1 全集 AUC: 0.6714\n",
            "  rf_tree_pred_2 全集 AUC: 0.6566\n",
            "  rf_tree_pred_3 全集 AUC: 0.6573\n",
            "  rf_tree_pred_4 全集 AUC: 0.6268\n",
            "  rf_tree_pred_5 全集 AUC: 0.6780\n",
            "  rf_tree_pred_6 全集 AUC: 0.6541\n",
            "  rf_tree_pred_7 全集 AUC: 0.6511\n",
            "  rf_tree_pred_8 全集 AUC: 0.6503\n",
            "  rf_tree_pred_9 全集 AUC: 0.6668\n",
            "  rf_tree_pred_10 全集 AUC: 0.6564\n",
            "  rf_leaf_idx_1 全集相关性: -0.2302\n",
            "  rf_leaf_idx_2 全集相关性: -0.1481\n",
            "  rf_leaf_idx_3 全集相关性: -0.0091\n",
            "  rf_leaf_idx_4 全集相关性: -0.1436\n",
            "  rf_leaf_idx_5 全集相关性: -0.2605\n",
            "  rf_leaf_idx_6 全集相关性: -0.1627\n",
            "  rf_leaf_idx_7 全集相关性: -0.2522\n",
            "  rf_leaf_idx_8 全集相关性: -0.1769\n",
            "  rf_importance_main 全集相关性: -0.2796\n",
            "  rf_importance_squared 全集相关性: -0.2797\n",
            "  rf_importance_log 全集相关性: -0.2291\n",
            "  rf_bootstrap_1 全集相关性: 0.3382\n",
            "  rf_bootstrap_2 全集相关性: 0.3387\n",
            "  rf_bootstrap_3 全集相关性: 0.3394\n",
            "  rf_bootstrap_4 全集相关性: 0.3396\n",
            "  rf_bootstrap_5 全集相关性: 0.3399\n",
            "  rf_bootstrap_6 全集相关性: 0.3399\n",
            "  rf_depth_1 全集相关性: -0.2302\n",
            "  rf_depth_2 全集相关性: -0.1481\n",
            "  rf_depth_3 全集相关性: -0.0091\n",
            "  rf_depth_4 全集相关性: -0.1436\n",
            "\n",
            "因子应用完成! 成功生成了 31 个因子到全数据集\n",
            "============================================================\n",
            "🎉 Generated 31 factors successfully!\n",
            "⚡ Models trained: 1 (vs ~3-5 in slow version)\n",
            "============================================================\n",
            "\n",
            "📊 Factor types summary:\n",
            "type\n",
            "bootstrap              6\n",
            "feature_importance     3\n",
            "leaf_index             8\n",
            "tree_depth             4\n",
            "tree_prediction       10\n",
            "dtype: int64\n",
            "\n",
            "============================================================\n",
            "因子性能评估对比 (训练集 vs 全集)\n",
            "============================================================\n",
            "训练集大小: 1065615 行 (70%)\n",
            "全数据集大小: 1522308 行 (100%)\n",
            "📊 Processing data...\n",
            "📊 Processing data...\n",
            "rf_tree_pred_1:\n",
            "  训练集 AUC: 0.6862\n",
            "  全集 AUC: 0.6714\n",
            "  性能差异: 0.0149\n",
            "rf_tree_pred_2:\n",
            "  训练集 AUC: 0.6820\n",
            "  全集 AUC: 0.6566\n",
            "  性能差异: 0.0255\n",
            "rf_tree_pred_3:\n",
            "  训练集 AUC: 0.6835\n",
            "  全集 AUC: 0.6573\n",
            "  性能差异: 0.0262\n",
            "rf_tree_pred_4:\n",
            "  训练集 AUC: 0.6596\n",
            "  全集 AUC: 0.6268\n",
            "  性能差异: 0.0328\n",
            "rf_tree_pred_5:\n",
            "  训练集 AUC: 0.6870\n",
            "  全集 AUC: 0.6780\n",
            "  性能差异: 0.0091\n",
            "rf_tree_pred_6:\n",
            "  训练集 AUC: 0.6908\n",
            "  全集 AUC: 0.6541\n",
            "  性能差异: 0.0366\n",
            "rf_tree_pred_7:\n",
            "  训练集 AUC: 0.6789\n",
            "  全集 AUC: 0.6511\n",
            "  性能差异: 0.0278\n",
            "rf_tree_pred_8:\n",
            "  训练集 AUC: 0.6781\n",
            "  全集 AUC: 0.6503\n",
            "  性能差异: 0.0278\n",
            "rf_tree_pred_9:\n",
            "  训练集 AUC: 0.6704\n",
            "  全集 AUC: 0.6668\n",
            "  性能差异: 0.0036\n",
            "rf_tree_pred_10:\n",
            "  训练集 AUC: 0.6774\n",
            "  全集 AUC: 0.6564\n",
            "  性能差异: 0.0210\n",
            "rf_leaf_idx_1:\n",
            "  训练集相关性: -0.2390\n",
            "  全集相关性: -0.2302\n",
            "  性能差异: 0.0088\n",
            "rf_leaf_idx_2:\n",
            "  训练集相关性: -0.1749\n",
            "  全集相关性: -0.1481\n",
            "  性能差异: 0.0268\n",
            "rf_leaf_idx_3:\n",
            "  训练集相关性: -0.0382\n",
            "  全集相关性: -0.0091\n",
            "  性能差异: 0.0291\n",
            "rf_leaf_idx_4:\n",
            "  训练集相关性: -0.1631\n",
            "  全集相关性: -0.1436\n",
            "  性能差异: 0.0194\n",
            "rf_leaf_idx_5:\n",
            "  训练集相关性: -0.2729\n",
            "  全集相关性: -0.2605\n",
            "  性能差异: 0.0124\n",
            "rf_leaf_idx_6:\n",
            "  训练集相关性: -0.1502\n",
            "  全集相关性: -0.1627\n",
            "  性能差异: 0.0125\n",
            "rf_leaf_idx_7:\n",
            "  训练集相关性: -0.2544\n",
            "  全集相关性: -0.2522\n",
            "  性能差异: 0.0022\n",
            "rf_leaf_idx_8:\n",
            "  训练集相关性: -0.1973\n",
            "  全集相关性: -0.1769\n",
            "  性能差异: 0.0204\n",
            "rf_importance_main: 全集相关性: -0.2796\n",
            "rf_importance_squared: 全集相关性: -0.2797\n",
            "rf_importance_log: 全集相关性: -0.2291\n",
            "rf_bootstrap_1: 全集相关性: 0.3382\n",
            "rf_bootstrap_2: 全集相关性: 0.3387\n",
            "rf_bootstrap_3: 全集相关性: 0.3394\n",
            "rf_bootstrap_4: 全集相关性: 0.3396\n",
            "rf_bootstrap_5: 全集相关性: 0.3399\n",
            "rf_bootstrap_6: 全集相关性: 0.3399\n",
            "rf_depth_1: 全集相关性: -0.2302\n",
            "rf_depth_2: 全集相关性: -0.1481\n",
            "rf_depth_3: 全集相关性: -0.0091\n",
            "rf_depth_4: 全集相关性: -0.1436\n",
            "\n",
            "📈 Top 10 factors by correlation:\n",
            "       factor_name             type  correlation  unique_ratio\n",
            "26  rf_bootstrap_6        bootstrap     0.339940      0.966057\n",
            "25  rf_bootstrap_5        bootstrap     0.339874      0.974101\n",
            "24  rf_bootstrap_4        bootstrap     0.339647      0.974058\n",
            "23  rf_bootstrap_3        bootstrap     0.339380      0.974015\n",
            "22  rf_bootstrap_2        bootstrap     0.338749      0.974106\n",
            "21  rf_bootstrap_1        bootstrap     0.338245      0.974029\n",
            "4   rf_tree_pred_5  tree_prediction     0.312291      0.000133\n",
            "0   rf_tree_pred_1  tree_prediction     0.301385      0.000089\n",
            "8   rf_tree_pred_9  tree_prediction     0.296288      0.000105\n",
            "7   rf_tree_pred_8  tree_prediction     0.281976      0.000109\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "extra_factor = pd.concat([factor_df, gp_factor],axis =1)"
      ],
      "metadata": {
        "id": "KBbOkoLKXbcS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extra_factor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        },
        "id": "96gGSOl-HvOr",
        "outputId": "231fb080-c5ba-41bd-fc1e-7ab583ea1522"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         rf_tree_pred_1  rf_tree_pred_2  rf_tree_pred_3  rf_tree_pred_4  \\\n",
              "0              0.573348        1.411724        1.364777       -0.988676   \n",
              "1              0.573348        1.411724        1.364777       -0.988676   \n",
              "2              0.573348        0.346282        1.167933       -0.988676   \n",
              "3              0.573348        0.959177        1.167933       -0.988676   \n",
              "4              1.020689        0.831203        1.167933        0.990808   \n",
              "...                 ...             ...             ...             ...   \n",
              "1522303       -1.294664       -1.554286       -1.900849       -1.952891   \n",
              "1522304       -0.156288       -0.960481       -1.900849       -0.928778   \n",
              "1522305       -0.935492        0.412662       -1.900849       -0.748495   \n",
              "1522306       -2.259776       -1.561753       -1.900849       -1.687048   \n",
              "1522307       -1.294664       -1.561753       -1.900849       -1.952891   \n",
              "\n",
              "         rf_tree_pred_5  rf_tree_pred_6  rf_tree_pred_7  rf_tree_pred_8  \\\n",
              "0              0.905704        1.419675        0.989516        1.421544   \n",
              "1              0.905704        1.419675        0.989516        1.421544   \n",
              "2              0.905704        1.824160        0.989516        1.421544   \n",
              "3              0.905704        1.824160        0.989516        1.421544   \n",
              "4              1.358280        0.885778        0.989516        1.421544   \n",
              "...                 ...             ...             ...             ...   \n",
              "1522303       -0.123315        0.131028       -1.921689        0.459393   \n",
              "1522304       -0.123315        0.131028       -1.502551       -0.719297   \n",
              "1522305        0.299333        0.131028        0.520201        0.459393   \n",
              "1522306       -0.123315        0.131028       -1.710091       -0.719297   \n",
              "1522307       -0.123315        0.131028       -1.710091       -0.292613   \n",
              "\n",
              "         rf_tree_pred_9  rf_tree_pred_10  ...  rf_depth_1  rf_depth_2  \\\n",
              "0             -1.088521         0.877164  ...    0.850267    0.397930   \n",
              "1             -1.088521         0.877164  ...    0.850267    0.397930   \n",
              "2             -1.088521         0.877164  ...    0.850267    0.373333   \n",
              "3             -1.088521         0.877164  ...    0.850267    0.213455   \n",
              "4              0.940734         1.608291  ...    1.065179    1.332604   \n",
              "...                 ...              ...  ...         ...         ...   \n",
              "1522303       -1.259734        -1.276827  ...   -1.398046   -1.348435   \n",
              "1522304       -0.150601        -1.562804  ...   -0.637587   -0.795009   \n",
              "1522305       -0.143008        -0.518277  ...    0.800672    0.484019   \n",
              "1522306       -1.259734        -1.562804  ...   -0.835968   -1.274645   \n",
              "1522307       -0.150601        -1.562804  ...   -1.398046   -1.274645   \n",
              "\n",
              "         rf_depth_3  rf_depth_4  rf_residual_1  rf_residual_2  rf_residual_3  \\\n",
              "0          1.050530   -0.007817       0.662695      -0.884270      -0.877767   \n",
              "1          1.050530   -0.007817       0.662695      -0.884270      -0.877767   \n",
              "2         -1.253436   -0.007817       0.677820      -0.837223      -0.846042   \n",
              "3         -1.253436   -0.007817       0.677733      -0.837495      -0.846227   \n",
              "4         -1.253436    0.623461       0.667617      -0.868960      -0.867521   \n",
              "...             ...         ...            ...            ...            ...   \n",
              "1522303   -0.485447   -1.428192      -0.817636      -0.401882      -0.518514   \n",
              "1522304   -0.485447   -0.439744       1.280044       1.035977       0.998884   \n",
              "1522305   -0.485447    0.274597      -1.059333       0.349910       0.191443   \n",
              "1522306   -0.485447   -0.589257      -0.775701      -0.532319      -0.623081   \n",
              "1522307   -0.485447   -1.428192       1.368734       1.311845       1.366477   \n",
              "\n",
              "         rf_residual_4  genetic_ordinal_1  genetic_ordinal_10  \n",
              "0             0.804800           7.779292            0.464701  \n",
              "1             0.804800           7.779292            0.125753  \n",
              "2             0.814299           7.779292           -0.188610  \n",
              "3             0.814244           7.779292           -0.394928  \n",
              "4             0.807903         -14.603992            0.018230  \n",
              "...                ...                ...                 ...  \n",
              "1522303      -0.961912          -0.720313           -0.000249  \n",
              "1522304       1.131085          -0.674258           -0.001130  \n",
              "1522305      -1.090437          -0.644826           -0.000089  \n",
              "1522306      -0.937758          -0.674483           -0.000625  \n",
              "1522307       1.170711          -0.670548            0.001562  \n",
              "\n",
              "[1522308 rows x 37 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1ff9281f-df98-4060-91dc-b32aea9cbace\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>rf_tree_pred_1</th>\n",
              "      <th>rf_tree_pred_2</th>\n",
              "      <th>rf_tree_pred_3</th>\n",
              "      <th>rf_tree_pred_4</th>\n",
              "      <th>rf_tree_pred_5</th>\n",
              "      <th>rf_tree_pred_6</th>\n",
              "      <th>rf_tree_pred_7</th>\n",
              "      <th>rf_tree_pred_8</th>\n",
              "      <th>rf_tree_pred_9</th>\n",
              "      <th>rf_tree_pred_10</th>\n",
              "      <th>...</th>\n",
              "      <th>rf_depth_1</th>\n",
              "      <th>rf_depth_2</th>\n",
              "      <th>rf_depth_3</th>\n",
              "      <th>rf_depth_4</th>\n",
              "      <th>rf_residual_1</th>\n",
              "      <th>rf_residual_2</th>\n",
              "      <th>rf_residual_3</th>\n",
              "      <th>rf_residual_4</th>\n",
              "      <th>genetic_ordinal_1</th>\n",
              "      <th>genetic_ordinal_10</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.573348</td>\n",
              "      <td>1.411724</td>\n",
              "      <td>1.364777</td>\n",
              "      <td>-0.988676</td>\n",
              "      <td>0.905704</td>\n",
              "      <td>1.419675</td>\n",
              "      <td>0.989516</td>\n",
              "      <td>1.421544</td>\n",
              "      <td>-1.088521</td>\n",
              "      <td>0.877164</td>\n",
              "      <td>...</td>\n",
              "      <td>0.850267</td>\n",
              "      <td>0.397930</td>\n",
              "      <td>1.050530</td>\n",
              "      <td>-0.007817</td>\n",
              "      <td>0.662695</td>\n",
              "      <td>-0.884270</td>\n",
              "      <td>-0.877767</td>\n",
              "      <td>0.804800</td>\n",
              "      <td>7.779292</td>\n",
              "      <td>0.464701</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.573348</td>\n",
              "      <td>1.411724</td>\n",
              "      <td>1.364777</td>\n",
              "      <td>-0.988676</td>\n",
              "      <td>0.905704</td>\n",
              "      <td>1.419675</td>\n",
              "      <td>0.989516</td>\n",
              "      <td>1.421544</td>\n",
              "      <td>-1.088521</td>\n",
              "      <td>0.877164</td>\n",
              "      <td>...</td>\n",
              "      <td>0.850267</td>\n",
              "      <td>0.397930</td>\n",
              "      <td>1.050530</td>\n",
              "      <td>-0.007817</td>\n",
              "      <td>0.662695</td>\n",
              "      <td>-0.884270</td>\n",
              "      <td>-0.877767</td>\n",
              "      <td>0.804800</td>\n",
              "      <td>7.779292</td>\n",
              "      <td>0.125753</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.573348</td>\n",
              "      <td>0.346282</td>\n",
              "      <td>1.167933</td>\n",
              "      <td>-0.988676</td>\n",
              "      <td>0.905704</td>\n",
              "      <td>1.824160</td>\n",
              "      <td>0.989516</td>\n",
              "      <td>1.421544</td>\n",
              "      <td>-1.088521</td>\n",
              "      <td>0.877164</td>\n",
              "      <td>...</td>\n",
              "      <td>0.850267</td>\n",
              "      <td>0.373333</td>\n",
              "      <td>-1.253436</td>\n",
              "      <td>-0.007817</td>\n",
              "      <td>0.677820</td>\n",
              "      <td>-0.837223</td>\n",
              "      <td>-0.846042</td>\n",
              "      <td>0.814299</td>\n",
              "      <td>7.779292</td>\n",
              "      <td>-0.188610</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.573348</td>\n",
              "      <td>0.959177</td>\n",
              "      <td>1.167933</td>\n",
              "      <td>-0.988676</td>\n",
              "      <td>0.905704</td>\n",
              "      <td>1.824160</td>\n",
              "      <td>0.989516</td>\n",
              "      <td>1.421544</td>\n",
              "      <td>-1.088521</td>\n",
              "      <td>0.877164</td>\n",
              "      <td>...</td>\n",
              "      <td>0.850267</td>\n",
              "      <td>0.213455</td>\n",
              "      <td>-1.253436</td>\n",
              "      <td>-0.007817</td>\n",
              "      <td>0.677733</td>\n",
              "      <td>-0.837495</td>\n",
              "      <td>-0.846227</td>\n",
              "      <td>0.814244</td>\n",
              "      <td>7.779292</td>\n",
              "      <td>-0.394928</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.020689</td>\n",
              "      <td>0.831203</td>\n",
              "      <td>1.167933</td>\n",
              "      <td>0.990808</td>\n",
              "      <td>1.358280</td>\n",
              "      <td>0.885778</td>\n",
              "      <td>0.989516</td>\n",
              "      <td>1.421544</td>\n",
              "      <td>0.940734</td>\n",
              "      <td>1.608291</td>\n",
              "      <td>...</td>\n",
              "      <td>1.065179</td>\n",
              "      <td>1.332604</td>\n",
              "      <td>-1.253436</td>\n",
              "      <td>0.623461</td>\n",
              "      <td>0.667617</td>\n",
              "      <td>-0.868960</td>\n",
              "      <td>-0.867521</td>\n",
              "      <td>0.807903</td>\n",
              "      <td>-14.603992</td>\n",
              "      <td>0.018230</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1522303</th>\n",
              "      <td>-1.294664</td>\n",
              "      <td>-1.554286</td>\n",
              "      <td>-1.900849</td>\n",
              "      <td>-1.952891</td>\n",
              "      <td>-0.123315</td>\n",
              "      <td>0.131028</td>\n",
              "      <td>-1.921689</td>\n",
              "      <td>0.459393</td>\n",
              "      <td>-1.259734</td>\n",
              "      <td>-1.276827</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.398046</td>\n",
              "      <td>-1.348435</td>\n",
              "      <td>-0.485447</td>\n",
              "      <td>-1.428192</td>\n",
              "      <td>-0.817636</td>\n",
              "      <td>-0.401882</td>\n",
              "      <td>-0.518514</td>\n",
              "      <td>-0.961912</td>\n",
              "      <td>-0.720313</td>\n",
              "      <td>-0.000249</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1522304</th>\n",
              "      <td>-0.156288</td>\n",
              "      <td>-0.960481</td>\n",
              "      <td>-1.900849</td>\n",
              "      <td>-0.928778</td>\n",
              "      <td>-0.123315</td>\n",
              "      <td>0.131028</td>\n",
              "      <td>-1.502551</td>\n",
              "      <td>-0.719297</td>\n",
              "      <td>-0.150601</td>\n",
              "      <td>-1.562804</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.637587</td>\n",
              "      <td>-0.795009</td>\n",
              "      <td>-0.485447</td>\n",
              "      <td>-0.439744</td>\n",
              "      <td>1.280044</td>\n",
              "      <td>1.035977</td>\n",
              "      <td>0.998884</td>\n",
              "      <td>1.131085</td>\n",
              "      <td>-0.674258</td>\n",
              "      <td>-0.001130</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1522305</th>\n",
              "      <td>-0.935492</td>\n",
              "      <td>0.412662</td>\n",
              "      <td>-1.900849</td>\n",
              "      <td>-0.748495</td>\n",
              "      <td>0.299333</td>\n",
              "      <td>0.131028</td>\n",
              "      <td>0.520201</td>\n",
              "      <td>0.459393</td>\n",
              "      <td>-0.143008</td>\n",
              "      <td>-0.518277</td>\n",
              "      <td>...</td>\n",
              "      <td>0.800672</td>\n",
              "      <td>0.484019</td>\n",
              "      <td>-0.485447</td>\n",
              "      <td>0.274597</td>\n",
              "      <td>-1.059333</td>\n",
              "      <td>0.349910</td>\n",
              "      <td>0.191443</td>\n",
              "      <td>-1.090437</td>\n",
              "      <td>-0.644826</td>\n",
              "      <td>-0.000089</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1522306</th>\n",
              "      <td>-2.259776</td>\n",
              "      <td>-1.561753</td>\n",
              "      <td>-1.900849</td>\n",
              "      <td>-1.687048</td>\n",
              "      <td>-0.123315</td>\n",
              "      <td>0.131028</td>\n",
              "      <td>-1.710091</td>\n",
              "      <td>-0.719297</td>\n",
              "      <td>-1.259734</td>\n",
              "      <td>-1.562804</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.835968</td>\n",
              "      <td>-1.274645</td>\n",
              "      <td>-0.485447</td>\n",
              "      <td>-0.589257</td>\n",
              "      <td>-0.775701</td>\n",
              "      <td>-0.532319</td>\n",
              "      <td>-0.623081</td>\n",
              "      <td>-0.937758</td>\n",
              "      <td>-0.674483</td>\n",
              "      <td>-0.000625</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1522307</th>\n",
              "      <td>-1.294664</td>\n",
              "      <td>-1.561753</td>\n",
              "      <td>-1.900849</td>\n",
              "      <td>-1.952891</td>\n",
              "      <td>-0.123315</td>\n",
              "      <td>0.131028</td>\n",
              "      <td>-1.710091</td>\n",
              "      <td>-0.292613</td>\n",
              "      <td>-0.150601</td>\n",
              "      <td>-1.562804</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.398046</td>\n",
              "      <td>-1.274645</td>\n",
              "      <td>-0.485447</td>\n",
              "      <td>-1.428192</td>\n",
              "      <td>1.368734</td>\n",
              "      <td>1.311845</td>\n",
              "      <td>1.366477</td>\n",
              "      <td>1.170711</td>\n",
              "      <td>-0.670548</td>\n",
              "      <td>0.001562</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1522308 rows × 37 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1ff9281f-df98-4060-91dc-b32aea9cbace')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-1ff9281f-df98-4060-91dc-b32aea9cbace button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-1ff9281f-df98-4060-91dc-b32aea9cbace');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-6f99f75d-681c-471a-b831-9e53a2cfffef\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-6f99f75d-681c-471a-b831-9e53a2cfffef')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-6f99f75d-681c-471a-b831-9e53a2cfffef button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_b77f5334-ed97-4ca7-99ca-0d4f4c71fd52\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('extra_factor')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_b77f5334-ed97-4ca7-99ca-0d4f4c71fd52 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('extra_factor');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "extra_factor"
            }
          },
          "metadata": {},
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "extra_factor.to_csv(\"/content/drive/MyDrive/extra_factor.csv\")"
      ],
      "metadata": {
        "id": "Sz8VSAMICykh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}