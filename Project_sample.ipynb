{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "class StockDataProcessor:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.scaler = RobustScaler()\n",
        "\n",
        "    def load_and_merge_data(self, df:pd.DataFrame):\n",
        "\n",
        "        data = df.copy()\n",
        "        data['date'] = pd.to_datetime(data['date'])\n",
        "\n",
        "        return data\n",
        "\n",
        "    def create_target_variable(self, df, return_col='ret'):\n",
        "\n",
        "        df['return_2c'] = (df[\"return\"l] > 0).astype(int)\n",
        "        df[\"target\"] = df[\"return_2c\"].shift(1)\n",
        "\n",
        "        return df\n",
        "\n",
        "\n",
        "    def remove_outliers(self, df, factor_cols, n_std=3):\n",
        "\n",
        "        for col in factor_cols:\n",
        "            mean = df[col].mean()\n",
        "            std = df[col].std()\n",
        "            df = df.mask(abs(df[col] - mean) <= n_std * std)\n",
        "\n",
        "        return df\n",
        "\n",
        "    def handle_missing_values(self, df, factor_cols):\n",
        "\n",
        "        df[factor_cols] = df.groupby('permno')[factor_cols].fillna(method='ffill')\n",
        "        df[factor_cols] = df.groupby('permno')[factor_cols].fillna(method='bfill')\n",
        "        df[factor_cols] = df[factor_cols].fillna(df[factor_cols].median())\n",
        "\n",
        "        return df\n",
        "\n",
        "    def create_lagged_features(self, df, factor_cols, lags=[1, 2, 3]):\n",
        "\n",
        "        for lag in lags:\n",
        "            for col in factor_cols:\n",
        "                df[f'{col}_lag{lag}'] = df.groupby('permno')[col].shift(lag)\n",
        "\n",
        "        return df.dropna()\n",
        "\n",
        "    def prepare_features_and_target(self, df, factor_cols):\n",
        "\n",
        "        feature_cols = [col for col in df.columns if any(factor in col for factor in factor_cols)]\n",
        "\n",
        "        X = df[feature_cols].values\n",
        "        y = df['target'].values\n",
        "\n",
        "        X_scaled = self.scaler.fit_transform(X)\n",
        "\n",
        "        return X_scaled, y, feature_cols\n",
        "\n",
        "class DeepStockClassifier(nn.Module):\n",
        "\n",
        "    def __init__(self, input_dim, hidden_dims=[256, 128, 64, 32], dropout_rate=0.3):\n",
        "        super(DeepStockClassifier, self).__init__()\n",
        "\n",
        "        layers = []\n",
        "        prev_dim = input_dim\n",
        "\n",
        "        for hidden_dim in hidden_dims:\n",
        "            layers.extend([\n",
        "                nn.Linear(prev_dim, hidden_dim),\n",
        "                nn.BatchNorm1d(hidden_dim),\n",
        "                nn.ReLU(),\n",
        "                nn.Dropout(dropout_rate)\n",
        "            ])\n",
        "            prev_dim = hidden_dim\n",
        "\n",
        "        layers.append(nn.Linear(prev_dim, 1))\n",
        "        layers.append(nn.Sigmoid())\n",
        "\n",
        "        self.network = nn.Sequential(*layers)\n",
        "\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_uniform_(m.weight)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.network(x)\n",
        "\n",
        "class ModelTrainer:\n",
        "\n",
        "    def __init__(self, model, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
        "        self.model = model.to(device)\n",
        "        self.device = device\n",
        "        self.train_losses = []\n",
        "        self.val_losses = []\n",
        "        self.train_accuracies = []\n",
        "        self.val_accuracies = []\n",
        "\n",
        "    def train_model(self, X_train, y_train, X_val, y_val,\n",
        "                   batch_size=1024, epochs=100, learning_rate=0.001,\n",
        "                   weight_decay=1e-4, patience=10):\n",
        "\n",
        "        train_dataset = TensorDataset(\n",
        "            torch.FloatTensor(X_train),\n",
        "            torch.FloatTensor(y_train)\n",
        "        )\n",
        "        val_dataset = TensorDataset(\n",
        "            torch.FloatTensor(X_val),\n",
        "            torch.FloatTensor(y_val)\n",
        "        )\n",
        "\n",
        "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "        optimizer = optim.Adam(self.model.parameters(),\n",
        "                             lr=learning_rate, weight_decay=weight_decay)\n",
        "        criterion = nn.BCELoss()\n",
        "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "            optimizer, mode='min', factor=0.5, patience=5, verbose=True\n",
        "        )\n",
        "\n",
        "        best_val_loss = float('inf')\n",
        "        patience_counter = 0\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "\n",
        "            self.model.train()\n",
        "            train_loss = 0.0\n",
        "            train_correct = 0\n",
        "            train_total = 0\n",
        "\n",
        "            for batch_X, batch_y in train_loader:\n",
        "                batch_X, batch_y = batch_X.to(self.device), batch_y.to(self.device)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                outputs = self.model(batch_X).squeeze()\n",
        "                loss = criterion(outputs, batch_y)\n",
        "                loss.backward()\n",
        "\n",
        "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
        "\n",
        "                optimizer.step()\n",
        "\n",
        "                train_loss += loss.item()\n",
        "                predicted = (outputs > 0.5).float()\n",
        "                train_total += batch_y.size(0)\n",
        "                train_correct += (predicted == batch_y).sum().item()\n",
        "\n",
        "            self.model.eval()\n",
        "            val_loss = 0.0\n",
        "            val_correct = 0\n",
        "            val_total = 0\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for batch_X, batch_y in val_loader:\n",
        "                    batch_X, batch_y = batch_X.to(self.device), batch_y.to(self.device)\n",
        "\n",
        "                    outputs = self.model(batch_X).squeeze()\n",
        "                    loss = criterion(outputs, batch_y)\n",
        "\n",
        "                    val_loss += loss.item()\n",
        "                    predicted = (outputs > 0.5).float()\n",
        "                    val_total += batch_y.size(0)\n",
        "                    val_correct += (predicted == batch_y).sum().item()\n",
        "\n",
        "            avg_train_loss = train_loss / len(train_loader)\n",
        "            avg_val_loss = val_loss / len(val_loader)\n",
        "            train_acc = train_correct / train_total\n",
        "            val_acc = val_correct / val_total\n",
        "\n",
        "            self.train_losses.append(avg_train_loss)\n",
        "            self.val_losses.append(avg_val_loss)\n",
        "            self.train_accuracies.append(train_acc)\n",
        "            self.val_accuracies.append(val_acc)\n",
        "\n",
        "            scheduler.step(avg_val_loss)\n",
        "\n",
        "            if avg_val_loss < best_val_loss:\n",
        "                best_val_loss = avg_val_loss\n",
        "                patience_counter = 0\n",
        "                torch.save(self.model.state_dict(), 'best_model.pth')\n",
        "            else:\n",
        "                patience_counter += 1\n",
        "\n",
        "            if epoch % 10 == 0:\n",
        "                print(f'Epoch [{epoch}/{epochs}] - '\n",
        "                      f'Train Loss: {avg_train_loss:.4f}, Train Acc: {train_acc:.4f}, '\n",
        "                      f'Val Loss: {avg_val_loss:.4f}, Val Acc: {val_acc:.4f}')\n",
        "\n",
        "            if patience_counter >= patience:\n",
        "                print(f'Early stopping at epoch {epoch}')\n",
        "                break\n",
        "\n",
        "        self.model.load_state_dict(torch.load('best_model.pth'))\n",
        "        print(\"Training completed!\")\n",
        "\n",
        "    def evaluate_model(self, X_test, y_test):\n",
        "\n",
        "        self.model.eval()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            X_test_tensor = torch.FloatTensor(X_test).to(self.device)\n",
        "            outputs = self.model(X_test_tensor).squeeze()\n",
        "            y_pred_proba = outputs.cpu().numpy()\n",
        "            y_pred = (y_pred_proba > 0.5).astype(int)\n",
        "\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        precision = precision_score(y_test, y_pred)\n",
        "        recall = recall_score(y_test, y_pred)\n",
        "        f1 = f1_score(y_test, y_pred)\n",
        "        auc = roc_auc_score(y_test, y_pred_proba)\n",
        "\n",
        "        print(f\"Test Results:\")\n",
        "        print(f\"Accuracy: {accuracy:.4f}\")\n",
        "        print(f\"Precision: {precision:.4f}\")\n",
        "        print(f\"Recall: {recall:.4f}\")\n",
        "        print(f\"F1-Score: {f1:.4f}\")\n",
        "        print(f\"AUC: {auc:.4f}\")\n",
        "\n",
        "        return {\n",
        "            'accuracy': accuracy,\n",
        "            'precision': precision,\n",
        "            'recall': recall,\n",
        "            'f1': f1,\n",
        "            'auc': auc,\n",
        "            'y_pred': y_pred,\n",
        "            'y_pred_proba': y_pred_proba\n",
        "        }\n",
        "\n",
        "    def plot_training_history(self):\n",
        "\n",
        "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "        ax1.plot(self.train_losses, label='Train Loss', color='blue')\n",
        "        ax1.plot(self.val_losses, label='Validation Loss', color='red')\n",
        "        ax1.set_title('Model Loss')\n",
        "        ax1.set_xlabel('Epoch')\n",
        "        ax1.set_ylabel('Loss')\n",
        "        ax1.legend()\n",
        "        ax1.grid(True)\n",
        "\n",
        "        ax2.plot(self.train_accuracies, label='Train Accuracy', color='blue')\n",
        "        ax2.plot(self.val_accuracies, label='Validation Accuracy', color='red')\n",
        "        ax2.set_title('Model Accuracy')\n",
        "        ax2.set_xlabel('Epoch')\n",
        "        ax2.set_ylabel('Accuracy')\n",
        "        ax2.legend()\n",
        "        ax2.grid(True)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "def main():\n",
        "\n",
        "    processor = StockDataProcessor()\n",
        "\n",
        "    df = df\n",
        "\n",
        "    df = processor.create_target_variable(df)\n",
        "\n",
        "    df = processor.remove_outliers(df, factor_cols)\n",
        "    df = processor.handle_missing_values(df, factor_cols)\n",
        "\n",
        "    df = processor.create_lagged_features(df, factor_cols, lags=[1, 2])\n",
        "\n",
        "    X, y, feature_names = processor.prepare_features_and_target(df, factor_cols)\n",
        "\n",
        "    print(f\"Data Shape: X={X.shape}, y={y.shape}\")\n",
        "\n",
        "\n",
        "    df_sorted = df.sort_values('date')\n",
        "    n_total = len(df_sorted)\n",
        "    n_train = int(n_total * 0.7)\n",
        "    n_val = int(n_total * 0.15)\n",
        "\n",
        "    train_idx = df_sorted.index[:n_train]\n",
        "    val_idx = df_sorted.index[n_train:n_train+n_val]\n",
        "    test_idx = df_sorted.index[n_train+n_val:]\n",
        "\n",
        "    X_train, X_val, X_test = X[train_idx], X[val_idx], X[test_idx]\n",
        "    y_train, y_val, y_test = y[train_idx], y[val_idx], y[test_idx]\n",
        "\n",
        "    print(f\"Train_set: {len(X_train)}, Validation_set: {len(X_val)}, Test_set: {len(X_test)}\")\n",
        "\n",
        "\n",
        "    input_dim = X_train.shape[1]\n",
        "    model = DeepStockClassifier(\n",
        "        input_dim=input_dim,\n",
        "        hidden_dims=[512, 256, 128, 64],\n",
        "        dropout_rate=0.3\n",
        "    )\n",
        "\n",
        "    trainer = ModelTrainer(model)\n",
        "    trainer.train_model(\n",
        "        X_train, y_train, X_val, y_val,\n",
        "        batch_size=2048,\n",
        "        epochs=100,\n",
        "        learning_rate=0.001,\n",
        "        patience=15\n",
        "    )\n",
        "\n",
        "\n",
        "    results = trainer.evaluate_model(X_test, y_test)\n",
        "\n",
        "    return model, processor, results\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    model, processor, results = main()"
      ],
      "metadata": {
        "id": "HW5KzaxFHIHE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}