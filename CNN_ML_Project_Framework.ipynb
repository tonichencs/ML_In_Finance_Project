{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Trail dataset"
      ],
      "metadata": {
        "id": "js3MZEHVXX_b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "E06dm-TfXaVC"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pickle\n",
        "import pandas as pd\n"
      ],
      "metadata": {
        "id": "MBkSwT3vHp4s"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def add_lag(x, y, lag):\n",
        "    \"\"\"\n",
        "    Align a single predictor sequence x with response sequence y such that\n",
        "      x_lag[k] corresponds to y_lag[k] = y[k + lag]\n",
        "\n",
        "    Common use case: use past values of x to predict the current value of y:\n",
        "       When lag > 0:\n",
        "           x_lag = x.iloc[0 : len(x) - lag]\n",
        "           y_lag = y.iloc[lag : len(y)]\n",
        "       When lag = 0:\n",
        "           both are returned as-is.\n",
        "\n",
        "    Parameters:\n",
        "    - x: pd.Series or np.ndarray, the predictor to be lagged (one-dimensional).\n",
        "    - y: pd.Series or np.ndarray, the corresponding response variable (one-dimensional).\n",
        "    - lag: int, number of lag steps (non-negative integer).\n",
        "\n",
        "    Returns:\n",
        "    - x_lagged, y_lagged:\n",
        "        Same type as the inputs (if x and y are pd.Series, returns pd.Series;\n",
        "        if they are np.ndarray, returns np.ndarray);\n",
        "        Each has length max_length = len(x) - lag = len(y) - lag.\n",
        "    \"\"\"\n",
        "\n",
        "    if lag < 0:\n",
        "        raise ValueError(f\"lag must be a non-negative integer; got {lag}\")\n",
        "\n",
        "    # If both inputs are pandas.Series\n",
        "    if isinstance(x, pd.Series) and isinstance(y, pd.Series):\n",
        "        if lag == 0:\n",
        "            # Return copies to avoid unintended external modifications\n",
        "            return x.reset_index(drop=True).copy(), y.reset_index(drop=True).copy()\n",
        "\n",
        "        # Ensure length is sufficient\n",
        "        if lag >= len(x) or lag >= len(y):\n",
        "            raise ValueError(f\"lag={lag} is too large; exceeds series length (len={len(x)})\")\n",
        "\n",
        "        # Create lagged versions: x[0:len-lag] aligns with y[lag:len]\n",
        "        x_lagged = x.iloc[0 : len(x) - lag].reset_index(drop=True)\n",
        "        y_lagged = y.iloc[lag : len(y)].reset_index(drop=True)\n",
        "        return x_lagged, y_lagged\n",
        "\n",
        "    # If both inputs are numpy.ndarray\n",
        "    elif isinstance(x, np.ndarray) and isinstance(y, np.ndarray):\n",
        "        if lag == 0:\n",
        "            return x.copy(), y.copy()\n",
        "\n",
        "        if lag >= x.shape[0] or lag >= y.shape[0]:\n",
        "            raise ValueError(f\"lag={lag} is too large; exceeds array length (len={x.shape[0]})\")\n",
        "\n",
        "        x_lagged = x[0 : x.shape[0] - lag].copy()\n",
        "        y_lagged = y[lag : y.shape[0]].copy()\n",
        "        return x_lagged, y_lagged\n",
        "\n",
        "    else:\n",
        "        raise TypeError(\"add_lag: x and y must both be pandas.Series or both be numpy.ndarray.\")\n"
      ],
      "metadata": {
        "id": "3I11OCvnTwRf"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = tf.keras.Input(shape=(None,), dtype=tf.float32, name='input_cnn')\n",
        "y = tf.keras.Input(shape=(None,), dtype=tf.float32, name='train_output_cnn')"
      ],
      "metadata": {
        "id": "1fDdu-kfUsvC"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Simple Network\n"
      ],
      "metadata": {
        "id": "Ktslarg1R6wg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------------------------------\n",
        "# train_with_refine.py\n",
        "# ----------------------------------------------------\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import pickle\n",
        "\n",
        "# Force TF1.x graph execution mode (disable Eager), otherwise using placeholders\n",
        "# will cause “Cannot use KerasTensor with tf.function” errors.\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# Hyperparameter settings\n",
        "# ----------------------------------------------------\n",
        "lag_range     = 20\n",
        "lag_epoch_num = 20\n",
        "# input_size: (time_steps, one_hot_dim, 1, 1); here one_hot_dim = 31\n",
        "input_size    = (20, 3, 1, 1)\n",
        "\n",
        "n_classes = 1   # If doing regression, set to 1; if classification, set to the number of classes\n",
        "learning_rate = 1e-3\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# 1) Define TF1-style placeholders\n",
        "# ----------------------------------------------------\n",
        "# Assume each window we feed into refine is a one-hot vector of shape (31,),\n",
        "# which inside refine will be reshaped to (20, 31, 1, 1)\n",
        "x = tf.compat.v1.placeholder(tf.float32, shape=[None, 3], name='input_placeholder')\n",
        "y = tf.compat.v1.placeholder(tf.float32, shape=[None],    name='label_placeholder')\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# 2) Define a simple feed-forward regression model in the computation graph.\n",
        "#    For demonstration, this is just one fully connected layer. You can replace\n",
        "#    it with your original convolutional model; this example is simplified.\n",
        "# ----------------------------------------------------\n",
        "def build_simple_regression_model(input_tensor):\n",
        "    \"\"\"\n",
        "    Input:  input_tensor, shape = [batch, 3]\n",
        "    Output: prediction,   shape = [batch]\n",
        "    \"\"\"\n",
        "    with tf.compat.v1.variable_scope(\"simple_fc\", reuse=tf.compat.v1.AUTO_REUSE):\n",
        "        # Map from 31 → 64 → 1\n",
        "        W1 = tf.compat.v1.get_variable(\"W1\", shape=[3, 64],\n",
        "                                       initializer=tf.random_normal_initializer())\n",
        "        b1 = tf.compat.v1.get_variable(\"b1\", shape=[64],\n",
        "                                       initializer=tf.random_normal_initializer())\n",
        "        h1 = tf.nn.relu(tf.matmul(input_tensor, W1) + b1)\n",
        "\n",
        "        W2 = tf.compat.v1.get_variable(\"W2\", shape=[64, 1],\n",
        "                                       initializer=tf.random_normal_initializer())\n",
        "        b2 = tf.compat.v1.get_variable(\"b2\", shape=[1],\n",
        "                                       initializer=tf.random_normal_initializer())\n",
        "        out = tf.matmul(h1, W2) + b2  # shape [batch, 1]\n",
        "        out = tf.squeeze(out, axis=1) # → [batch]\n",
        "        return out\n",
        "\n",
        "# Build prediction tensor in the graph\n",
        "prediction = build_simple_regression_model(x)\n",
        "\n",
        "# Loss: Mean Squared Error (MSE)\n",
        "cost = tf.reduce_mean(tf.square(prediction - y))\n",
        "\n",
        "# Optimizer\n",
        "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate).minimize(cost)\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# 3) Define the refine_input_with_lag_multi function here\n",
        "#    (or import it from elsewhere). Below is a minimal version\n",
        "#    that ensures x, y, prediction, cost, optimizer are the graph nodes above.\n",
        "# ----------------------------------------------------\n",
        "\n",
        "def refine_input_with_lag_multi(x_train: pd.DataFrame,\n",
        "                                y_train: pd.Series,\n",
        "                                x_test: pd.DataFrame,\n",
        "                                y_test: pd.Series):\n",
        "    \"\"\"\n",
        "    Search for the best lag independently for each column in x_train.\n",
        "      - x_train: DataFrame, shape = [num_samples, num_features]\n",
        "      - y_train: Series,   shape = [num_samples]\n",
        "      - x_test, y_test are the same structure.\n",
        "    Returns:\n",
        "      best_lags: List[int], the best lag for each column\n",
        "      x_train_lagged, y_train_lagged, x_test_lagged, y_test_lagged:\n",
        "         Aligned versions ready for the next training step.\n",
        "    \"\"\"\n",
        "\n",
        "    n_features = x_train.shape[1]\n",
        "    feature_names = list(x_train.columns)\n",
        "    best_lags = [0] * n_features\n",
        "\n",
        "    for j, col in enumerate(feature_names):\n",
        "        print(f\"==> Searching best lag for feature [{j+1}/{n_features}] '{col}' ...\")\n",
        "        all_lag_losses = []\n",
        "\n",
        "        for i in range(lag_range):\n",
        "            # ---------------------------\n",
        "            # 1) Construct the training subset for lag = i (only lag column j)\n",
        "            # ---------------------------\n",
        "            x_tr_j = x_train.copy().reset_index(drop=True)\n",
        "            y_tr_j = y_train.copy().reset_index(drop=True)\n",
        "\n",
        "            if i > 0:\n",
        "                # Apply lag to column j\n",
        "                x_col_lagged, y_lagged = add_lag(x_tr_j[col], y_tr_j, i)\n",
        "                x_tr_j[col] = x_col_lagged\n",
        "                # Truncate the first i rows of the other columns\n",
        "                for k, other in enumerate(feature_names):\n",
        "                    if k != j:\n",
        "                        x_tr_j[other] = x_tr_j[other].iloc[i:].reset_index(drop=True)\n",
        "                # Align y\n",
        "                y_tr_j = y_lagged\n",
        "                x_tr_j = x_tr_j.iloc[0: len(y_tr_j)].reset_index(drop=True)\n",
        "            # If lag == 0, use original data directly\n",
        "\n",
        "            # ---------------------------\n",
        "            # 2) Quickly train for lag_epoch_num epochs and accumulate loss\n",
        "            # ---------------------------\n",
        "            with tf.compat.v1.Session() as sess:\n",
        "                sess.run(tf.compat.v1.global_variables_initializer())\n",
        "\n",
        "                for epoch in range(lag_epoch_num):\n",
        "                    lag_loss = 0.0\n",
        "                    num_segments = int(len(x_tr_j) / input_size[0])\n",
        "                    for seg in range(num_segments):\n",
        "                        # Extract the next window of length input_size[0]\n",
        "                        window_values = x_tr_j[col].values[seg*input_size[0] : seg*input_size[0] + input_size[0]]\n",
        "                        # Build one-hot input: shape = [batch_size=input_size[0], one_hot_dim=31]\n",
        "                        # Here we treat batch_size as “one batch containing input_size[0] samples”,\n",
        "                        # so the placeholder x=[None,31] input becomes shape (20,31)\n",
        "                        x_in = np.zeros((input_size[0], input_size[1]), dtype=np.float32)\n",
        "                        for idx, v in enumerate(window_values):\n",
        "                            x_in[idx, int(v)] = 1.0\n",
        "                        # Build y_in: shape = [input_size[0]]\n",
        "                        y_in = y_tr_j.values[seg*input_size[0] : seg*input_size[0] + input_size[0]]\n",
        "\n",
        "                        # Run optimization\n",
        "                        _, c = sess.run([optimizer, cost], feed_dict={x: x_in, y: y_in})\n",
        "                        lag_loss += float(c)\n",
        "\n",
        "                    # (Optional) print loss per epoch:\n",
        "                    # print(f\"    col='{col}', lag={i}, epoch={epoch+1}/{lag_epoch_num}, loss={lag_loss:.4f}\")\n",
        "\n",
        "                all_lag_losses.append(lag_loss)\n",
        "\n",
        "        best_i = int(np.argmin(all_lag_losses))\n",
        "        best_lags[j] = best_i\n",
        "        print(f\"  >>> Best lag for feature '{col}' = {best_i}, corresponding loss = {all_lag_losses[best_i]:.4f}\\n\")\n",
        "\n",
        "    # ---------------------------\n",
        "    # 3) After determining the best lag for each column,\n",
        "    #    align everything according to the maximum lag\n",
        "    # ---------------------------\n",
        "    max_lag = max(best_lags)\n",
        "\n",
        "    # Align y_train and y_test first using max_lag\n",
        "    y_train_lagged = y_train.copy().reset_index(drop=True)\n",
        "    y_test_lagged  = y_test.copy().reset_index(drop=True)\n",
        "    if max_lag > 0:\n",
        "        _, y_train_lagged = add_lag(y_train_lagged, y_train_lagged, max_lag)\n",
        "        _, y_test_lagged  = add_lag(y_test_lagged,  y_test_lagged,  max_lag)\n",
        "\n",
        "    # Build aligned X_train_lagged and X_test_lagged\n",
        "    X_tr_lagged = pd.DataFrame()\n",
        "    X_te_lagged = pd.DataFrame()\n",
        "\n",
        "    for j, col in enumerate(feature_names):\n",
        "        lag_j = best_lags[j]\n",
        "        col_train = x_train[col].copy().reset_index(drop=True)\n",
        "        col_test  = x_test[col].copy().reset_index(drop=True)\n",
        "\n",
        "        if lag_j > 0:\n",
        "            col_tr_l, _ = add_lag(col_train, col_train, lag_j)\n",
        "            col_te_l, _ = add_lag(col_test,  col_test,  lag_j)\n",
        "        else:\n",
        "            col_tr_l = col_train.copy()\n",
        "            col_te_l = col_test.copy()\n",
        "\n",
        "        # Truncate to the same length as y_train_lagged\n",
        "        col_tr_l = col_tr_l.iloc[0: len(y_train_lagged)].reset_index(drop=True)\n",
        "        col_te_l = col_te_l.iloc[0: len(y_test_lagged)].reset_index(drop=True)\n",
        "\n",
        "        X_tr_lagged[col] = col_tr_l\n",
        "        X_te_lagged[col] = col_te_l\n",
        "\n",
        "    # Save best_lags\n",
        "   # pickle.dump(best_lags, open(\"data/best_lags_per_feature.p\", \"wb\"))\n",
        "\n",
        "    return best_lags, X_tr_lagged, y_train_lagged, X_te_lagged, y_test_lagged\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# 4) Demonstrate how to actually call refine_input_with_lag_multi\n",
        "# ----------------------------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    # 4.1 Prepare the real X_train, y_train, X_test, y_test\n",
        "    # Assume you have read in:\n",
        "    #   X_train: DataFrame with shape = (1837, 3)\n",
        "    #   y_train: Series    with shape = (1837,)\n",
        "    #   X_test:  DataFrame with shape = (460, 3)\n",
        "    #   y_test:  Series    with shape = (460,)\n",
        "\n",
        "    # The following just simulates data; in practice, load your own data.\n",
        "    X_train = pd.DataFrame(\n",
        "        np.random.randint(0, 3, size=(1837, 3)),\n",
        "        columns=[f\"feat_{i}\" for i in range(3)]\n",
        "    )\n",
        "    y_train = pd.Series(np.random.randn(1837))\n",
        "\n",
        "    X_test = pd.DataFrame(\n",
        "        np.random.randint(0, 3, size=(460, 3)),\n",
        "        columns=[f\"feat_{i}\" for i in range(3)]\n",
        "    )\n",
        "    y_test = pd.Series(np.random.randn(460))\n",
        "\n",
        "    # 4.2 Directly call the function\n",
        "    best_lags, X_tr_lagged, y_tr_lagged, X_te_lagged, y_te_lagged = \\\n",
        "        refine_input_with_lag_multi(X_train, y_train, X_test, y_test)\n",
        "\n",
        "    print(\"Final best lag for each column:\", best_lags)\n",
        "    print(\"Aligned training set shape:\", X_tr_lagged.shape, y_tr_lagged.shape)\n",
        "    print(\"Aligned testing set shape: \", X_te_lagged.shape, y_te_lagged.shape)\n",
        "\n",
        "    # From here,  use X_tr_lagged, y_tr_lagged for your actual model training and evaluation.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dsbr3iSYErah",
        "outputId": "fe167fc8-6c49-464f-a55e-b74b22465279"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==> Searching best lag for feature [1/3] 'feat_0' ...\n",
            "  >>> Best lag for feature 'feat_0' = 18, corresponding loss = 93.8416\n",
            "\n",
            "==> Searching best lag for feature [2/3] 'feat_1' ...\n",
            "  >>> Best lag for feature 'feat_1' = 18, corresponding loss = 93.2147\n",
            "\n",
            "==> Searching best lag for feature [3/3] 'feat_2' ...\n",
            "  >>> Best lag for feature 'feat_2' = 18, corresponding loss = 93.8707\n",
            "\n",
            "Final best lag for each column: [18, 18, 18]\n",
            "Aligned training set shape: (1819, 3) (1819,)\n",
            "Aligned testing set shape:  (442, 3) (442,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train_with_refine.py\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import pickle\n",
        "\n",
        "\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "tf.compat.v1.reset_default_graph()\n",
        "\n",
        "# Hyper parameter\n",
        "lag_range     = 20\n",
        "lag_epoch_num = 20\n",
        "time_steps    = 20\n",
        "one_hot_dim   = 3\n",
        "input_size    = (time_steps, one_hot_dim, 1, 1)\n",
        "\n",
        "hm_epoch      = 10\n",
        "learning_rate = 1e-3\n",
        "\n",
        "# ----------------------------------------\n",
        "x = tf.compat.v1.placeholder(tf.float32, shape=[None, 3], name='input_placeholder')\n",
        "y = tf.compat.v1.placeholder(tf.float32, shape=[None],   name='label_placeholder')\n",
        "\n",
        "\n",
        "def build_simple_regression_model(input_tensor):\n",
        "    \"\"\"\n",
        "    Input:  input_tensor，shape = [batch, 3]\n",
        "    Output: prediction，  shape = [batch]\n",
        "    \"\"\"\n",
        "    with tf.compat.v1.variable_scope(\"simple_fc\", reuse=tf.compat.v1.AUTO_REUSE):\n",
        "        # first layer：3 → 64\n",
        "        W1 = tf.compat.v1.get_variable(\n",
        "            \"W1\",\n",
        "            shape=[3, 64],\n",
        "            initializer=tf.random_normal_initializer()\n",
        "        )\n",
        "        b1 = tf.compat.v1.get_variable(\n",
        "            \"b1\",\n",
        "            shape=[64],\n",
        "            initializer=tf.random_normal_initializer()\n",
        "        )\n",
        "        h1 = tf.nn.relu(tf.matmul(input_tensor, W1) + b1)\n",
        "\n",
        "        # Second layer：64 → 1\n",
        "        W2 = tf.compat.v1.get_variable(\n",
        "            \"W2\",\n",
        "            shape=[64, 1],\n",
        "            initializer=tf.random_normal_initializer()\n",
        "        )\n",
        "        b2 = tf.compat.v1.get_variable(\n",
        "            \"b2\",\n",
        "            shape=[1],\n",
        "            initializer=tf.random_normal_initializer()\n",
        "        )\n",
        "        out = tf.matmul(h1, W2) + b2   # shape [batch, 1]\n",
        "        out = tf.squeeze(out, axis=1)  # → [batch]\n",
        "        return out\n",
        "\n",
        "prediction = build_simple_regression_model(x)\n",
        "\n",
        "# Loss: Mean Squared Error (MSE)\n",
        "cost = tf.reduce_mean(tf.square(prediction - y))\n",
        "\n",
        "# Optimizer\n",
        "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate).minimize(cost)\n",
        "\n",
        "\n",
        "def refine_input_with_lag_multi(x_train: pd.DataFrame,\n",
        "                                y_train: pd.Series,\n",
        "                                x_test: pd.DataFrame,\n",
        "                                y_test: pd.Series):\n",
        "    \"\"\"\n",
        "    find lag for each feature in x_train。\n",
        "      - x_train: DataFrame, shape = [num_samples, num_features]\n",
        "      - y_train: Series,   shape = [num_samples]\n",
        "      - x_test, y_test: same structure。\n",
        "    return：\n",
        "      best_lags: List[int]，best lag for each feature,\n",
        "      x_train_lagged, y_train_lagged, x_test_lagged, y_test_lagged:\n",
        "    \"\"\"\n",
        "    n_features = x_train.shape[1]\n",
        "    feature_names = list(x_train.columns)\n",
        "    best_lags = [0] * n_features\n",
        "\n",
        "    for j, col in enumerate(feature_names):\n",
        "        print(f\"==> Find lag for [{j+1}/{n_features}] '{col}' Find best lag …\")\n",
        "        all_lag_losses = []\n",
        "\n",
        "        for i in range(lag_range):\n",
        "\n",
        "            x_tr_j = x_train.copy().reset_index(drop=True)\n",
        "            y_tr_j = y_train.copy().reset_index(drop=True)\n",
        "\n",
        "            if i > 0:\n",
        "                x_col_lagged, y_lagged = add_lag(x_tr_j[col], y_tr_j, i)\n",
        "                x_tr_j[col] = x_col_lagged\n",
        "                for k, other in enumerate(feature_names):\n",
        "                    if k != j:\n",
        "                        x_tr_j[other] = x_tr_j[other].iloc[i:].reset_index(drop=True)\n",
        "                y_tr_j = y_lagged\n",
        "                x_tr_j = x_tr_j.iloc[0: len(y_tr_j)].reset_index(drop=True)\n",
        "\n",
        "\n",
        "            with tf.compat.v1.Session() as sess:\n",
        "                sess.run(tf.compat.v1.global_variables_initializer())\n",
        "\n",
        "                for epoch in range(lag_epoch_num):\n",
        "                    lag_loss = 0.0\n",
        "                    num_segments = int(len(x_tr_j) / input_size[0])\n",
        "                    for seg in range(num_segments):\n",
        "                        window_values = x_tr_j[col].values[\n",
        "                            seg*input_size[0] : seg*input_size[0] + input_size[0]\n",
        "                        ]\n",
        "\n",
        "                        x_in = np.zeros((input_size[0], input_size[1]), dtype=np.float32)\n",
        "                        for idx, v in enumerate(window_values):\n",
        "                            x_in[idx, int(v)] = 1.0\n",
        "\n",
        "                        y_in = y_tr_j.values[\n",
        "                            seg*input_size[0] : seg*input_size[0] + input_size[0]\n",
        "                        ]\n",
        "\n",
        "                        _, c = sess.run([optimizer, cost], feed_dict={x: x_in, y: y_in})\n",
        "                        lag_loss += float(c)\n",
        "\n",
        "                all_lag_losses.append(lag_loss)\n",
        "\n",
        "        best_i = int(np.argmin(all_lag_losses))\n",
        "        best_lags[j] = best_i\n",
        "        print(f\"  >>> feature '{col}' best lag = {best_i}, respective loss = {all_lag_losses[best_i]:.4f}\\n\")\n",
        "\n",
        "    max_lag = max(best_lags)\n",
        "\n",
        "    y_train_lagged = y_train.copy().reset_index(drop=True)\n",
        "    y_test_lagged  = y_test.copy().reset_index(drop=True)\n",
        "    if max_lag > 0:\n",
        "        _, y_train_lagged = add_lag(y_train_lagged, y_train_lagged, max_lag)\n",
        "        _, y_test_lagged  = add_lag(y_test_lagged,  y_test_lagged,  max_lag)\n",
        "\n",
        "    X_tr_lagged = pd.DataFrame()\n",
        "    X_te_lagged = pd.DataFrame()\n",
        "\n",
        "    for j, col in enumerate(feature_names):\n",
        "        lag_j = best_lags[j]\n",
        "        col_train = x_train[col].copy().reset_index(drop=True)\n",
        "        col_test  = x_test[col].copy().reset_index(drop=True)\n",
        "\n",
        "        if lag_j > 0:\n",
        "            col_tr_l, _ = add_lag(col_train, col_train, lag_j)\n",
        "            col_te_l, _ = add_lag(col_test,  col_test,  lag_j)\n",
        "        else:\n",
        "            col_tr_l = col_train.copy()\n",
        "            col_te_l = col_test.copy()\n",
        "\n",
        "        col_tr_l = col_tr_l.iloc[0: len(y_train_lagged)].reset_index(drop=True)\n",
        "        col_te_l = col_te_l.iloc[0: len(y_test_lagged)].reset_index(drop=True)\n",
        "\n",
        "        X_tr_lagged[col] = col_tr_l\n",
        "        X_te_lagged[col] = col_te_l\n",
        "\n",
        "\n",
        "    return best_lags, X_tr_lagged, y_train_lagged, X_te_lagged, y_test_lagged\n",
        "\n",
        "def conv_neural_network_full(x_train, y_train, x_test, y_test):\n",
        "\n",
        "    best_lags, x_tr_lag, y_tr_lag, x_te_lag, y_te_lag = refine_input_with_lag_multi(\n",
        "        x_train, y_train, x_test, y_test\n",
        "    )\n",
        "\n",
        "    with tf.compat.v1.Session() as sess:\n",
        "        sess.run(tf.compat.v1.global_variables_initializer())\n",
        "\n",
        "        for epoch in range(hm_epoch):\n",
        "            epoch_loss = 0.0\n",
        "            num_batches = int(len(x_tr_lag.values) / time_steps)\n",
        "            for b in range(num_batches):\n",
        "                window = x_tr_lag.values[b * time_steps : b * time_steps + time_steps]\n",
        "\n",
        "                x_in = np.zeros((one_hot_dim, time_steps, 1, 1), dtype=np.float32)\n",
        "\n",
        "                for idx, row in enumerate(window):\n",
        "                    for f, cat in enumerate(row):\n",
        "                        x_in[f, idx, 0, 0] = 1.0\n",
        "\n",
        "                y_in = y_tr_lag.values[b * time_steps : b * time_steps + time_steps]\n",
        "\n",
        "                x_reshaped = np.zeros((time_steps, one_hot_dim), dtype=np.float32)\n",
        "                for t in range(time_steps):\n",
        "                    for f in range(one_hot_dim):\n",
        "                        x_reshaped[t, f] = x_in[f, t, 0, 0]\n",
        "\n",
        "                _, c = sess.run([optimizer, cost], feed_dict={x: x_reshaped, y: y_in})\n",
        "                epoch_loss += float(c)\n",
        "\n",
        "            print(f\"Epoch {epoch+1}/{hm_epoch} completed, loss = {epoch_loss:.4f}\")\n",
        "\n",
        "\n",
        "        total = 0\n",
        "        correct_count = 0.0\n",
        "        #  MSE\n",
        "        mse_window = tf.reduce_mean(tf.square(prediction - y))\n",
        "        num_test_batches = int(len(x_te_lag.values) / time_steps)\n",
        "        for b in range(num_test_batches):\n",
        "            window = x_te_lag.values[b * time_steps : b * time_steps + time_steps]\n",
        "\n",
        "            x_in = np.zeros((one_hot_dim, time_steps, 1, 1), dtype=np.float32)\n",
        "            for idx, row in enumerate(window):\n",
        "                for f, cat in enumerate(row):\n",
        "                    x_in[f, idx, 0, 0] = 1.0\n",
        "\n",
        "            y_in = y_te_lag.values[b * time_steps : b * time_steps + time_steps]\n",
        "\n",
        "            # (3,20,1,1) → (20,3)\n",
        "            x_reshaped = np.zeros((time_steps, one_hot_dim), dtype=np.float32)\n",
        "            for t in range(time_steps):\n",
        "                for f in range(one_hot_dim):\n",
        "                    x_reshaped[t, f] = x_in[f, t, 0, 0]\n",
        "\n",
        "            total += time_steps\n",
        "            mse_val = sess.run(mse_window, feed_dict={x: x_reshaped, y: y_in})\n",
        "            if abs(mse_val) < 5.0:\n",
        "                correct_count += time_steps\n",
        "\n",
        "        accuracy = correct_count / total\n",
        "        print(f\"Test accuracy (|MSE| < 5) = {accuracy:.4f}\")\n",
        "\n",
        "        # saver = tf.compat.v1.train.Saver()\n",
        "        # save_path = saver.save(sess, \"data/model/recurrent/recurrent.ckpt\")\n",
        "        # print(f\"Model saved in file: {save_path}\")\n",
        "\n",
        "#trail run\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    #   X_train:  DataFrame，shape = (1837, 3)\n",
        "    #   y_train:  Series，   shape = (1837,)\n",
        "    #   X_test:   DataFrame，shape = (460, 3)\n",
        "    #   y_test:   Series，   shape = (460,)\n",
        "    #\n",
        "    # generated data for trail run\n",
        "    X_train = pd.DataFrame(\n",
        "        np.random.randint(0, one_hot_dim, size=(1837, one_hot_dim)),\n",
        "        columns=[f\"feat_{i}\" for i in range(one_hot_dim)]\n",
        "    )\n",
        "    y_train = pd.Series(np.random.randn(1837))\n",
        "\n",
        "    X_test = pd.DataFrame(\n",
        "        np.random.randint(0, one_hot_dim, size=(460, one_hot_dim)),\n",
        "        columns=[f\"feat_{i}\" for i in range(one_hot_dim)]\n",
        "    )\n",
        "    y_test = pd.Series(np.random.randn(460))\n",
        "\n",
        "    conv_neural_network_full(X_train, y_train, X_test, y_test)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CAjqKYopgSPr",
        "outputId": "5d0ad79f-ddd9-4db1-d364-ee4c0f5a6ad3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==> Find lag for [1/3] 'feat_0' Find best lag …\n",
            "  >>> feature 'feat_0' best lag = 18, respective loss = 86.1062\n",
            "\n",
            "==> Find lag for [2/3] 'feat_1' Find best lag …\n",
            "  >>> feature 'feat_1' best lag = 18, respective loss = 86.0524\n",
            "\n",
            "==> Find lag for [3/3] 'feat_2' Find best lag …\n",
            "  >>> feature 'feat_2' best lag = 18, respective loss = 86.0052\n",
            "\n",
            "Epoch 1/10 completed, loss = 86.2934\n",
            "Epoch 2/10 completed, loss = 86.1988\n",
            "Epoch 3/10 completed, loss = 86.1865\n",
            "Epoch 4/10 completed, loss = 86.1764\n",
            "Epoch 5/10 completed, loss = 86.1708\n",
            "Epoch 6/10 completed, loss = 86.1657\n",
            "Epoch 7/10 completed, loss = 86.1611\n",
            "Epoch 8/10 completed, loss = 86.1569\n",
            "Epoch 9/10 completed, loss = 86.1531\n",
            "Epoch 10/10 completed, loss = 86.1497\n",
            "Test accuracy (|MSE| < 5) = 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train_with_refine.py\n",
        "# -------------------------------------------------\n",
        "# Description\n",
        "#   1. Keep \"simple fully connected model\" for lag search → placeholder x_fc:[None,3], y_fc:[None]\n",
        "#   2. Add \"complex CNN\" for final binary classification → placeholder x_cnn:[None,20,3,1], y_cnn:[None], keep_prob\n",
        "#   3. refine_input_with_lag_multi internally calls simple model (optimizer_fc, cost_fc)\n",
        "#   4. conv_neural_network_full internally calls complex CNN (optimizer_cnn, loss_cnn, accuracy_op)\n",
        "# -------------------------------------------------\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import pickle\n",
        "\n",
        "# ----------------------------------------\n",
        "# TF1 Setup\n",
        "# ----------------------------------------\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "tf.compat.v1.reset_default_graph()\n",
        "\n",
        "# ----------------------------------------\n",
        "# Hyperparameters\n",
        "# ----------------------------------------\n",
        "lag_range     = 20\n",
        "lag_epoch_num = 20\n",
        "\n",
        "time_steps    = 20   # window length (rows)\n",
        "n_features    = 3    # number of original feature columns\n",
        "n_classes     = 2    # binary classification\n",
        "input_size_fc = (time_steps, n_features, 1, 1)  # for annotation only\n",
        "\n",
        "hm_epoch_cnn  = 10   # number of CNN training epochs\n",
        "learning_rate = 1e-3\n",
        "keep_rate     = 0.8  # dropout keep probability\n",
        "\n",
        "# ========================================\n",
        "# 1) \"Simple fully connected model\" section (for refine search)\n",
        "#    placeholders and model/optimizer\n",
        "# ========================================\n",
        "# Note: This section is exactly your initial build_simple_regression_model, just renamed\n",
        "x_fc = tf.compat.v1.placeholder(tf.float32, shape=[None, 3], name='input_placeholder_fc')\n",
        "y_fc = tf.compat.v1.placeholder(tf.float32, shape=[None],   name='label_placeholder_fc')\n",
        "\n",
        "def build_simple_regression_model(input_tensor):\n",
        "    \"\"\"\n",
        "    Input:  input_tensor, shape = [batch, 3]\n",
        "    Output: prediction,   shape = [batch]\n",
        "    \"\"\"\n",
        "    with tf.compat.v1.variable_scope(\"simple_fc\", reuse=tf.compat.v1.AUTO_REUSE):\n",
        "        # First layer: 3 → 64\n",
        "        W1 = tf.compat.v1.get_variable(\n",
        "            \"W1\",\n",
        "            shape=[3, 64],\n",
        "            initializer=tf.random_normal_initializer()\n",
        "        )\n",
        "        b1 = tf.compat.v1.get_variable(\n",
        "            \"b1\",\n",
        "            shape=[64],\n",
        "            initializer=tf.random_normal_initializer()\n",
        "        )\n",
        "        h1 = tf.nn.relu(tf.matmul(input_tensor, W1) + b1)\n",
        "\n",
        "        # Second layer: 64 → 1\n",
        "        W2 = tf.compat.v1.get_variable(\n",
        "            \"W2\",\n",
        "            shape=[64, 1],\n",
        "            initializer=tf.random_normal_initializer()\n",
        "        )\n",
        "        b2 = tf.compat.v1.get_variable(\n",
        "            \"b2\",\n",
        "            shape=[1],\n",
        "            initializer=tf.random_normal_initializer()\n",
        "        )\n",
        "        out = tf.matmul(h1, W2) + b2   # shape [batch, 1]\n",
        "        out = tf.squeeze(out, axis=1)  # → [batch]\n",
        "        return out\n",
        "\n",
        "# Build \"simple FC\" prediction, loss, and optimizer in the graph\n",
        "prediction_fc = build_simple_regression_model(x_fc)\n",
        "cost_fc       = tf.reduce_mean(tf.square(prediction_fc - y_fc))\n",
        "optimizer_fc  = tf.compat.v1.train.AdamOptimizer(learning_rate).minimize(cost_fc)\n",
        "\n",
        "# ========================================\n",
        "# 2) \"Complex CNN\" section (for final classification)\n",
        "#    placeholders and model/optimizer/accuracy\n",
        "# ========================================\n",
        "# x_cnn: [batch, 20, 3, 1]; y_cnn: [batch]; keep_prob: scalar\n",
        "x_cnn    = tf.compat.v1.placeholder(tf.float32,\n",
        "                                    shape=[None, time_steps, n_features, 1],\n",
        "                                    name='input_placeholder_cnn')\n",
        "y_cnn    = tf.compat.v1.placeholder(tf.int32,\n",
        "                                    shape=[None],\n",
        "                                    name='label_placeholder_cnn')\n",
        "keep_prob = tf.compat.v1.placeholder(tf.float32, name='keep_prob')\n",
        "\n",
        "def build_complex_cnn_model(input_tensor, keep_prob_tensor):\n",
        "    \"\"\"\n",
        "    input_tensor : [batch, 20, 3, 1]\n",
        "    return       : logits [batch, n_classes]\n",
        "    \"\"\"\n",
        "    with tf.compat.v1.variable_scope(\"complex_cnn\", reuse=tf.compat.v1.AUTO_REUSE):\n",
        "        # 1) conv1 + pool (kernel 3×3, in channels 1 → out channels 32)\n",
        "        conv1 = tf.nn.conv2d(\n",
        "            input_tensor,\n",
        "            tf.compat.v1.get_variable(\"W_conv1\",\n",
        "                                      shape=[3, 3, 1, 32],\n",
        "                                      initializer=tf.random_normal_initializer(stddev=0.1)),\n",
        "            strides=[1, 1, 1, 1],\n",
        "            padding='SAME')\n",
        "        conv1 = tf.nn.relu(conv1 + tf.compat.v1.get_variable(\"b_conv1\",\n",
        "                                                             shape=[32],\n",
        "                                                             initializer=tf.zeros_initializer()))\n",
        "        pool1 = tf.nn.max_pool2d(conv1,\n",
        "                                 ksize=[1, 2, 2, 1],\n",
        "                                 strides=[1, 2, 2, 1],\n",
        "                                 padding='SAME')\n",
        "\n",
        "        # 2) conv2 + pool (kernel 3×3, in channels 32 → out channels 64)\n",
        "        conv2 = tf.nn.conv2d(\n",
        "            pool1,\n",
        "            tf.compat.v1.get_variable(\"W_conv2\",\n",
        "                                      shape=[3, 3, 32, 64],\n",
        "                                      initializer=tf.random_normal_initializer(stddev=0.1)),\n",
        "            strides=[1, 1, 1, 1],\n",
        "            padding='SAME')\n",
        "        conv2 = tf.nn.relu(conv2 + tf.compat.v1.get_variable(\"b_conv2\",\n",
        "                                                             shape=[64],\n",
        "                                                             initializer=tf.zeros_initializer()))\n",
        "        pool2 = tf.nn.max_pool2d(conv2,\n",
        "                                 ksize=[1, 2, 2, 1],\n",
        "                                 strides=[1, 2, 2, 1],\n",
        "                                 padding='SAME')\n",
        "\n",
        "        # 3) flatten\n",
        "        #    original dims: height=20→pool1→10→pool2→5; width=3→pool1→2→pool2→1\n",
        "        flat = tf.reshape(pool2, [-1, 5 * 1 * 64])   # 5 × 1 × 64 = 320\n",
        "\n",
        "        # 4) FC(128) + dropout\n",
        "        fc = tf.matmul(\n",
        "            flat,\n",
        "            tf.compat.v1.get_variable(\"W_fc\",\n",
        "                                      shape=[5 * 1 * 64, 128],\n",
        "                                      initializer=tf.random_normal_initializer(stddev=0.1))\n",
        "        ) + tf.compat.v1.get_variable(\"b_fc\",\n",
        "                                      shape=[128],\n",
        "                                      initializer=tf.zeros_initializer())\n",
        "        fc = tf.nn.relu(fc)\n",
        "        fc = tf.nn.dropout(fc, keep_prob_tensor)\n",
        "\n",
        "        # 5) output layer (Dense → 2 classes)\n",
        "        logits = tf.matmul(\n",
        "            fc,\n",
        "            tf.compat.v1.get_variable(\"W_out\",\n",
        "                                      shape=[128, n_classes],\n",
        "                                      initializer=tf.random_normal_initializer(stddev=0.1))\n",
        "        ) + tf.compat.v1.get_variable(\"b_out\",\n",
        "                                      shape=[n_classes],\n",
        "                                      initializer=tf.zeros_initializer())\n",
        "\n",
        "        return logits\n",
        "\n",
        "# Build \"complex CNN\" logits, loss, optimizer, and accuracy\n",
        "logits_cnn   = build_complex_cnn_model(x_cnn, keep_prob)\n",
        "loss_cnn     = tf.reduce_mean(\n",
        "    tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits_cnn, labels=y_cnn)\n",
        ")\n",
        "optimizer_cnn = tf.compat.v1.train.AdamOptimizer(learning_rate).minimize(loss_cnn)\n",
        "\n",
        "# Accuracy operation\n",
        "pred_class   = tf.argmax(logits_cnn, axis=1, output_type=tf.int32)\n",
        "accuracy_op  = tf.reduce_mean(tf.cast(tf.equal(pred_class, y_cnn), tf.float32))\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 3) refine_input_with_lag_multi function\n",
        "#\n",
        "#    Keep the logic the same as before, but replace all calls\n",
        "#    to \"x, y, optimizer, cost\" with \"x_fc, y_fc, optimizer_fc, cost_fc\".\n",
        "# ---------------------------------------------------------------------\n",
        "def refine_input_with_lag_multi(x_train: pd.DataFrame,\n",
        "                                y_train: pd.Series,\n",
        "                                x_test: pd.DataFrame,\n",
        "                                y_test: pd.Series):\n",
        "    \"\"\"\n",
        "    Search for optimal lag independently for each feature in x_train using the simple FC model\n",
        "    (x_fc, y_fc, optimizer_fc, cost_fc).\n",
        "    Returns:\n",
        "      best_lags: List[int]\n",
        "      x_train_lagged, y_train_lagged, x_test_lagged, y_test_lagged\n",
        "    \"\"\"\n",
        "    n_feats = x_train.shape[1]\n",
        "    feature_names = list(x_train.columns)\n",
        "    best_lags = [0] * n_feats\n",
        "\n",
        "    for j, col in enumerate(feature_names):\n",
        "        print(f\"==> Searching best lag for feature [{j+1}/{n_feats}] '{col}' ...\")\n",
        "        all_lag_losses = []\n",
        "\n",
        "        for i in range(lag_range):\n",
        "            # 1) Construct training subset for lag = i (lag this column only, truncate others)\n",
        "            x_tr_j = x_train.copy().reset_index(drop=True)\n",
        "            y_tr_j = y_train.copy().reset_index(drop=True)\n",
        "\n",
        "            if i > 0:\n",
        "                x_col_lagged, y_lagged = add_lag(x_tr_j[col], y_tr_j, i)\n",
        "                x_tr_j[col] = x_col_lagged\n",
        "                for k, other in enumerate(feature_names):\n",
        "                    if k != j:\n",
        "                        x_tr_j[other] = x_tr_j[other].iloc[i:].reset_index(drop=True)\n",
        "                y_tr_j = y_lagged\n",
        "                x_tr_j = x_tr_j.iloc[0: len(y_tr_j)].reset_index(drop=True)\n",
        "\n",
        "            # when lag = 0, use original data directly\n",
        "\n",
        "            # 2) Quickly train simple FC for lag_epoch_num epochs and accumulate loss\n",
        "            with tf.compat.v1.Session() as sess:\n",
        "                sess.run(tf.compat.v1.global_variables_initializer())\n",
        "\n",
        "                for epoch in range(lag_epoch_num):\n",
        "                    lag_loss = 0.0\n",
        "                    num_segments = int(len(x_tr_j) / time_steps)\n",
        "\n",
        "                    for seg in range(num_segments):\n",
        "                        # extract continuous window of length time_steps\n",
        "                        window_values = x_tr_j[col].values[\n",
        "                            seg*time_steps : seg*time_steps + time_steps\n",
        "                        ]\n",
        "\n",
        "                        # convert \"window_values\" to one-hot: shape = [20, 3]\n",
        "                        x_in_fc = np.zeros((time_steps, n_features), dtype=np.float32)\n",
        "                        for idx_, v in enumerate(window_values):\n",
        "                            x_in_fc[idx_, int(v)] = 1.0\n",
        "\n",
        "                        # y_in_fc: shape = [20]\n",
        "                        y_in_fc = y_tr_j.values[\n",
        "                            seg*time_steps : seg*time_steps + time_steps\n",
        "                        ].astype(np.float32)\n",
        "\n",
        "                        # run simple FC model's optimizer & loss\n",
        "                        _, c_fc = sess.run(\n",
        "                            [optimizer_fc, cost_fc],\n",
        "                            feed_dict={x_fc: x_in_fc, y_fc: y_in_fc}\n",
        "                        )\n",
        "                        lag_loss += float(c_fc)\n",
        "\n",
        "                    # Optional: print loss per epoch\n",
        "                    # print(f\"    col='{col}', lag={i}, epoch={epoch+1}, loss={lag_loss:.4f}\")\n",
        "\n",
        "                all_lag_losses.append(lag_loss)\n",
        "\n",
        "        best_i = int(np.argmin(all_lag_losses))\n",
        "        best_lags[j] = best_i\n",
        "        print(f\"  >>> Feature '{col}' best lag = {best_i}, loss = {all_lag_losses[best_i]:.4f}\\n\")\n",
        "\n",
        "    # 3) After determining best lags for all columns, align all columns by the maximum lag\n",
        "    max_lag = max(best_lags)\n",
        "\n",
        "    y_train_lagged = y_train.copy().reset_index(drop=True)\n",
        "    y_test_lagged  = y_test.copy().reset_index(drop=True)\n",
        "    if max_lag > 0:\n",
        "        _, y_train_lagged = add_lag(y_train_lagged, y_train_lagged, max_lag)\n",
        "        _, y_test_lagged  = add_lag(y_test_lagged,  y_test_lagged,  max_lag)\n",
        "\n",
        "    X_tr_lagged = pd.DataFrame()\n",
        "    X_te_lagged = pd.DataFrame()\n",
        "\n",
        "    for j, col in enumerate(feature_names):\n",
        "        lag_j = best_lags[j]\n",
        "        col_train = x_train[col].copy().reset_index(drop=True)\n",
        "        col_test  = x_test[col].copy().reset_index(drop=True)\n",
        "\n",
        "        if lag_j > 0:\n",
        "            col_tr_l, _ = add_lag(col_train, col_train, lag_j)\n",
        "            col_te_l, _ = add_lag(col_test,  col_test,  lag_j)\n",
        "        else:\n",
        "            col_tr_l = col_train.copy()\n",
        "            col_te_l = col_test.copy()\n",
        "\n",
        "        col_tr_l = col_tr_l.iloc[0: len(y_train_lagged)].reset_index(drop=True)\n",
        "        col_te_l = col_te_l.iloc[0: len(y_test_lagged)].reset_index(drop=True)\n",
        "\n",
        "        X_tr_lagged[col] = col_tr_l\n",
        "        X_te_lagged[col] = col_te_l\n",
        "\n",
        "    return best_lags, X_tr_lagged, y_train_lagged, X_te_lagged, y_test_lagged\n",
        "\n",
        "\n",
        "# ----------------------------------------\n",
        "# add_lag helper function (unchanged)\n",
        "# ----------------------------------------\n",
        "def add_lag(col: pd.Series, y: pd.Series, lag: int):\n",
        "    col_lagged = col.shift(lag).iloc[lag:].reset_index(drop=True)\n",
        "    y_lagged   = y.shift(lag).iloc[lag:].reset_index(drop=True)\n",
        "    return col_lagged, y_lagged\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 4) Main training function conv_neural_network_full (calls \"complex CNN\")\n",
        "# ---------------------------------------------------------------------\n",
        "def conv_neural_network_full(x_train, y_train, x_test, y_test):\n",
        "    # 1) First perform lag search and alignment (call refine_input_with_lag_multi)\n",
        "    best_lags, x_tr_lag, y_tr_lag, x_te_lag, y_te_lag = refine_input_with_lag_multi(\n",
        "        x_train, y_train, x_test, y_test\n",
        "    )\n",
        "\n",
        "    with tf.compat.v1.Session() as sess:\n",
        "        sess.run(tf.compat.v1.global_variables_initializer())\n",
        "\n",
        "        # ---------------------------\n",
        "        # 2) Train in batches (each batch = one window) for hm_epoch_cnn epochs\n",
        "        # ---------------------------\n",
        "        for epoch in range(hm_epoch_cnn):\n",
        "            epoch_loss = 0.0\n",
        "            epoch_acc  = 0.0\n",
        "            num_batches = int(len(x_tr_lag.values) / time_steps)\n",
        "\n",
        "            for b in range(num_batches):\n",
        "                # extract a window (20, 3), convert to float32\n",
        "                window = x_tr_lag.values[\n",
        "                    b * time_steps : b * time_steps + time_steps\n",
        "                ].astype(np.float32)\n",
        "\n",
        "                # reshape to [1, 20, 3, 1]\n",
        "                x_in_cnn = window.reshape((1, time_steps, n_features, 1))\n",
        "\n",
        "                # take label of the last time_step in this window, map -1→0, +1→1\n",
        "                y_raw = y_tr_lag.values[b * time_steps + time_steps - 1]\n",
        "                y_label = np.array([1 if y_raw > 0 else 0], dtype=np.int32)\n",
        "\n",
        "                # single step training: compute loss_cnn & accuracy\n",
        "                _, c_val, acc_val = sess.run(\n",
        "                    [optimizer_cnn, loss_cnn, accuracy_op],\n",
        "                    feed_dict={x_cnn: x_in_cnn, y_cnn: y_label, keep_prob: keep_rate}\n",
        "                )\n",
        "                epoch_loss += float(c_val)\n",
        "                epoch_acc  += float(acc_val)\n",
        "\n",
        "            print(f\"Epoch {epoch+1}/{hm_epoch_cnn} - \"\n",
        "                  f\"loss: {epoch_loss/num_batches:.4f}, \"\n",
        "                  f\"acc: {epoch_acc/num_batches:.4f}\")\n",
        "\n",
        "        # ---------------------------\n",
        "        # 3) Test evaluation\n",
        "        # ---------------------------\n",
        "        num_test_batches = int(len(x_te_lag.values) / time_steps)\n",
        "        test_acc_sum = 0.0\n",
        "\n",
        "        for b in range(num_test_batches):\n",
        "            window = x_te_lag.values[\n",
        "                b * time_steps : b * time_steps + time_steps\n",
        "            ].astype(np.float32)\n",
        "            x_in_cnn = window.reshape((1, time_steps, n_features, 1))\n",
        "\n",
        "            y_raw = y_te_lag.values[b * time_steps + time_steps - 1]\n",
        "            y_label = np.array([1 if y_raw > 0 else 0], dtype=np.int32)\n",
        "\n",
        "            acc_batch = sess.run(\n",
        "                accuracy_op,\n",
        "                feed_dict={x_cnn: x_in_cnn, y_cnn: y_label, keep_prob: 1.0}\n",
        "            )\n",
        "            test_acc_sum += float(acc_batch)\n",
        "\n",
        "        print(f\"Test accuracy = {test_acc_sum / num_test_batches:.4f}\")\n",
        "\n",
        "        # (Optional) Save model\n",
        "        # saver = tf.compat.v1.train.Saver()\n",
        "        # saver.save(sess, \"data/model/cnn/cnn.ckpt\")\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 5) Example entry (__main__) - generate random data demonstration\n",
        "# ---------------------------------------------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    # Randomly generate example data\n",
        "    X_train = pd.DataFrame(\n",
        "        np.random.randint(0, n_features, size=(1837, n_features)),\n",
        "        columns=[f\"feat_{i}\" for i in range(n_features)]\n",
        "    )\n",
        "    y_train = pd.Series(np.random.choice([-1, 1], size=1837))\n",
        "\n",
        "    X_test = pd.DataFrame(\n",
        "        np.random.randint(0, n_features, size=(460, n_features)),\n",
        "        columns=[f\"feat_{i}\" for i in range(n_features)]\n",
        "    )\n",
        "    y_test = pd.Series(np.random.choice([-1, 1], size=460))\n",
        "\n",
        "    conv_neural_network_full(X_train, y_train, X_test, y_test)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D8WtL1ZZVQhV",
        "outputId": "5dfe1665-03df-448a-a25f-2899afcf314d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==> Searching best lag for feature [1/3] 'feat_0' ...\n",
            "  >>> Feature 'feat_0' best lag = 19, loss = 89.9793\n",
            "\n",
            "==> Searching best lag for feature [2/3] 'feat_1' ...\n",
            "  >>> Feature 'feat_1' best lag = 19, loss = 89.9672\n",
            "\n",
            "==> Searching best lag for feature [3/3] 'feat_2' ...\n",
            "  >>> Feature 'feat_2' best lag = 19, loss = 89.9488\n",
            "\n",
            "Epoch 1/10 - loss: 1.0967, acc: 0.4556\n",
            "Epoch 2/10 - loss: 0.8183, acc: 0.5222\n",
            "Epoch 3/10 - loss: 0.7318, acc: 0.5667\n",
            "Epoch 4/10 - loss: 0.6715, acc: 0.6556\n",
            "Epoch 5/10 - loss: 0.6861, acc: 0.5667\n",
            "Epoch 6/10 - loss: 0.6923, acc: 0.5333\n",
            "Epoch 7/10 - loss: 0.6148, acc: 0.7111\n",
            "Epoch 8/10 - loss: 0.5101, acc: 0.8111\n",
            "Epoch 9/10 - loss: 0.6070, acc: 0.6778\n",
            "Epoch 10/10 - loss: 0.5184, acc: 0.7111\n",
            "Test accuracy = 0.3182\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(4, 8))\n",
        "ax.set_xlim(0, 1)\n",
        "ax.set_ylim(0, 9)\n",
        "\n",
        "layers = [\n",
        "    (\"Input\\n20×3×1\", 8),\n",
        "    (\"Conv1\\n3×3, 32\\n→20×3×32\", 7),\n",
        "    (\"MaxPool1\\n2×2\\n→10×2×32\", 6),\n",
        "    (\"Conv2\\n3×3, 64\\n→10×2×64\", 5),\n",
        "    (\"MaxPool2\\n2×2\\n→5×1×64\", 4),\n",
        "    (\"Flatten\\n→320\", 3),\n",
        "    (\"FC\\n320→128\", 2),\n",
        "    (\"Dropout\\nkeep=0.8\\n→128\", 1),\n",
        "    (\"Output\\n128→2\\n(logits)\", 0)\n",
        "]\n",
        "\n",
        "for text, y in layers:\n",
        "    rect = patches.Rectangle((0.1, y - 0.4), 0.8, 0.8, fill=False)\n",
        "    ax.add_patch(rect)\n",
        "    ax.text(0.5, y, text, ha='center', va='center')\n",
        "\n",
        "ax.axis('off')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 676
        },
        "id": "-ST1u6CllpfL",
        "outputId": "d98781dd-e087-4bb5-df99-aad2e98dd5da"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 400x800 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUoAAAKTCAYAAAB/8EF/AAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAa6dJREFUeJzt3Xl4DdfjP/D3DbLvkkiQRaw3liKqQosEiYiIVkVTaSSUlmi1tX9oUR+7Fv1Yqkq0iLV2iogkdiGJ/YpYIkpCYwkSZDu/P/zMt7dZJiJxc5P363nu83TOOXPmTB7z7sydO2cUQggBIiIqko6mB0BEVNExKImIZDAoiYhkMCiJiGQwKImIZDAoiYhkMCiJiGQwKImIZDAoiYhkMCiJiGQwKImIZDAoiYhkMCiJiGQwKImIZDAoiYhkMCiJiGQwKImIZDAoiYhkMCiJiGQwKImIZDAoiYhkMCiJiGQwKImIZDAoiYhkMCiJiGQwKImIZDAoiYhkMCiJiGQwKImIZDAoiYhkMCiJiGQwKImIZDAoiYhkMCiJiGQwKImIZDAoiYhkMCiJiGQwKImIZDAoiYhkMCiJiGQwKImIZDAoiYhkMCiJiGQwKImIZDAoiYhkMCiJiGQwKImIZDAoiYhkMCiJiGRU1/QASiIlJQXp6emaHgYRlQMrKys4ODhoehjFqvBBmZKSAqVSiaysLE0PhYjKgaGhIVQqVYUOywoflOnp6cjKysLq1auhVCo1PRwiKkMqlQqBgYFIT09nUJYFpVKJ1q1ba3oYRFQF8WYOEZEMBiURkQwGJRGRDAYlEZEMBiURkQwGJRGRDAYlEZEMBiURkQwGJRGRDAYlEZEMBiWVqeDgYPTu3fuNbnPlypUwNzd/o9ukqoVBSUQkg0FJ5aZz58748ssvMWbMGFhaWsLW1haTJ09Wa6NQKLBkyRJ4e3vDwMAAzs7O2LRpk1QfHR0NhUKBhw8fSmWnT5+GQqFAcnIyoqOjERISgoyMDCgUCigUigLbIHpdDEoqV7/99huMjIxw4sQJzJ49G99//z0iIiLU2nz77bfo06cPzpw5g/79++Ojjz6CSqUqUf/t27fH/PnzYWpqitTUVKSmpmLUqFHlsStUhTEoqVy1aNECkyZNQsOGDREUFIQ2bdogMjJSrU3fvn3x6aefolGjRpg6dSratGmD//3vfyXqX1dXF2ZmZlAoFLC1tYWtrS2MjY3LY1eoCmNQUrlq0aKF2rKdnR3u3r2rVubm5lZguaRnlERvAoOSylWNGjXUlhUKBfLz80u8vo7Oi3+iQgipLCcnp2wGR1RCDErSuOPHjxdYfvnaD2trawBAamqqVH/69Gm19rq6usjLyyvfQVKVxqAkjdu4cSNWrFiBy5cvY9KkSYiNjcXw4cMBAA0aNIC9vT0mT56MpKQk7Nq1Cz/88IPa+k5OTnjy5AkiIyOldywRlSUGJWnclClTsG7dOrRo0QK///471q5dCxcXFwAvLt3Xrl2LS5cuoUWLFpg1axb++9//qq3fvn17fP755+jXrx+sra0xe/ZsTewGVWIK8c8vfyqg+Ph4uLq6Ii4uji8Xq4QUCgW2bNnyxp/moYpBW45vnlESEclgUBIRydCa93pT5VTBv/khAsAzSiIiWQxKIiIZDEoiIhkMSpI1Y8YMvP322zAxMYGNjQ169+6NxMREtTbPnj1DaGgoatasCWNjY/Tp0wd37twp9TY3b96MNm3awNzcHEZGRmjZsiVWrVr1Wvvx5ZdfwtXVFXp6emjZsuVr9UVVC4OSZMXExCA0NBTHjx9HREQEcnJy4OnpiczMTKnN119/jR07dmDjxo2IiYnB7du38cEHH6j1k5WVhdjY2EK3ERUVpbZsaWmJCRMm4NixYzh79ixCQkIQEhKCvXv3qrWLjo4utL/Y2Fi18b00cOBA9OvXryS7TfR/RAUXFxcnAIi4uDhND4X+v7t37woAIiYmRgghxMOHD0WNGjXExo0bpTYqlUoAEMeOHZPKNm3aJIyNjcXhw4fV+hsxYoSoVauWyMjIKHa7rVq1EhMnTpSWHz9+LOzs7MRXX32l1u7QoUPCyMhIrF+/vtB+Jk2aJN56660S7SuVL205vnlGSa8sIyMDwIuzPgCIi4tDTk4OunbtKrVp0qQJHBwccOzYMamsT58++Pbbb9GjRw/pzHLs2LFYvXo1IiIiYGpqWuj2hBCIjIxEYmIiOnbsKJUbGxsjIiICq1evxrhx4wAAJ06cQI8ePTBhwgT4+/uX7Y5TlcXfUdIryc/Px1dffYUOHTqgWbNmAIC0tDTo6uoWeMFXrVq1kJaWplY2ZswYPH36FF5eXvjwww+xceNGHDhwAM2bNy+wrYyMDNSpUwfPnz9HtWrVsHjxYnTr1k2tTdOmTREREQEPDw/cu3cPmzZtwldffYXx48eX7Y5TlcagpFcSGhqK8+fP4/Dhw6XuY9KkSYiJicGvv/6KFStWFPmMr4mJCU6fPi3NDPTNN9/A2dkZnTt3VmvXsmVLzJ8/HwMGDMB7772H77//vtRjIyoML72pxIYPH46dO3ciKioKdevWlcptbW2RnZ2t9gIwALhz5w5sbW0L9DN//nycOnUKAQEBGDt2bJGzmevo6KBBgwZo2bIlRo4ciQ8//BAzZswo0C4xMRFjxoxBQEAAEhISCkzDRvS6GJQkSwiB4cOHY8uWLThw4ADq1aunVu/q6ooaNWqovQsnMTERKSkpBV7zsHjxYkycOBE7d+5EeHg4/P390aVLFyQlJcmOIz8/H8+fP1cru3r1Kjw8PNC3b1+Eh4dj586d+O6777Bw4cLX2GMidbz0JlmhoaEIDw/Htm3bYGJiIn3vaGZmBgMDA5iZmWHQoEH45ptvYGlpCVNTU3zxxRdwc3NDu3btpH62bduGUaNGYfv27dJNmf/97394/vw5PDw8cOHCBemGzowZM9CmTRvUr18fz58/x+7du7Fq1SosWbJE6u/Jkyfw8PCAj48PfvrpJwBAp06dsHXrVvj5+aF27dpqP1G6cuUKnjx5grS0NDx9+lSaKd3FxQW6urrl+jckLafp2+5ytOXnA5UZgEI/YWFhUpunT5+KYcOGCQsLC2FoaCjef/99kZqaqtZPRkaG9JOif8rLyxM7duxQK5swYYJo0KCB0NfXFxYWFsLNzU2sW7euwLo7d+4U+fn5Bcqjo6PFw4cP1co6depU6H5cv379Ff4aVJa05fjmxL1EpDHacnzzO0oiIhkMSiIiGQxKIiIZDEoiIhkMSiIiGQxKIiIZDEoiIhkMSiIiGQxKIiIZDEoiIhkMSiIiGQxKIiIZDEoiIhlaMx9lUbNgE5H20pbjusIHpZWVFQwNDREYGKjpoRBROTA0NISVlZWmh1GsCj8fJQCkpKQgPT1d08MgonJgZWUFBwcHTQ+jWFoRlEREmsSbOUREMhiUREQyGJRERDIYlEREMhiUREQyGJRERDIYlEREMhiUREQyGJRERDIq/LPeAB9hJKrMtOERxgoflCkpKVAqlcjKytL0UIioHBgaGkKlUlXosKzwQZmeno6srCysXr0aSqVS08MhojKkUqkQGBiI9PR0BmVZUCqVaN26taaHQURVEG/mEBHJYFASEclgUBIRyWBQUplIS0vDF198AWdnZ+jp6cHe3h6+vr6IjIx8Y2NITU3Fxx9/jEaNGkFHRwdfffXVG9s2VW4MSnptycnJcHV1xYEDBzBnzhycO3cOe/bsgbu7O0JDQ9/YOJ4/fw5ra2tMnDgRb7311hvbLlV+DEp6bcOGDYNCoUBsbCz69OmDRo0aoWnTpvjmm29w/PhxAC9+D+vn5wdjY2OYmprC398fd+7ckfqYPHkyWrZsiVWrVsHJyQlmZmb46KOP8PjxYwDAL7/8gtq1ayM/P19t235+fhg4cCAAwMnJCQsWLEBQUBDMzMze0N5TVcCgpNdy//597NmzB6GhoTAyMipQb25ujvz8fPj5+eH+/fuIiYlBREQErl27hn79+qm1vXr1KrZu3YqdO3di586diImJwcyZMwEAffv2xb179xAVFVVg2/379y/fnaQqT2t+R0kV05UrVyCEQJMmTYpsExkZiXPnzuH69euwt7cHAPz+++9o2rQpTp48ibfffhsAkJ+fj5UrV8LExAQA8MknnyAyMhLTpk2DhYUFvL29ER4eji5dugAANm3aBCsrK7i7u5fzXlJVxzNKei0leYmnSqWCvb29FJIA4OLiAnNzc6hUKqnMyclJCkkAsLOzw927d6Xl/v37448//sDz588BAGvWrMFHH30EHR3+M6byxX9h9FoaNmwIhUKBS5cuvXZfNWrUUFtWKBRq30n6+vpCCIFdu3bh5s2bOHToEC+76Y1gUNJrsbS0hJeXFxYtWoTMzMwC9Q8fPoRSqcTNmzdx8+ZNqfzixYt4+PAhXFxcSrwtfX19fPDBB1izZg3Wrl2Lxo0b87FWeiP4HSW9tkWLFqFDhw5o27Ytvv/+e7Ro0QK5ubmIiIjAkiVLcPHiRTRv3hz9+/fH/PnzkZubi2HDhqFTp05o06bNK22rf//+6NmzJy5cuIDAwMAC9adPnwYAPHnyBH///TdOnz4NXV3dVwpkon9jUNJrc3Z2Rnx8PKZNm4aRI0ciNTUV1tbWcHV1xZIlS6BQKLBt2zZ88cUX6NixI3R0dNC9e3f873//e+VteXh4wNLSEomJifj4448L1Ldq1Ur677i4OISHh8PR0RHJycmvs4tUxSlESb6N16D4+Hi4uroiLi6Ol1lElYy2HN/8jpKISAaDkohIBoOSiEgGg5KISAaDkohIBoOSSmTJkiVo0aIFTE1NYWpqCjc3N/z555+l7i8xMRHu7u6oVasW9PX14ezsjIkTJyInJ+eV+pk8eTKaNGkCIyMjWFhYoGvXrjhx4oRUn5ycjEGDBqFevXowMDBA/fr1MWnSJGRnZ5d67FT1MCipROrWrYuZM2ciLi4Op06dgoeHB/z8/HDhwgW1dtHR0YWuf/DgQeTl5UnLNWrUQFBQEPbt24fExETMnz8fy5Ytw6RJk15pXI0aNcLChQtx7tw5HD58GE5OTvD09MTff/8NALh06RLy8/OxdOlSXLhwAfPmzcPPP/+M//znP6/2B6CqTVRwcXFxAoCIi4vT9FDoXywsLMSvv/4qLScnJwtjY2Mxb948tXYbN24Uenp64siRI8X29/XXX4t33333tcaUkZEhAIj9+/cX2Wb27NmiXr16r7UdKhvacnzzyRx6ZXl5edi4cSMyMzPh5uYmlTs6OmLHjh3w8fGBnp4ehg4diu3btyMwMBDLly9H+/bti+zzypUr2LNnDz744INSjys7Oxu//PILzMzMip3hPCMjA5aWlqXeDlVBmk5qOdryf5yq4OzZs8LIyEhUq1ZNmJmZiV27dhXabu/evUJfX1988cUXQl9fXyxbtqzIPt3c3ISenp4AIIYMGSLy8vJeeVw7duwQRkZGQqFQiNq1a4vY2Ngi2yYlJQlTU1Pxyy+/vPJ2qOxpy/HNoKQSe/78uUhKShKnTp0S48aNE1ZWVuLChQuFtp0yZYoAIIKCgortMyUlRVy4cEGEh4eLOnXqiFmzZr3yuJ48eSKSkpLEsWPHxMCBA4WTk5O4c+dOgXZ//fWXqF+/vhg0aNArb4PKh7Yc3wxKKrUuXbqIIUOGFCg/dOiQMDIyEsHBwUJPT0+sX7++RP2tWrVKGBgYiNzc3NcaV4MGDcT06dPVym7duiUaNmwoPvnkk1KdtVL50Jbjm3e9qdTy8/Ol2cZfOnHiBHx8fDB58mSEhYVh5cqVCAoKwtatW0vUX05OToEXiL3uuG7duoXOnTvD1dUVYWFhnBGdXhlv5lCJjB8/Ht7e3nBwcMDjx48RHh6O6Oho7N27V2qTkpKC7t27Y8yYMRg1ahQA4KOPPkJ2djYCAgIQHR2Nd955B8CL1zjUqFEDzZs3h56eHk6dOoXx48ejX79+BWY6L0pmZiamTZuGXr16wc7ODunp6Vi0aBFu3bqFvn37Avi/kHR0dMTcuXOlnw0BgK2tbVn9eaiSY1BSidy9exdBQUFITU2FmZkZWrRogb1796Jbt25SGwcHB6xatQo9e/ZUWzcoKAg2NjZqc0VWr14ds2bNwuXLlyGEgKOjI4YPH46vv/5aahMdHQ13d3dcv34dTk5OBcZUrVo1XLp0Cb/99hvS09NRs2ZNvP322zh06BCaNm0KAIiIiMCVK1dw5coV1K1bV219UbFnGKQKhPNRUoUVFhaG6dOn4+LFiyU+yyTtoi3HN7+soQpr9+7dmD59OkOSNI6X3lRhbdy4UdNDIALAM0oiIlkMSiIiGQxKIiIZDEqSVdI5Hc+ePYv33nsP+vr6sLe3x+zZs19ru5999hnq168PAwMDWFtbw8/PD5cuXSp1f5s3b0abNm1gbm4OIyMjtGzZEqtWrZLqc3JyMHbsWDRv3hxGRkaoXbs2goKCcPv27dfaD9J+DEqSVZI5HR89egRPT084OjoiLi4Oc+bMweTJk/HLL7+o9RUfH49Hjx4V2MatW7dw+fJltbKXT9KoVCrs3bsXQgh4enqqzWuZlZWF2NjYQscdFRWltmxpaYkJEybg2LFjOHv2LEJCQhASEiL9aD4rKwvx8fH49ttvER8fj82bNyMxMRG9evV6tT8YVT4afYCyBLTlWdDKJDc3V1y8eLHYNv+e03Hx4sXCwsJCPH/+XCobO3asaNy4sdp6PXv2FB06dBBPnjyRytLS0kTjxo3F0KFDi93mmTNnBABx5coVqWzTpk3C2NhYHD58WK3tiBEjRK1atURGRkaxfbZq1UpMnDixyPrY2FgBQNy4caPYfqh0tOX45hklFbBr1y60a9euyDM1oOCcjseOHUPHjh2hq6srlXl5eSExMREPHjyQysLDw5GbmwtfX188ffoU6enp6NKlCxo2bIgFCxYUub3MzEyEhYWhXr16sLe3l8r79OmDb7/9Fj169JDGO3bsWKxevRoREREwNTUttD8hBCIjI5GYmIiOHTsWu58KhQLm5uZFtqEqQNNJLUdb/o9T2fz3v/8VZmZm4uTJkwXqCpvTsVu3bgVmErpw4YIAUODs9MGDB6JVq1aiW7duomXLlqJbt27i2bNnhY5j0aJFwsjISAAQjRs3Vjub/KfJkycLc3Nz8emnnwozM7Mi/708fPhQGBkZierVqws9PT2xfPnyIv8GT58+Fa1btxYff/xxkW3o9WjL8c2grKKioqIEANmPs7Oz2npFzen4KkEphBBXr14V1atXF6ampuLBgwdFjvPhw4fi8uXLIiYmRvj6+orWrVuLp0+fFtrW3d1dABArVqwosr+8vDyRlJQkEhISxNy5c4WZmZmIiooq0C47O1v4+vqKVq1ayV6+U+lpy/HNJ3OqqLZt20KlUhVZv2HDBkydOhVz5syRym7fvg13d3e0b9++wE0aW1tb3LlzR63s5fK/Z+nJzMxEUFAQ2rVrh3v37iEkJAQbN25E9eoF/zmamZnBzMwMDRs2RLt27WBhYYEtW7YgICBArd38+fNx6tQpBAQEYOzYsWjXrh2USmWB/nR0dNCgQQMAQMuWLaFSqTBjxgx07txZapOTkwN/f3/cuHEDBw4cKPLynaoOfkdZRRkaGqJJkyaFfm7evIkZM2Zg7dq10jts5OZ0dHNzw8GDB9VeNxsREYHGjRvDwsJCKnv69Cl69uyJvLw8/Pnnn4iMjMT58+fRv39/tbvZhREvroAKzIG5ePFiTJw4ETt37kR4eDj8/f3RpUsXJCUlyf4d/j135cuQTEpKwv79+1GzZk3ZPqgK0PQprRxtOTWvTB49eiT27NkjLf/111+iQYMGokuXLuKvv/4Sqamp0uelhw8filq1aolPPvlEnD9/Xqxbt04YGhqKpUuXqvX9/vvvC1dXV/Hw4UOpLCUlRTg5OYkvv/xSKrt69aqYPn26OHXqlLhx44Y4cuSI8PX1FZaWlmqvedi6daswMDAQERERUll+fr749NNPRd26ddUum6dPny727dsnrl69Ki5evCjmzp0rqlevLr3TJzs7W/Tq1UvUrVtXnD59Wm0//3k3n8qOthzfDEqSFRYWVuR3mP905swZ8e677wo9PT1Rp04dMXPmzAJ9HT58WNy7d69A+bVr18TZs2el5Vu3bglvb29hY2MjatSoIerWrSs+/vhjcenSJbX1MjIyRExMTIH+8vLyxI4dO9TKJkyYIBo0aCD09fWFhYWFcHNzE+vWrZPqr1+/XuR+FvY9Jr0+bTm+OR8lEWmMthzf/I6SiEgGg5KISAaDkohIBoOSiEgGg5KISIbWPJlT3FMkRKSdtOW4rvBBaWVlBUNDQwQGBmp6KERUDgwNDWFlZaXpYRSrwv+OEgBSUlKQnp6u6WEQUTmwsrKCg4ODpodRLK0ISiIiTeLNHCIiGQxKIiIZDEoiIhkMSiIiGQxKIiIZDEoiIhkMSiIiGQxKIiIZDEoiIhkMSiIiGRV+UgyAz3oTVWba8Kx3hQ/KlJQUKJVKZGVlaXooRFQODA0NoVKpKnRYVvigTE9PR1ZWFlavXg2lUqnp4RBRGVKpVAgMDER6ejqDsiwolcoK/TpLIqq8eDOHiEgGg5KqBIVCga1bt2p6GKSlGJRUQHBwMBQKBT7//PMCdaGhoVAoFAgODi6z7XXu3BkKhQIKhQL6+vpwcXHB4sWLy6z/wkybNg3t27eHoaEhzM3Ny3VbpP0YlFQoe3t7rFu3Dk+fPpXKnj17hvDw8HL50n3w4MFITU3FxYsX4e/vj9DQUKxdu7bMt/NSdnY2+vbti6FDh5bbNqjyYFBSoVq3bg17e3ts3rxZKtu8eTMcHBzQqlUrqWzPnj149913YW5ujpo1a6Jnz564evWqVP/777/D2NgYSUlJUtmwYcPQpEkTtZ98GRoawtbWFs7Ozpg8eTIaNmyI7du3A3jxEzE/Pz8YGxvD1NQU/v7+uHPnjtp4lyxZgvr160NXVxeNGzfGqlWrit2/KVOm4Ouvv0bz5s1L9weiKoVBSUUaOHAgwsLCpOUVK1YgJCRErU1mZia++eYbnDp1CpGRkdDR0cH777+P/Px8AEBQUBB69OiB/v37Izc3F7t27cKvv/6KNWvWwNDQsMhtGxgYIDs7G/n5+fDz88P9+/cRExODiIgIXLt2Df369ZPabtmyBSNGjMDIkSNx/vx5fPbZZwgJCUFUVFQZ/0WoyhIVXFxcnAAg4uLiND2UKmPAgAHCz89P3L17V+jp6Ynk5GSRnJws9PX1xd9//y38/PzEgAEDCl3377//FgDEuXPnpLL79++LunXriqFDh4patWqJadOmqa3TqVMnMWLECCGEELm5uWLVqlUCgFi4cKHYt2+fqFatmkhJSZHaX7hwQQAQsbGxQggh2rdvLwYPHqzWZ9++fUWPHj2kZQBiy5YtBcYbFhYmzMzMXuGvQ2VJW45vnlFSkaytreHj44OVK1ciLCwMPj4+Bd6/nJSUhICAADg7O8PU1BROTk4AXlwuv2RhYYHly5dLl8fjxo0rsK3FixfD2NgYBgYGGDx4ML7++msMHToUKpUK9vb2sLe3l9q6uLjA3NwcKpUKwIsfLXfo0EGtvw4dOkj1RK9La35wTpoxcOBADB8+HACwaNGiAvW+vr5wdHTEsmXLULt2beTn56NZs2bIzs5Wa3fw4EFUq1YNqampyMzMhImJiVp9//79MWHCBBgYGMDOzg46Ovx/OFUc/NdIxerevTuys7ORk5MDLy8vtbp79+4hMTEREydORJcuXaBUKvHgwYMCfRw9ehSzZs3Cjh07YGxsLAXvP5mZmaFBgwaoU6eOWkgqlUrcvHkTN2/elMouXryIhw8fwsXFRWpz5MgRtf6OHDki1RO9Lp5RUrGqVasmXcJWq1ZNrc7CwgI1a9bEL7/8Ajs7O6SkpBS4rH78+DE++eQTfPnll/D29kbdunXx9ttvw9fXFx9++KHs9rt27YrmzZujf//+mD9/PnJzczFs2DB06tQJbdq0AQCMHj0a/v7+aNWqFbp27YodO3Zg8+bN2L9/f5H9pqSk4P79+0hJSUFeXh5Onz4NAGjQoAGMjY1f5U9EVQDPKEmWqakpTE1NC5Tr6Ohg3bp1iIuLQ7NmzfD1119jzpw5am1GjBgBIyMjTJ8+HQDQvHlzTJ8+HZ999hlu3bolu22FQoFt27bBwsICHTt2RNeuXeHs7Iz169dLbXr37o0FCxZg7ty5aNq0KZYuXYqwsDB07ty5yH6/++47tGrVCpMmTcKTJ0/QqlUrtGrVCqdOnSrhX4WqEoUQQmh6EMWJj4+Hq6sr4uLiOCkGUSWjLcc3zyiJiGQwKImIZDAoiYhkMCiJiGQwKImIZDAoqdzMmDEDb7/9NkxMTGBjY4PevXsjMTGx1P1FR0fDz88PdnZ2MDIyQsuWLbFmzZoyHDFR4RiUVG5iYmIQGhqK48ePIyIiAjk5OfD09ERmZqZau+jo6ELXP3jwIPLy8qTlo0ePokWLFvjjjz9w9uxZhISEICgoCDt37izP3SDi7EH05ty9e1cAEDExMVJZcnKyMDY2FvPmzVNru3HjRqGnpyeOHDlSbJ89evQQISEh5TFcegO05fjmI4z0xmRkZAAALC0tpTJHR0fs2LEDPj4+0NPTw9ChQ7F9+3YEBgZi+fLlaN++vWyffI0xlTcGJb0R+fn5+Oqrr9ChQwc0a9ZMra5z587YsmUL/Pz8oFKpsGzZMixcuBD9+/cvts8NGzbg5MmTWLp0aXkOnYhBSW9GaGgozp8/j8OHDxda7+npifHjx2PSpEkICgrCp59+Wmx/UVFRCAkJwbJly9C0adPyGDKRhDdzqNwNHz4cO3fuRFRUFOrWrVtom8OHD2P27NkIDg7G+vXrsWHDhiL7i4mJga+vL+bNm4egoKDyGjaRhEFJ5UYIgeHDh2PLli04cOAA6tWrV2i7EydOwMfHB5MnT0ZYWBhWrlyJoKCgQt/DHR0dDR8fH8yaNQtDhgwp5z0geoGX3lRuQkNDER4ejm3btsHExARpaWkAXkzSa2BgAODFvJDdu3fHmDFjMGrUKADARx99hOzsbAQEBCA6OhrvvPMOgBeX2z179sSIESPQp08fqT9dXV21G0REZU7Tt93laMvPB6ggAIV+wsLC1Nrt2LGj0PX//PNP8fz5c2l5wIABhfbXqVOnctwLKk/acnzzjJLKjSjhVKc9e/YstLx79+5qyytXrsTKlStfd1hEr4zfURIRyWBQEhHJYFASEclgUBIRyWBQEhHJYFASEclgUFKJTJs2De3bt4ehoSHMzc0LbZOSkgIfHx8YGhrCxsYGo0ePRm5ubqm2l5OTg7Fjx6J58+YwMjJC7dq1ERQUhNu3b5d6Hw4fPowOHTqgZs2aMDAwQJMmTTBv3jy1NmU92TBVDgxKKpHs7Gz07dsXQ4cOLbQ+Ly8PPj4+yM7OxtGjR/Hbb79h5cqV+O6779TaXb58udCwe/ToEeLi4qTlrKwsxMfH49tvv0V8fDw2b96MxMRE9OrVq8B2Dx06VOiYoqKi1JaNjIwwfPhwHDx4ECqVChMnTsTEiRPxyy+/SG1KOtkwVTGa/sW7HG355X5lkpubKy5evFhoXVhYmDAzMytQvnv3bqGjoyPS0tKksiVLlghTU1O1p2uGDx8uGjdurNbuyZMnokOHDsLb27vYccXGxgoA4saNG1LZ8ePHhb6+vti0aZNa23nz5gkjIyNx7dq1Yvt8//33RWBgYJH1hU02TGVHW45vnlFSAbt27UK7du0QGxtb4nWOHTuG5s2bo1atWlKZl5cXHj16hAsXLkhlP/74Ixo3boyuXbsiPT0dT58+Rc+ePfH8+XOsW7eu2G1kZGRAoVCoXfq/8847WLFiBQIDA7Fjxw4AwOLFi/Gf//wH27dvL3IiDgBISEjA0aNH0alTp2K3CYDPkldxfISRCujVqxfGjBkDT09P7N+/H23atJFdJy0tTS0kAUjLLyevAIAaNWpgw4YN8PPzQ7du3WBlZYUHDx4gKioKpqamRfb/7NkzjB07FgEBAQXaBQQE4NmzZ/D398enn36KZcuWYcuWLfDw8Ci0r7p16+Lvv/9Gbm4uJk+eXOTcl8VNNkxVC4OyioqOjoa7u7tsu379+uHq1atlum09PT2sX78eTk5OOHfuHFQqFSwsLIpsn5OTA39/fwghsGTJkkLbhISE4NChQ1i4cCG+++47eHt7F9nfoUOH8OTJExw/fhzjxo1DgwYNEBAQUKCd3GTDVHUwKKuotm3bQqVSFVm/YcMGTJ06FXPmzClRf7a2tgUu1e/cuSPV/VNubi4GDRoEOzs7NGvWDAMGDMC+fftgbGxcoN+XIXnjxg0cOHCgyLPOTZs2Ye3atQgODsbcuXPRpUsXdOzYsdC2Ly/Hmzdvjjt37mDy5MkFgvLlZMMHDx4scrJhqkI0/SWpHG35srcy2bdvn9DX1xcbN24sUCd3M+fOnTtS2dKlS4Wpqal49uyZVJaXlycCAgJEw4YNxe3bt8WjR49Eu3btRMeOHUVmZqZan9nZ2aJ3796iadOm4u7du0WOd9u2bUJfX1+sW7dOCCHE7NmzhYmJiTh69Kjsvk6ZMkU4OjpKy/n5+SI0NFTUrl1bXL58WXZ9ej3acnwzKKmAR48eiT179qiV3bhxQyQkJIgpU6YIY2NjkZCQIBISEsTjx4+FEC/ulDdr1kx4enqK06dPiz179ghra2sxfvx4tX6+/vprUa9ePZGSkiKVPXz4ULi6uopevXpJZdnZ2aJXr16ibt264vTp0yI1NVX6/PMuemxsrNDX1xe//fab2namTp0qzMzMxPXr16WyhQsXiu3bt4vLly+Ly5cvi19//VWYmJiICRMmSG2GDh0qzMzMRHR0tNo2s7KySv8HpSJpy/HNoKQSKWrS3KioKKlNcnKy8Pb2FgYGBsLKykqMHDlS5OTkqPVz7tw5tfB66d69e+Lw4cPS8vXr14uc+Pef28zOzi4Q6i9t375dbfmnn34STZs2FYaGhsLU1FS0atVKLF68WOTl5UltitrmvycbprKhLce3QogSzq6qIfHx8XB1dUVcXBxat26t6eEQURnSluObv6MkIpLBoCQiksGgJCKSwaAkIpLBoCQikqE1T+YU9xQJEWknbTmuK3xQWllZwdDQEIGBgZoeChGVA0NDQ1hZWWl6GMWq8L+jBF7MnJ2enq7pYRBRObCysoKDg4Omh1EsrQhKIiJN4s0cIiIZDEoiIhkMSiIiGQxKIiIZDEoiIhkMSiIiGQxKIiIZDEoiIhkMSiIiGRX+WW+AjzASVWba8AhjhQ/KlJQUKJVKZGVlaXooRFQODA0NoVKpKnRYVvigTE9PR1ZWFlavXg2lUqnp4RBRGVKpVAgMDER6ejqDsiwolcoK/ZY2Iqq8eDOHiEgGg5KISAaDkohIBoOSykRaWhq++OILODs7Q09PD/b29vD19UVkZOQbG8PmzZvRrVs3WFtbw9TUFG5ubti7d+8b2z5VXgxKem3JyclwdXXFgQMHMGfOHJw7dw579uyBu7s7QkND39g4Dh48iG7dumH37t2Ii4uDu7s7fH19kZCQ8MbGQJWUqODi4uIEABEXF6fpoVARvL29RZ06dcSTJ08K1D148EAIIcSNGzdEr169hJGRkTAxMRF9+/YVaWlpUrtJkyaJt956S/z+++/C0dFRmJqain79+olHjx4JIYRYunSpsLOzE3l5eWr99+rVS4SEhBQ5NhcXFzFlypQy2EsqD9pyfPOMkl7L/fv3sWfPHoSGhsLIyKhAvbm5OfLz8+Hn54f79+8jJiYGERERuHbtGvr166fW9urVq9i6dSt27tyJnTt3IiYmBjNnzgQA9O3bF/fu3UNUVFSBbffv37/QseXn5+Px48ewtLQswz2mqkhrfkdJFdOVK1cghECTJk2KbBMZGYlz587h+vXrsLe3BwD8/vvvaNq0KU6ePIm3334bwItgW7lyJUxMTAAAn3zyCSIjIzFt2jRYWFjA29sb4eHh6NKlCwBg06ZNsLKygru7e6HbnTt3Lp48eQJ/f/+y3GWqgnhGSa9FlOAlniqVCvb29lJIAoCLiwvMzc2hUqmkMicnJykkAcDOzg53796Vlvv3748//vgDz58/BwCsWbMGH330EXR0Cv4zDg8Px5QpU7BhwwbY2NiUat+IXmJQ0mtp2LAhFAoFLl269Np91ahRQ21ZoVAgPz9fWvb19YUQArt27cLNmzdx6NChQi+7161bh08//RQbNmxA165dX3tcRAxKei2Wlpbw8vLCokWLkJmZWaD+4cOHUCqVuHnzJm7evCmVX7x4EQ8fPoSLi0uJt6Wvr48PPvgAa9aswdq1a9G4ceMCj7WuXbsWISEhWLt2LXx8fEq/Y0T/wKCk17Zo0SLk5eWhbdu2+OOPP5CUlASVSoWffvoJbm5u6Nq1K5o3b47+/fsjPj4esbGxCAoKQqdOndCmTZtX2lb//v2xa9curFixosDZZHh4OIKCgvDDDz/gnXfeQVpaGtLS0pCRkVGWu0tVEIOSXpuzszPi4+Ph7u6OkSNHolmzZujWrRsiIyOxZMkSKBQKbNu2DRYWFujYsSO6du0KZ2dnrF+//pW35eHhAUtLSyQmJuLjjz9Wq/vll1+Qm5uL0NBQ2NnZSZ8RI0aU1a5SFaUQJfk2XoPi4+Ph6uqKuLg4zh5EVMloy/HNM0oiIhkMSiIiGQxKIiIZDEoiIhkMSiIiGQxKKpElS5agRYsWMDU1leZ6/PPPP0vdX2JiItzd3VGrVi3o6+vD2dkZEydORE5Oziv3pVKp0KtXL5iZmcHIyAhvv/02UlJSCrQTQsDb2xsKhQJbt24t9dip6mFQUonUrVsXM2fORFxcHE6dOgUPDw/4+fnhwoULau2io6MLXf/gwYPIy8uTlmvUqIGgoCDs27cPiYmJmD9/PpYtW4ZJkya90riuXr2Kd999F02aNEF0dDTOnj2Lb7/9Fvr6+gXazp8/HwqF4pX6JwLA+Sip9CwsLMSvv/4qLScnJwtjY2Mxb948tXYbN24Uenp64siRI8X29/XXX4t33333lcbQr18/ERgYKNsuISFB1KlTR6SmpgoAYsuWLa+0HSof2nJ884ySXlleXh7WrVuHzMxMuLm5SeWOjo7YsWMHJkyYgCVLlgAAtm/fjsDAQCxfvhzt27cvss8rV65gz5496NSpU4nHkZ+fj127dqFRo0bw8vKCjY0N3nnnnQKX1VlZWfj444+xaNEi2NravtrOEgE8o6SSO3v2rDAyMhLVqlUTZmZmYteuXYW227t3r9DX1xdffPGF0NfXF8uWLSuyTzc3N6GnpycAiCFDhhSYwbw4L88ODQ0NxY8//igSEhLEjBkzhEKhENHR0VK7IUOGiEGDBknL4BllhaEtxzeDkkrs+fPnIikpSZw6dUqMGzdOWFlZiQsXLhTadsqUKQKACAoKKrbPlJQUceHCBREeHi7q1KkjZs2aVeLx3Lp1SwAQAQEBauW+vr7io48+EkIIsW3bNtGgQQPx+PFjqZ5BWXFoy/HNS28qMV1dXTRo0ACurq6YMWMG3nrrLSxYsKBAu8OHD2P27NkIDg7G+vXrsWHDhiL7tLe3h4uLCwICAjBz5kxMnjxZ7aZPcaysrFC9evUCU7UplUrprveBAwdw9epVmJubo3r16qhe/cWk/n369EHnzp1LuOdU1fFVEFRq+fn50mzjL504cQI+Pj6YPHkyRo0aBS8vLwQFBUFXVxe9e/eW7S8nJwf5+fmoVq2a7PZ1dXXx9ttvIzExUa388uXLcHR0BACMGzcOn376qVp98+bNMW/ePPj6+pZgL4kYlFRC48ePh7e3NxwcHPD48WOEh4cjOjpa7b3ZKSkp6N69O8aMGYNRo0YBAD766CNkZ2cjICAA0dHReOeddwC8eI1DjRo10Lx5c+jp6eHUqVMYP348+vXrV2Cm8+KMHj0a/fr1Q8eOHeHu7o49e/Zgx44d0s+UbG1tC72B4+DggHr16r3GX4SqFE1f+8vRlu8wKruBAwcKR0dHoaurK6ytrUWXLl3Evn37CrTbsWNHoev/+eef4vnz59LyunXrROvWrYWxsbEwMjISLi4uYvr06eLp06dSm6ioKAFAXL9+vdixLV++XDRo0EDo6+uLt956S2zdurXY9uB3lBWGthzfnI+SKqywsDBMnz4dFy9efKWzTNIe2nJ882YOVVi7d+/G9OnTGZKkcfyOkiqsjRs3anoIRAB4RklEJItBSUQkg0FJRCSDQUklMm3aNLRv3x6GhoYwNzcvtE1KSgp8fHxgaGgIGxsbjB49Grm5uaXaXk5ODsaOHYvmzZvDyMgItWvXRlBQEG7fvv0aewE8f/4cEyZMgKOjI/T09ODk5IQVK1YU2nbdunVQKBSyP5Snyo9BSSWSnZ2Nvn37YujQoYXW5+XlwcfHB9nZ2Th69Ch+++03rFy5Et99951au8uXLxcado8ePUJcXJy0nJWVhfj4eHz77beIj4/H5s2bkZiYiF69ehXY7qFDhwodU1RUVIEyf39/REZGYvny5UhMTMTatWvRuHHjAu2Sk5MxatQovPfee4X2TVWMpn/IKUdbfpBameTm5oqLFy8WWhcWFibMzMwKlO/evVvo6OiItLQ0qWzJkiXC1NRU7Yfmw4cPF40bN1Zr9+TJE9GhQwfh7e1d7LhiY2MFAHHjxg2p7Pjx40JfX19s2rRJre28efOEkZGRuHbtmlT2559/CjMzM3Hv3r1it5Obmyvat28vfv31VzFgwADh5+dXbHsqPW05vnlGSQXs2rUL7dq1Q2xsbInXOXbsGJo3b45atWpJZV5eXnj06JHaLOg//vgjGjdujK5duyI9PR1Pnz5Fz5498fz5c6xbt67YbWRkZEChUKhd+r/zzjtYsWIFAgMDsWPHDgDA4sWL8Z///Afbt29Xe0xx+/btaNOmDWbPno06deqgUaNGGDVqFJ4+faq2ne+//x42NjYYNGhQifefKjf+jpIK6NWrF8aMGQNPT0/s378fbdq0kV0nLS1NLSQBSMtpaWlSWY0aNbBhwwb4+fmhW7dusLKywoMHDxAVFQVTU9Mi+3/27BnGjh2LgICAAu0CAgLw7Nkz+Pv749NPP8WyZcuwZcsWeHh4qLW7du0aDh8+DH19fWzZsgXp6ekYNmwY7t27h7CwMAAvZj5avnw5Tp8+LbvPVHUwKKuo6OhouLu7y7br168frl69Wqbb1tPTw/r16+Hk5IRz585BpVLBwsKiyPY5OTnw9/eHEEKaOf3fQkJCcOjQISxcuBDfffcdvL29C7TJz8+HQqHAmjVrYGZmBuDFGe6HH36IxYsXIzc3F5988gmWLVsGKyurstlZqhQYlFVU27ZtoVKpiqzfsGEDpk6dijlz5pSoP1tb2wKX6nfu3JHq/ik3NxeDBg2CnZ0dmjVrhgEDBmDfvn0wNjYu0O/LkLxx4wYOHDhQ5Fnnpk2bsHbtWgQHB2Pu3Lno0qULOnbsqNbGzs4OderUkUISeDF3pRACf/31FzIzM5GcnKw2/Vp+fj4AoHr16khMTET9+vVL9PegSkbTX5LK0ZYveyuTffv2CX19fbFx48YCdXI3c+7cuSOVLV26VJiamopnz55JZXl5eSIgIEA0bNhQ3L59Wzx69Ei0a9dOdOzYUWRmZqr1mZ2dLXr37i2aNm0q7t69W+R4t23bJvT19cW6deuEEELMnj1bmJiYiKNHj6q1W7p0qTAwMFCb7Xzr1q1CR0dHZGVliadPn4pz586pffz8/ISHh4c4d+6c2k0pKhvacnwzKKmAR48eiT179qiV3bhxQyQkJIgpU6YIY2NjkZCQIBISEqTQyc3NFc2aNROenp7i9OnTYs+ePcLa2lqMHz9erZ+vv/5a1KtXT6SkpEhlDx8+FK6urqJXr15SWXZ2tujVq5eoW7euOH36tEhNTZU+/wys2NhYoa+vL3777Te17UydOlWYmZmpTdH2+PFjUbduXfHhhx+KCxcuiJiYGNGwYUPx6aefFvm34F3v8qUtxzeDkkpkwIABAkCBT1RUlNQmOTlZeHt7CwMDA2FlZSVGjhwpcnJy1Po5d+5cofNL3rt3Txw+fFhavn79eqHb+/c2s7OzC4T6S9u3by9QplKpRNeuXYWBgYGoW7eu+Oabb0RWVlax+82gLD/acnxzPkoi0hhtOb75O0oiIhkMSiIiGQxKIiIZDEoiIhkMSiIiGVrzZE5xT5EQkXbSluO6wgellZUVDA0NERgYqOmhEFE5MDQ0rPDP1lf431ECL2bOTk9P1/QwiKgcWFlZwcHBQdPDKJZWBCURkSbxZg4RkQwGJRGRDAYlEZEMBiURkQwGJRGRDAYlEZEMBiURkQwGJRGRDAYlEZEMBiURkYwKPykGwGe9iSozbXjWu8IHZUpKCpRKJbKysjQ9FCIqB4aGhlCpVBU6LCt8UKanpyMrKwurV6+GUqnU9HCIqAypVCoEBgYiPT2dQVkWlEplhX6dJRFVXryZQ0Qkg0FJVYJCocDWrVs1PQzSUgxKKiA4OBgKhQKff/55gbrQ0FAoFAoEBweX2fY6d+4MhUIBhUIBfX19uLi4YPHixWXW/78lJydj0KBBqFevHgwMDFC/fn1MmjQJ2dnZ5bZN0m4MSiqUvb091q1bh6dPn0plz549Q3h4eLl86T548GCkpqbi4sWL8Pf3R2hoKNauXVvm2wGAS5cuIT8/H0uXLsWFCxcwb948/Pzzz/jPf/5TLtsj7cegpEK1bt0a9vb22Lx5s1S2efNmODg4oFWrVlLZnj178O6778Lc3Bw1a9ZEz549cfXqVan+999/h7GxMZKSkqSyYcOGoUmTJmo/+TI0NIStrS2cnZ0xefJkNGzYENu3bwfw4idifn5+MDY2hqmpKfz9/XHnzh218S5ZsgT169eHrq4uGjdujFWrVhW5b927d0dYWBg8PT3h7OyMXr16YdSoUWr7SvRPDEoq0sCBAxEWFiYtr1ixAiEhIWptMjMz8c033+DUqVOIjIyEjo4O3n//feTn5wMAgoKC0KNHD/Tv3x+5ubnYtWsXfv31V6xZswaGhoZFbtvAwADZ2dnIz8+Hn58f7t+/j5iYGERERODatWvo16+f1HbLli0YMWIERo4cifPnz+Ozzz5DSEgIoqKiSryvGRkZsLS0LHF7qmJEBRcXFycAiLi4OE0PpcoYMGCA8PPzE3fv3hV6enoiOTlZJCcnC319ffH3338LPz8/MWDAgELX/fvvvwUAce7cOans/v37om7dumLo0KGiVq1aYtq0aWrrdOrUSYwYMUIIIURubq5YtWqVACAWLlwo9u3bJ6pVqyZSUlKk9hcuXBAARGxsrBBCiPbt24vBgwer9dm3b1/Ro0cPaRmA2LJlS6FjTkpKEqampuKXX34p6Z+Iyoi2HN88o6QiWVtbw8fHBytXrkRYWBh8fHwKvH85KSkJAQEBcHZ2hqmpKZycnAC8uFx+ycLCAsuXL5cuj8eNG1dgW4sXL4axsTEMDAwwePBgfP311xg6dChUKhXs7e1hb28vtXVxcYG5uTlUKhWAFz9a7tChg1p/HTp0kOqLc+vWLXTv3h19+/bF4MGDS/y3oapFa35wTpoxcOBADB8+HACwaNGiAvW+vr5wdHTEsmXLULt2beTn56NZs2YF7iAfPHgQ1apVQ2pqKjIzM2FiYqJW379/f0yYMAEGBgaws7ODjk75/z/89u3bcHd3R/v27fHLL7+U+/ZIe/GMkorVvXt3ZGdnIycnB15eXmp19+7dQ2JiIiZOnIguXbpAqVTiwYMHBfo4evQoZs2ahR07dsDY2FgK3n8yMzNDgwYNUKdOHbWQVCqVuHnzJm7evCmVXbx4EQ8fPoSLi4vU5siRI2r9HTlyRKovzK1bt9C5c2e4uroiLCzsjQQzaS+eUVKxqlWrJl3CVqtWTa3OwsICNWvWxC+//AI7OzukpKQUuKx+/PgxPvnkE3z55Zfw9vZG3bp18fbbb8PX1xcffvih7Pa7du2K5s2bo3///pg/fz5yc3MxbNgwdOrUCW3atAEAjB49Gv7+/mjVqhW6du2KHTt2YPPmzdi/f3+hfb4MSUdHR8ydOxd///23VGdra/tKfx+qGvi/UZJlamoKU1PTAuU6OjpYt24d4uLi0KxZM3z99deYM2eOWpsRI0bAyMgI06dPBwA0b94c06dPx2effYZbt27JbluhUGDbtm2wsLBAx44d0bVrVzg7O2P9+vVSm969e2PBggWYO3cumjZtiqVLlyIsLAydO3cutM+IiAhcuXIFkZGRqFu3Luzs7KQPUWEUQgih6UEUJz4+Hq6uroiLi+OkGESVjLYc3zyjJCKSwaAkIpLBoCQiksGgJCKSwaAkIpLBoKRyM2PGDLz99tswMTGBjY0NevfujcTExFL3Fx0dDT8/P9jZ2cHIyAgtW7bEmjVrynDERIVjUFK5iYmJQWhoKI4fP46IiAjk5OTA09MTmZmZau2io6MLXf/gwYPIy8uTlo8ePYoWLVrgjz/+wNmzZxESEoKgoCDs3LmzPHeDiLMH0Ztz9+5dAUDExMRIZcnJycLY2FjMmzdPre3GjRuFnp6eOHLkSLF99ujRQ4SEhJTHcOkN0Jbjm48w0huTkZEBAGrzPjo6OmLHjh3w8fGBnp4ehg4diu3btyMwMBDLly9H+/btZfvka4ypvDEo6Y3Iz8/HV199hQ4dOqBZs2ZqdZ07d8aWLVvg5+cHlUqFZcuWYeHChejfv3+xfW7YsAEnT57E0qVLy3PoRAxKejNCQ0Nx/vx5HD58uNB6T09PjB8/HpMmTUJQUBA+/fTTYvuLiopCSEgIli1bhqZNm5bHkIkkvJlD5W748OHYuXMnoqKiULdu3ULbHD58GLNnz0ZwcDDWr1+PDRs2FNlfTEwMfH19MW/ePAQFBZXXsIkkDEoqN0IIDB8+HFu2bMGBAwdQr169QtudOHECPj4+mDx5MsLCwrBy5UoEBQUV+h7u6Oho+Pj4YNasWRgyZEg57wHRC7z0pnITGhqK8PBwbNu2DSYmJkhLSwPwYpJeAwMDAC9eGdG9e3eMGTMGo0aNAgB89NFHyM7ORkBAAKKjo/HOO+8AeHG53bNnT4wYMQJ9+vSR+tPV1eWLwah8afq2uxxt+fkAFQSg0E9YWJhaux07dhS6/p9//imeP38uLQ8YMKDQ/jp16lSOe0HlSVuOb55RUrkRJZzqtGfPnoWWd+/eXW155cqVWLly5esOi+iV8TtKIiIZDEoiIhkMSiIiGQxKIiIZDEoiIhkMSio1JycnKBQKtc/MmTNL3d/Bgwfh6+uL2rVrQ6FQFPqD89JYuXIlWrRoAX19fdjY2CA0NLTQdleuXIGJiQnMzc3LZLtUeTAo6bV8//33SE1NlT5ffPGFWv3du3dx8eLFAuvl5OQUeO47MzMTb731FhYtWlTsNouavzI2NrbAXJc//vgjJkyYgHHjxuHChQvYv38/vLy8Ch1PQEAA3nvvvWK3TVWUpn/IKUdbfpBa2eXm5oqLFy+qlTk6OhaYR/Lf5s6dK2rVqiUuXbokleXk5Ig+ffqIZs2aidzc3ELXAyC2bNlSoPzx48fCzs5OfPXVV2rlhw4dEkZGRmL9+vVS2f3794WBgYHYv3+/zN4JMWbMGBEYGCjCwsKEmZmZbHsqG9pyfPOMkkpk165daNeuHWJjY9XKZ86ciZo1a6JVq1aYM2cOcnNz1epHjhyJPn36oEuXLrh69Sry8/MRFBSEhIQE7NmzB9WqVXulcRgbGyMiIgKrV6/GuHHjALx4VrxHjx6YMGEC/P39pbYRERHIz8/HrVu3oFQqUbduXfj7++PmzZtqfR44cAAbN26UPZOlqotP5lCJ9OrVC2PGjIGnpyf279+PNm3a4Msvv0Tr1q1haWmJo0ePYvz48UhNTcWPP/6otu7ChQvx9OlTeHh4oF27djh27BgOHjyIOnXqlGosTZs2RUREBDw8PHDv3j1s2rQJX331FcaPH6/W7tq1a8jPz8f06dOxYMECmJmZYeLEiejWrRvOnj0LXV1d3Lt3D8HBwVi9ejVMTU1L/fehSk7Tp7RytOXUvDKIiooq8vnsf36cnZ0LXX/58uWievXq4tmzZwXqcnNzRYMGDQQAceDAAdmxoIhL73/67bffBADx3nvvFVo/bdo0AUDs3btXKrt7967Q0dERe/bsEUII8f7774uxY8dK9bz0frO05fjmpTdJ2rZtC5VKVeRnypQpqF69OubMmVPo+u+88w5yc3ORnJxcoG7kyJF48uQJevbsic8++wypqamvNdbExESMGTMGAQEBSEhIwA8//FCgjZ2dHQDAxcVFKrO2toaVlRVSUlIAvLjsnjt3LqpXr47q1atj0KBByMjIQPXq1bFixYrXGiNVHrz0JomhoSGaNGlSaF1ERARmzJiBtWvX4oMPPii0zenTp6GjowMbGxu18rFjxyI8PBwxMTFo2LAhPvzwQ3Tp0gUxMTGwtrZ+5XFevXoVHh4e6Nu3L/73v/8hJiYGPXr0gJ6eHoYPHy6169ChA4AXofpywuD79+8jPT0djo6OAIBjx46pvelx27ZtmDVrFo4ePVrqrwaoEtL0Ka0cbTk1r+wePXokXa4KIcTRo0fFvHnzxOnTp8XVq1fF6tWrhbW1tQgKClJbb8GCBcLS0lKcOXNGKnv+/Lnw9vYWrVq1Urvr/fjxY5GQkCASEhIEAPHjjz+KhIQEcePGDbU2Dg4OYvDgwSI/P18q37dvnzAwMBB//PGH2vb9/PxE06ZNxZEjR8S5c+dEz549hYuLi8jOzi50P3np/WZpy/HNoKRSiYuLE++8844wMzMT+vr6QqlUiunTpxf4fvKvv/4S8fHxBdZ/+vSp2Ldvn1pZUd+RDhgwQK3dzp071ULypejoaPHw4UO1soyMDDFw4EBhbm4uLC0txfvvvy9SUlKK3C8G5ZulLce3QogSThqoIfHx8XB1dUVcXBxat26t6eEQURnSluObN3OIiGQwKImIZDAoiYhkMCiJiGQwKImIZGjND85VKpWmh0BEZUxbjusKH5RWVlYwNDREYGCgpodCROXA0NAQVlZWmh5GsSr87ygBICUlBenp6ZoeBhGVAysrKzg4OGh6GMXSiqAkItIk3swhIpLBoCQiksGgJCKSwaAkIpLBoCQiksGgJCKSwaAkIpLBoCQiksGgJCKSwaAkIpJR4SfFAPisN1Flpg3Pelf4oExJSYFSqURWVpamh0JE5cDQ0BAqlapCh2WFD8r09HRkZWVh9erVUCqVmh4OEZUhlUqFwMBApKenMyjLglKprNCvsySiyos3c4iIZDAoiYhkMCiJiGQwKImIZDAoiYhkMCiJiGQwKImIZDAoiYhkMCiJiGQwKImIZDAoqcx17twZX331laaHQVRmGJRUKsHBwVAoFAU+V65ceeW+oqOjoVAo8PDhQ7VyBi5VFFozKQZVPN27d0dYWJhambW1tYZGQ1R+eEZJpaanpwdbW1u1T7Vq1Qq0W7VqFdq0aQMTExPY2tri448/xt27dwEAycnJcHd3BwBYWFhAoVAgODgYwcHBiImJwYIFC6Sz1eTkZADA+fPn4e3tDWNjY9SqVQuffPKJ2sTOnTt3xpdffokxY8bA0tIStra2mDx5crn/PajyYlBSucvJycHUqVNx5swZbN26FcnJyQgODgYA2Nvb448//gAAJCYmIjU1FQsWLMCCBQvg5uaGwYMHIzU1FampqbC3t8fDhw/h4eGBVq1a4dSpU9izZw/u3LkDf39/tW3+9ttvMDIywokTJzB79mx8//33iIiIeNO7TpUEL72p1Hbu3AljY2Np2dvbGxs3bizQbuDAgdJ/Ozs746effsLbb7+NJ0+ewNjYGJaWlgAAGxsbmJubS211dXVhaGgIW1tbqWzhwoVo1aoVpk+fLpWtWLEC9vb2uHz5Mho1agQAaNGiBSZNmgQAaNiwIRYuXIjIyEh069atbHaeqhQGJZWau7s7lixZIi0bGRkV2i4uLg6TJ0/GmTNn8ODBA+Tn5wN48ZoPFxeXV9rmmTNnEBUVpRbQL129elUtKP/Jzs5OutwnelUMSio1IyMjNGjQoNg2mZmZ8PLygpeXF9asWQNra2ukpKTAy8sL2dnZr7zNJ0+ewNfXF7NmzSpQZ2dnJ/13jRo11OoUCoUU0ESvikFJ5erSpUu4d+8eZs6cCXt7ewDAqVOn1Nro6uoCAPLy8gqU/7usdevW+OOPP+Dk5ITq1fnPl94M3syhcuXg4ABdXV3873//w7Vr17B9+3ZMnTpVrY2joyMUCgV27tyJv//+G0+ePAEAODk54cSJE0hOTkZ6ejry8/MRGhqK+/fvIyAgACdPnsTVq1exd+9ehISEFAhVorLCoKRyZW1tjZUrV2Ljxo1wcXHBzJkzMXfuXLU2derUwZQpUzBu3DjUqlULw4cPBwCMGjUK1apVg4uLi3TJXrt2bRw5cgR5eXnw9PRE8+bN8dVXX8Hc3Bw6OvznTOVDIYQQmh5EceLj4+Hq6oq4uDi+hZGoktGW45v/CyYiksGgJCKSwaAkIpLBoCQiksGgJCKSwaAkjenVqxccHBygr68POzs7fPLJJ7h9+7ZUHx0dDT8/P9jZ2cHIyAgtW7bEmjVrCvSzceNGNGnSBPr6+mjevDl27979JneDqgAGJWmMu7s7NmzYgMTERPzxxx+4evUqPvzwQ6n+6NGjaNGiBf744w+cPXsWISEhCAoKws6dO9XaBAQEYNCgQUhISEDv3r3Ru3dvnD9/XhO7RJUUf0dJb0ReXh4uX74MpVJZZJvt27ejd+/eeP78eYFntV/y8fFBrVq1sGLFCgBAv379kJmZqRae7dq1Q8uWLfHzzz+X7U5QmdOW45tnlPRG7Nq1C+3atUNsbGyh9ffv38eaNWvQvn37IkMSADIyMqRp2QDg2LFj6Nq1q1obLy8vHDt2rGwGTgQGJb0hvXr1wpgxY+Dp6ak2KcbYsWNhZGSEmjVrIiUlBdu2bSuyjw0bNuDkyZMICQmRytLS0lCrVi21drVq1UJaWlrZ7wRVWQxKKjMvXxJW1GfixInIyMhAv379pHVGjx6NhIQE7Nu3D9WqVUNQUBAK+zYoKioKISEhWLZsGZo2bfomd4uI06xR2Wnbti1UKlWR9Rs2bMDUqVMxZ84cqczKygpWVlZo1KgRlEol7O3tcfz4cbi5uUltYmJi4Ovri3nz5iEoKEitT1tbW9y5c0et7M6dO2qzohO9LgYllRlDQ0M0adKk0LqIiAjMmDEDa9euxQcffFBom5cT6z5//lwqi46ORs+ePTFr1iwMGTKkwDpubm6IjIxUe61tRESEWtASvS4GJb0R7dq1w9atW+Hl5QUAOHHiBE6ePIl3330XFhYWuHr1Kr799lvUr19fCrmoqCj07NkTI0aMQJ8+faTvHXV1daUbOiNGjECnTp3www8/wMfHB+vWrcOpU6fwyy+/aGZHqXISFVxcXJwAIOLi4jQ9FCpDZ8+eFe7u7sLS0lLo6ekJJycn8fnnn4u//vpLajNgwAABoMCnU6dOan1t2LBBNGrUSOjq6oqmTZuKXbt2veG9odLSluObv6MkIo3RluObd72JiGQwKImIZDAoiYhkMCiJiGQwKImIZDAoiYhkMCiJiGQwKImIZDAoiYhkMCiJiGQwKImIZDAoiYhkMCiJiGRozXyUxc2cTUTaSVuO6woflFZWVjA0NERgYKCmh0JE5cDQ0BBWVlaaHkaxKvx8lACQkpKC9PR0TQ+DiMqBlZUVHBwcND2MYmlFUBIRaRJv5hARyWBQEhHJYFASEclgUBIRyWBQEhHJYFASEclgUBIRyWBQEhHJYFASEclgUBIRyajwk2IAfNabqDLThme9K3xQpqSkQKlUIisrS9NDIaJyYGhoCJVKVaHDssIHZXp6OrKysrB69WoolUpND4eIypBKpUJgYCDS09MZlGVBqVSidevWmh4GEVVBvJlDRCSDQUlEJINBSUQkg0FJRCSDQUlEJINBSUQkg0FJRCSDQUlEJINBSUQkg0FJRCSDQUkaERwcDIVCUeBz5coVAEBaWhq++OILODs7Q09PD/b29vD19UVkZKSGR05VkdY8602VT/fu3REWFqZWZm1tjeTkZHTo0AHm5uaYM2cOmjdvjpycHOzduxehoaG4dOmShkZMVRWDkjRGT08Ptra2BcqHDRsGhUKB2NhYGBkZSeVNmzbFwIED3+QQiQDw0psqmPv372PPnj0IDQ1VC8mXzM3N3/ygqMpjUJLG7Ny5E8bGxtKnb9++uHLlCoQQaNKkiaaHRyThpTdpjLu7O5YsWSItGxkZISUlRYMjIiocg5I0xsjICA0aNFAr09PTg0Kh4A0bqlB46U0ViqWlJby8vLBo0SJkZmYWqH/48OGbHxRVeQxKqnAWLVqEvLw8tG3bFn/88QeSkpKgUqnw008/wc3NTdPDoyqIl95U4Tg7OyM+Ph7Tpk3DyJEjkZqaCmtra7i6uqp9p0n0pjAoSSNWrlxZbL2dnR0WLlyIhQsXvpkBERWDl95ERDIYlEREMhiUREQyGJRERDIYlEREMhiUVKglS5agRYsWMDU1hampKdzc3PDnn38CeDFxxRdffIHGjRvDwMAADg4O+PLLL5GRkaHWR0pKCnx8fGBoaAgbGxuMHj0aubm5pR7T5s2b4enpiZo1a0KhUOD06dNq9SUd18mTJ9GlSxeYm5vDwsICXl5eOHPmTKnHRZUfg5IKVbduXcycORNxcXE4deoUPDw84OfnhwsXLuD27du4ffs25s6di/Pnz2PlypXYs2cPBg0aJK2fl5cHHx8fZGdn4+jRo/jtt9+wcuVKfPfdd6UeU2ZmJt59913MmjWr0PqSjOvJkyfo3r07HBwccOLECRw+fBgmJibw8vJCTk5OqcdGlZyo4OLi4gQAERcXp+mhVHkWFhbi119/LbRuw4YNQldXV+Tk5AghhNi9e7fQ0dERaWlpUpslS5YIU1NT8fz5c9ltnTt3rsi669evCwAiISFBtp9/j+vkyZMCgEhJSZHanD17VgAQSUlJsv1R2dKW45tnlCQrLy8P69atQ2ZmZpGPEGZkZMDU1BTVq794huHYsWNo3rw5atWqJbXx8vLCo0ePcOHChWK3l5qainbt2pXJUzj/Hlfjxo1Rs2ZNLF++HNnZ2Xj69CmWL18OpVIJJyen194eVU58MoeKdO7cObi5ueHZs2cwNjbGli1b4OLiUqBdeno6pk6diiFDhkhlaWlpaiEJQFpOS0srdrt2dnbYunUr/Pz8UK1aNbV+X0Vh4zIxMUF0dDR69+6NqVOnAgAaNmyIvXv3SmFK9G88o6QiNW7cGKdPn8aJEycwdOhQDBgwABcvXlRr8+jRI/j4+MDFxQWTJ09+pf4Le7nYy0+3bt2QlZWFzz//vFRzVBY1rqdPn2LQoEHo0KEDjh8/jiNHjqBZs2bw8fHB06dPX3k7VDXwf6FUJF1dXWm+SFdXV5w8eRILFizA0qVLAQCPHz9G9+7dYWJigi1btqBGjRrSura2toiNjVXr786dO1IdAKhUqiK3rVKp0L9/fwwdOhQODg6vNO7ixhUeHo7k5GQcO3YMOjo6UpmFhQW2bduGjz766JW2RVUDg5JKLD8/H8+fPwfw4ozNy8sLenp62L59O/T19dXaurm5Ydq0abh79y5sbGwAABERETA1NZUu34t63UNaWhqGDRuGIUOG4IcffnilMcqNKysrCzo6OlAoFFLZy+X8/PxX2hZVHQxKKtT48ePh7e0NBwcHPH78GOHh4YiOjsbevXvx6NEjeHp6IisrC6tXr8ajR4/w6NEjAC9eN1utWjV4enrCxcUFn3zyCWbPno20tDRMnDgRoaGh0NPTK3bbtra2WLhwIfr06aNWfv/+faSkpOD27dsAgMTERKm9ra1ticbVrVs3jB49GqGhofjiiy+Qn5+PmTNnonr16nB3dy/rPyNVFpq+7S5HW34+UNkMHDhQODo6Cl1dXWFtbS26dOki9u3bJ4QQIioqSgAo9HP9+nWpj+TkZOHt7S0MDAyElZWVGDlypPQzndIICwsrdJuTJk16pXHt27dPdOjQQZiZmQkLCwvh4eEhjh07VupxUelpy/GtEEKIN5zNryQ+Ph6urq6Ii4tD69atNT0cIipD2nJ88643EZEMBiURkQwGJRGRDAYlEZEMBiURkQwGJRGRDAYlEZEMBiURkQwGJRGRDAYlEZEMBiURkQwGJRGRDAYlEZEMrZmPsrjZsIlIO2nLcV3hg9LKygqGhoYIDAzU9FCIqBwYGhrCyspK08MoVoWfjxIAUlJSkJ6erulhEFE5sLKyeuX3Ir1pWhGURESaxJs5REQyGJRERDIYlEREMhiUREQyGJRERDIYlEREMhiUREQyGJRERDIYlEREMhiUREQyKvykGACf9SaqzLThWe8KH5QpKSlQKpXIysrS9FCIqBwYGhpCpVJV6LCs8EGZnp6OrKwsrF69GkqlUtPDIaIypFKpEBgYiPT0dAZlWVAqlWjdurWmh0FEVRBv5hARyWBQEhHJYFBSiQQHB0OhUEChUKBGjRqoVasWunXrhhUrViA/P1/TwysxJycnzJ8/X9PDIC3DoKQS6969O1JTU5GcnIw///wT7u7uGDFiBHr27Inc3NxC18nJyXnDoyQqewxKKjE9PT3Y2tqiTp06aN26Nf7zn/9g27Zt+PPPP7Fy5UoAgEKhwJIlS9CrVy8YGRlh2rRpAIAlS5agfv360NXVRePGjbFq1Sq1vl+u5+3tDQMDAzg7O2PTpk1qbc6dOwcPDw8YGBigZs2aGDJkCJ48eSLVd+7cGV999ZXaOr1790ZwcLBUf+PGDXz99dfS2TFRSTAo6bV4eHjgrbfewubNm6WyyZMn4/3338e5c+cwcOBAbNmyBSNGjMDIkSNx/vx5fPbZZwgJCUFUVJRaX99++y369OmDM2fOoH///vjoo4+k15lmZmbCy8sLFhYWOHnyJDZu3Ij9+/dj+PDhJR7r5s2bUbduXXz//fdITU1Fampq2fwRqNLTmp8HUcXVpEkTnD17Vlr++OOPERISIi0HBAQgODgYw4YNAwB88803OH78OObOnQt3d3epXd++ffHpp58CAKZOnYqIiAj873//w+LFixEeHo5nz57h999/h5GREQBg4cKF8PX1xaxZs1CrVi3ZcVpaWqJatWowMTGBra1tmew7VQ08o6TXJoRQu4xt06aNWr1KpUKHDh3Uyjp06CCdLb7k5uZWYPllG5VKhbfeeksKyZd95OfnIzExsUz2g6goDEp6bSqVCvXq1ZOW/xlmb5KOjg7+/fZl3kyissCgpNdy4MABnDt3Dn369CmyjVKpxJEjR9TKjhw5AhcXF7Wy48ePF1h++diqUqnEmTNnkJmZqdaHjo4OGjduDACwtrZW+94xLy8P58+fV+tTV1cXeXl5r7CHRAxKegXPnz9HWloabt26hfj4eEyfPh1+fn7o2bMngoKCilxv9OjRWLlyJZYsWYKkpCT8+OOP2Lx5M0aNGqXWbuPGjVixYgUuX76MSZMmITY2VrpZ079/f+jr62PAgAE4f/48oqKi8MUXX+CTTz6Rvp/08PDArl27sGvXLly6dAlDhw7Fw4cP1bbh5OSEgwcP4tatW5yRikpOVHBxcXECgIiLi9P0UKq0AQMGCAACgKhevbqwtrYWXbt2FStWrBB5eXlSOwBiy5YtBdZfvHixcHZ2FjVq1BCNGjUSv//+u1o9ALFo0SLRrVs3oaenJ5ycnMT69evV2pw9e1a4u7sLfX19YWlpKQYPHiweP34s1WdnZ4uhQ4cKS0tLYWNjI2bMmCH8/PzEgAEDpDbHjh0TLVq0EHp6ekIL/vlXetpyfCuE+NeXOhVMfHw8XF1dERcXx0kxKjGFQoEtW7agd+/emh4KvUHacnzz0puISAaDkohIBn9wThVCBf8GiKo4nlESEclgUBIRyWBQUgGFzcJDVJUxKKlS2LhxI5o0aQJ9fX00b94cu3fvll1nzZo1eOutt2BoaAg7OzsMHDgQ9+7dewOjJW3DoCStd/ToUQQEBGDQoEFISEhA79690bt37wKPL/7TkSNHEBQUhEGDBuHChQvYuHEjYmNjMXjw4Dc4ctIWDEqStWvXLpiZmWHNmjW4efMm/P39YW5uDktLS/j5+SE5OVmt/a+//gqlUgl9fX00adIEixcvluqSk5OhUCiwbt06tG/fHvr6+mjWrBliYmJKPb4FCxage/fuGD16NJRKJaZOnYrWrVtj4cKFRa5z7NgxODk54csvv0S9evXw7rvv4rPPPkNsbGypx0GVF4OSihUeHo6AgACsWbMG/v7+8PLygomJCQ4dOoQjR47A2NgY3bt3R3Z2NoAXl7Pfffcdpk2bBpVKhenTp+Pbb7/Fb7/9ptbv6NGjMXLkSCQkJMDNzQ2+vr5ql73GxsbFfj7//HOp7bFjx9C1a1e1/r28vHDs2LEi98vNzQ03b97E7t27IYTAnTt3sGnTJvTo0aMs/mxU2Wj2CUp52vIsaGXSqVMnMWLECLFw4UJhZmYmoqOjhRBCrFq1SjRu3Fjk5+dLbZ8/fy4MDAzE3r17hRBC1K9fX4SHh6v1N3XqVOHm5iaEEOL69esCgJg5c6ZUn5OTI+rWrStmzZollSUlJRX7uXPnjtS2Ro0aBba5aNEiYWNjU+x+btiwQRgbG4vq1asLAMLX11dkZ2e/yp+KXpO2HN/8wTkVatOmTbh79y6OHDmCt99+GwBw5swZXLlyBSYmJmptnz17hqtXryIzMxNXr17FoEGD1L7ry83NhZmZmdo6/5ykt3r16mjTpo3aRL4NGjQoj92SXLx4ESNGjMB3330HLy8vpKamYvTo0fj888+xfPnyct02aR8GJRWqVatWiI+Px4oVK9CmTRsoFAo8efIErq6uWLNmTYH21tbW0ou+li1bhnfeeUetvlq1aq+0fWNj42LrAwMD8fPPPwMAbG1tcefOHbX6O3fuFPu6hxkzZqBDhw4YPXo0AKBFixYwMjLCe++9h//+97+ws7N7pfFS5cagpELVr18fP/zwAzp37oxq1aph4cKFaN26NdavXw8bGxuYmpoWWMfMzAy1a9fGtWvX0L9//2L7P378ODp27AjgxRlnXFyc2ovCTp8+Xez6/9y+m5sbIiMj1X77GRERUeDVEv+UlZWF6tXV//m/DHPBxynp3zR97S9HW77DqExefkcphBCXLl0Stra2YsSIESIzM1M0bNhQdO7cWRw8eFBcu3ZNREVFiS+++ELcvHlTCCHEsmXLhIGBgViwYIFITEwUZ8+eFStWrBA//PCDEOL/vqN0cHAQmzdvFiqVSgwZMkQYGxuLv//+u1TjPXLkiKhevbqYO3euUKlUYtKkSaJGjRri3LlzUptx48aJTz75RFoOCwsT1atXF4sXLxZXr14Vhw8fFm3atBFt27Yt5V+NSkNbjm8GJRXwz6AUQoiLFy8KGxsb8c0334jU1FQRFBQkrKyshJ6ennB2dhaDBw8WGRkZUvs1a9aIli1bCl1dXWFhYSE6duwoNm/eLIT4v6AMDw8Xbdu2Fbq6usLFxUUcOHDgtca8YcMG0ahRI6GrqyuaNm0qdu3apVY/YMAA0alTJ7Wyn376Sbi4uAgDAwNhZ2cn+vfvL/7666/XGge9Gm05vjlxL71RycnJqFevHhISEtCyZUtND4c0TFuOb/6OkohIBoOSiEgG73rTG+Xk5MS7yqR1eEZJRCSDQUlEJINBSRozbdo0tG/fHoaGhjA3Ny9Qf+bMGQQEBMDe3h4GBgZQKpVYsGBBgXacV5LKG4OSNCY7Oxt9+/bF0KFDC62Pi4uDjY0NVq9ejQsXLmDChAkYP3682vRpnFeS3gTezKE3Ii8vD5cvX4ZSqZTKpkyZAgBYuXJloesMHDhQbdnZ2RnHjh3D5s2bpccd/zmvJADUq1cPn332GWbNmlUOe0FVFc8o6Y3YtWsX2rVr99oT42ZkZMDS0lJa5ryS9CYwKOmN6NWrF8aMGQNPT0+cOnWqVH0cPXoU69evx5AhQ6SyDh06YM2aNejXrx90dXVha2sLMzMzLFq0qKyGTsSgpLITHR0NhUJR5GfixInIyMhAv379Xrnv8+fPw8/PD5MmTYKnp6dU/s95JePi4rBnzx4kJyerzYBO9Lr4HSWVmbZt26pNvvtvGzZswNSpUzFnzpxX6vfixYvo0qULhgwZgokTJ6rVcV5JehMYlFRmDA0N0aRJk0LrIiIiMGPGDKxduxYffPBBifu8cOECPDw8MGDAAEybNq1APeeVpDeBQUlvRLt27bB161Z4eXlJZSkpKbh//z5SUlKQl5cnTdbboEEDGBsb4/z58/Dw8ICXlxe++eYbpKWlAXgRhNbW1gAAX19fDB48GEuWLJFe6fDVV1+hbdu2qF279hvfT6qkNDrJWwloy3x19OoGDBggABT4REVFCSGEmDRpUqH1jo6Oav1wXkntpS3HN+ejJCKN0Zbjm3e9iYhkMCiJiGQwKImIZDAoiYhkMCiJiGRoze8oi3vig4i0k7Yc1xU+KK2srGBoaIjAwEBND4WIyoGhoSGsrKw0PYxiVfjfUQIvnuBIT0/X9DCIqBxYWVnBwcFB08MollYEJRGRJvFmDhGRDAYlEZEMBiURkQwGJRGRDAYlEZEMBiURkQwGJRGRDAYlEZEMBiURkYwK/6w3wEcYiSozbXiEscIHZUpKCpRKJbKysjQ9FCIqB4aGhlCpVBU6LCt8UKanpyMrKwurV6+GUqnU9HCIqAypVCoEBgYiPT2dQVkWlEplhX5LGxFVXryZQ0Qkg0FJRCSDQUlEJINBSa/l5s2bGDhwIGrXrg1dXV04OjpixIgRuHfvXon7SE5OhkKhwOnTp8tljAqFAlu3bi2XvqlqYFBSqV27dg1t2rRBUlIS1q5diytXruDnn39GZGQk3NzccP/+fU0PkahMMCip1EJDQ6Grq4t9+/ahU6dOcHBwgLe3N/bv349bt25hwoQJAAo/ozM3N8fKlSsBAPXq1QMAtGrVCgqFAp07dwYABAcHo3fv3pgyZQqsra1hamqKzz//HNnZ2VI/Tk5OmD9/vlrfLVu2xOTJk6V6AHj//fehUCikZaJXwaCkUrl//z727t2LYcOGwcDAQK3O1tYW/fv3x/r161GSVzLFxsYCAPbv34/U1FRs3rxZqouMjIRKpUJ0dDTWrl2LzZs3Y8qUKSUe58mTJwEAYWFhSE1NlZaJXgWDkkolKSkJQogiHwJQKpV48OAB/v77b9m+rK2tAQA1a9aEra0tLC0tpTpdXV2sWLECTZs2hY+PD77//nv89NNPyM/PL9E4X/Ztbm4OW1tbaZnoVTAo6bWU90s833rrLRgaGkrLbm5uePLkCW7evFmu2yX6JwYllUqDBg2gUCigUqkKrVepVLCwsIC1tTUUCkWBQM3JySmTcejo6JRb30QvMSipVGrWrIlu3bph8eLFePr0qVpdWloa1qxZg379+kGhUMDa2hqpqalSfVJSktokJ7q6ugCAvLy8Ats5c+aMWv/Hjx+HsbEx7O3tAaBA348ePcL169fV+qhRo0ahfROVFIOSSm3hwoV4/vw5vLy8cPDgQdy8eRN79uxBt27dUKdOHUybNg0A4OHhgYULFyIhIQGnTp3C559/jho1akj92NjYwMDAAHv27MGdO3eQkZEh1WVnZ2PQoEG4ePEidu/ejUmTJmH48OHQ0dGR+l61ahUOHTqEc+fOYcCAAahWrZraOJ2cnBAZGYm0tDQ8ePDgDfxlqLJhUFKpNWzYEKdOnYKzszP8/f1Rv359DBkyBO7u7jh27Jh0U+aHH36Avb093nvvPXz88ccYNWqU2veO1atXx08//YSlS5eidu3a8PPzk+q6dOmChg0bomPHjujXrx969eol/fQHAMaPH49OnTqhZ8+e8PHxQe/evVG/fn21cf7www+IiIiAvb09WrVqVb5/FKqUFKK8v41/TfHx8XB1dUVcXBxnD6pigoOD8fDhQz5VU4lpy/HNM0oiIhkMSiIiGVozcS9VPS8fcSTSNJ5REhHJYFASEclgUFKpHTx4EL6+vqhdu3aBGYJycnIwduxYNG/eHEZGRqhduzaCgoJw+/ZttT4uX74MPz8/WFlZwdTUFO+++y6ioqJKPaZly5bhvffeg4WFBSwsLNC1a1dp0g2i0mJQUqllZmbirbfewqJFiwrUZWVlIT4+Ht9++y3i4+OxefNmJCYmolevXmrtevbsidzcXBw4cABxcXF466230LNnT6SlpZVqTNHR0QgICEBUVBSOHTsGe3t7eHp64tatW6XqjwgAICq4uLg4AUDExcVpeihUDABiy5YtxbaJjY0VAMSNGzeEEEL8/fffAoA4ePCg1ObRo0cCgIiIiJDd5sWLF0Vubm6xbXJzc4WJiYn47bff5HeC3jhtOb55RklvTEZGBhQKBczNzQG8eF68cePG+P3335GZmYnc3FwsXboUNjY2cHV1LbYvIQQCAwMRFBRU7JRrWVlZyMnJUZu6jehVMSjpjXj27BnGjh2LgIAAmJqaAngx8/n+/fuRkJAAExMT6Ovr48cff8SePXtgYWFRbH8vvxM9fvw4goODiwzLsWPHonbt2ujatWuZ7xNVHQxKKnc5OTnw9/eHEAJLliyRyoUQCA0NhY2NDQ4dOoTY2Fj07t0bvr6+0oxAnTt3hkKhKPTj4OCAa9euYdWqVVizZk2B7c6cORPr1q3Dli1boK+v/8b2lyqfCv+sN2kHhUKBLVu2oHfv3mrlL0Py2rVrOHDgAGrWrCnVRUZGwtPTEw8ePJDOMoEXk20MGjQI48aNQ0pKitqUbP/0+PFjfPTRR6hTpw7+/PNPGBkZSXVz587Ff//7X+zfvx9t2rQp252lKodP5lC5eRmSSUlJiIqKUgtJAFIAvpwy7SUdHR3pUtrBwaHQvoUQ8PDwgJ2dHXbv3q0WkrNnz8a0adOwd+9ehiSVCQYlldqTJ09w5coVafn69es4ffo0LC0tYWdnhw8//BDx8fHYuXMn8vLypJ/8WFpaQldXF25ubrCwsMCAAQPw3XffwcDAAMuWLcP169fh4+NT7LYVCgXGjRuH9u3bw9jYWCqfNWsWvvvuO4SHh8PJyUnaprGxsVo7oleiyVvupN2ioqIEgAKfAQMGiOvXrxdaB0BERUVJfZw8eVJ4enoKS0tLYWJiItq1ayd2795d6jE5OjoWus1Jkya9/g5TlcXvKImIZPCuNxGRDAYlEZEMBiURkQwGJRGRDAYlldq9e/dgY2OD5ORkREdHQ6FQ4OHDh2W6jX9P3/aqfv75Z/j6+pbdgKhKYlBSqU2bNg1+fn5wcnIqt22kpqbC29sbAJCcnAyFQoHTp0+XeP2BAwciPj4ehw4dKqcRUlXAoKRSycrKwvLlyzFo0KBy3Y6trS309PRKvb6uri4+/vhj/PTTT2U4KqpqGJRUKrt374aenh7atWtXZJs//vgDTZs2hZ6eHpycnPDDDz+o1aempsLHxwcGBgaoV6+e9DTN/PnzpTb/vPSuV68eAKBVq1ZQKBTo3LkzgBeT9bZt2xZGRkYwNzdHhw4dcOPGDakPX19fbN++HU+fPi2bnacqh48wUqkcOnSo2Dkj4+Li4O/vj8mTJ6Nfv344evQohg0bhpo1ayI4OBgAEBQUhPT0dERHR6NGjRr45ptvcPfu3SL7jI2NRdu2bbF//340bdoUurq6yM3NRe/evTF48GCsXbsW2dnZiI2NhUKhkNZr06YNcnNzceLECSlciV4Fg5JK5caNG6hdu3aR9T/++CO6dOmCb7/9FgDQqFEjXLx4EXPmzEFwcDAuXbqE/fv34+TJk9LEFb/++isaNmxYZJ/W1tYAXkz4a2trCwC4f/8+MjIy0LNnT9SvXx8AoFQq1dYzNDSEmZmZ2lkm0avgpTeVytOnT4ud41GlUqFDhw5qZR06dEBSUhLy8vKQmJiI6tWro3Xr1lJ9gwYNZCfs/TdLS0sEBwfDy8sLvr6+WLBggTSX5T8ZGBgUOV0bkRwGJZWKlZUVHjx4oOlhAADCwsJw7NgxtG/fHuvXr0ejRo1w/PhxtTb379+XzkiJXhWDkkqlVatWuHjxYpH1SqUSR44cUSs7cuQIGjVqhGrVqqFx48bIzc1FQkKCVH/lypViw1dXVxcAkJeXV+h4xo8fj6NHj6JZs2YIDw+X6q5evYpnz56hVatWJd4/on9iUFKpeHl54cKFC0UG28iRIxEZGYmpU6fi8uXL+O2337Bw4UKMGjUKANCkSRN07doVQ4YMQWxsLBISEjBkyBAYGBio3Yj5JxsbGxgYGGDPnj24c+cOMjIycP36dYwfPx7Hjh3DjRs3sG/fPiQlJal9T3no0CE4OztL32ESvTJNz/NG2qtt27bi559/FkL839yUDx48kOo3bdokXFxcRI0aNYSDg4OYM2eO2vq3b98W3t7eQk9PTzg6Oorw8HBhY2Mj9SlEwdfgLlu2TNjb2wsdHR3RqVMnkZaWJnr37i3s7OyErq6ucHR0FN99953Iy8uT1vH09BQzZswonz8CVQmcj5JKbdeuXRg9ejTOnz9f4HUOpfHXX3/B3t4e+/fvR5cuXcpghMCFCxfg4eGBy5cvw8zMrEz6pKqHPw+iUvPx8UFSUhJu3boFe3v7V17/wIEDePLkCZo3b47U1FSMGTMGTk5O6NixY5mNMTU1Fb///jtDkl4LzyhJY/bu3YuRI0fi2rVrMDExQfv27TF//nw4OjpqemhEahiUREQyeNebiEgGg5KISAaDkohIBoOSiEgGg5KISAaDkohIBoOSiEgGg5KISAaDkohIxv8Do4DQqwCohqUAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}